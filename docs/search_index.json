[["index.html", "Data Science from Scratch Code Chapter 1 Computer Set-Up 1.1 Mac Tricks 1.2 Virtual Environment 1.3 How to efficiently manage a Project? 1.4 Terminal-Commands: 1.5 How to activate &amp; use Python? 1.6 LaTeX 1.7 Wörterbuch 1.8 Ausblick", " Data Science from Scratch Code Joffrey Anthony 2022-05-19 Chapter 1 Computer Set-Up When building projects efficiently, you will need different set-ups in order to have everything in your code work properly, not only on your own machine, but also on other machines, for example of another team-member. 1.1 Mac Tricks Before we start with the whole set-up, you will also need to know how to work more efficiently with your Macbook-Computer. Those “tricks” will be useful for many other programs you will need when working in the field of datascience. 1.1.1 Öffne die Developer-Tools von Google-Chrome command + shift + i 1.1.2 Speichern command + s 1.1.3 Speichern Unter shift + option + command + s 1.1.4 Finder command + leerschlag 1.1.5 Programm schliessen command + q 1.1.6 Neuer Ordner shift + command + n 1.1.7 Tilde Zeichen Das ist nützlich für shortcuts im Terminal, wenn man das Working Directory festlegt: option + n 1.1.8 Backslash Useful for paths within the Directory: option + shift + 7 1.1.9 Screenshot vom Bildschirm command + shift + 3 1.1.10 Teil des Bildschirms Screen-Schoten shift + command + 4 1.1.11 Verlauf löschen in Chrome Browser shift + command + entfernen 1.1.12 Verlauf löschen Safari command + alt + e 1.1.13 Bildschirm Aufnahme Funktioniert nur, wenn QuickTime Player aktiv ist: option + command + n 1.1.14 Lesezeichen in Google Chrome command + d 1.1.15 Search Console Pop Up option + command + j 1.1.16 Format Kopieren einer Zelle (in Numbers) option + command + c 1.1.17 Format übertragen einer Zelle (in Numbers) option + command + v 1.1.18 Switch between Applications on your computer command + tab 1.1.19 Move Forward through Tabs control + tab 1.1.20 Zahl als Exponent Geht nur in Pages oder Tablett!: shift + control + command + „+“-Zeichen WÄHREND man die Zahl mit der Maus markiert Dann ist sie noch „normal“ und nicht tiefgestellt —&gt; also so: zum Beispiel: 2. 1.1.21 Source-Code einer Webseite aufschalten Option + CMD + u im Browser drücken 1.1.22 Interaktive Code-Ansicht für das Abchecken von Webseiten fn + F12-Taste auf Touch Bar drücken 1.1.23 Approximate Symbol ≈ option + x 1.2 Virtual Environment Because the libraries you work with in your projects will be updated over time (this is generally bad news, since this will cause all sorts of dependency problems across your libaries you use), it is crucial to understand that you will need a virtual environment (venv). There, you will install all the libraries you need. The major advantage here, is that you can control the version you install the library. Furthermore, you can send the venv to another computer and the people will download exactly the versions of each library. This allows that your code will always work, independently of the machine you will use! There will be no dependency problems anymore, which is a huge win.. :) 1.2.1 Create a venv Go to your terminal and plug in the following code: conda create -n YOUR-VENVIRONMENT-NAME-HERE python=3.6 1.2.2 Activate your newly created venv Weirdly though, you also need to activate the environment you created above. Plug in the following code: source activate YOUR-ENVIRONMENT-NAME Note that the code above can also be used to activate virtual environments you created in the past! =) 1.2.3 Install packages Now that you are in your new venv, you can start downloading some packages: python -m pip install SOME-PACKAGES 1.2.4 Overview of packages To check your packages within your venv, simply type: conda list 1.2.5 Overview of every venv To check all the venv I created, simply type: conda env list 1.2.6 Execute any python skript In order to execute a Python-script, you will need to head towards the directory that the .py-file is and - then - type in: cd go-to-the-dir-where-your-file-is python my-script.py 1.2.7 Deactivate the venv After you have completed what you wanted, you will need to shut-down the venv. Simply type: conda deactivate 1.2.8 Delete a venv Simply type: conda env remove -n my-new-env 1.3 How to efficiently manage a Project? In today’s world, it is from utmost importance to be able to explain &amp; document your work, otherwise you cannot convince your managers / higher-ups of your work. Ideally, your documentation should be done in an environment that can be accessed anywhere! And what better tool nowadays than a website, since everyone has access to a computer with a browser today! =) 1.3.1 Create a Website on Github (for free) Für mich: https://www.youtube.com/watch?v=xt3-JgAxWgE For visitors, I recommend you to watch this tutorial: https://www.youtube.com/watch?v=m5D-yoH416Y 1.3.2 R Markdown Syntax Es folgt eine Zusammenfassung der wichtigsten Markdown-Sytnax, damit ich schnell gute Anleitungen &amp; Tutorials für jegliche Themen meiner Wahl verfassen kann. 1.3.2.1 Headers Headers sind die &lt;h1&gt; bis &lt;h6&gt; in der HTML-Sprache. Diese werden hier mit Hashtags # geschrieben. Es gibt folgende Möglichkeiten: # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style: Alt-H1 ====== Alt-H2 ------ 1.3.2.2 Wichtige Textausschnitte markieren Wenn ein Autor einen Text verfasst, dann kann es sehr hilfreich für einen Leser sein, wenn die Hauptbotschaften hervorgehoben werden. Es gibt in Markdown grundsätzlich sieben Wege, um dies zu tun: Italics, aka kursiv, mit *asterisks* or _underscores_. Fettgedruckt, aka bold, mit **asterisks** or __underscores__. Kombiniere fett &amp; kursiv mit **asterisks and _underscores_**. Wörter unterstreichen mit HTML-Tag &lt;u&gt;dies ist wichtig&lt;/u&gt;! Durchgestrichen mit zwei tildes. ~~Scratch this.~~ (= drücke Option + n) Codes hervorheben mit Appostroph. `&lt;h1&gt;` (= drücke shift + ^-Taste) Ganze Code-Chunks mit drei Appostrophs. ```noob``` Als Output, sieht es so aus: Fettgedruckte Wörter Italics, dh kursiv Kombination aus fettgedrucktem und kursiv. Dies ist nicht korrekt gelöst! Du bisch en spasst, just kiddin du Esel ;) Codes im Text hervorheben, also zum Beispiel &lt;h1&gt;. Sogenannte Code-Chunks, wie oben verwendet, braucht einen neue Zeile, also: This is a Code Chunk, where you can write your R-Code, for example. 1.3.2.3 Listen Oftmals ist es von Vorteil, wenn jeder Schritt einer Erklärung oder Anleitung mit Hilfe von Listenpunkten heruntergebrochen wird. Ein Mensch versteht dann viel besser, was er zu tun hat, wenn er etwas Neues / zum ersten Mal macht. Wie du es bereits aus HTML kennst, gibt es sogenannte Unordered Lists (HTML-Code wäre ul) oder odered Lists (HTML-Code wäre ol). Ausserdem können Listen innerhalb von Listen erstellt werden, wenn ein Arbeitsschritt noch weiter heruntergebrochen werden kann: 1. Erstes Element einer Ordered List, aka das erste &lt;li&gt;&lt;/li&gt; im HTML-Code. 2. Zweites Element einer Ordered List, aka das zweite &lt;li&gt;&lt;/li&gt; im HTML-Code. * Unordered sub-list. ACHTUNG: Drücke 2-mal auf die Tab-Taste, sonst wird keine Einrückung gemacht! 1. Bemerke, dass ich hier absichtlich wieder bei &quot;1&quot; beginne, weil ich dir zeigen will: Actual numbers don&#39;t matter, just that it&#39;s a number 1. Ordered sub-list 2. Vierter Listenpunkt. Alternative für Unordered Lists: * Unordered list can use asterisks - Or minuses + Or pluses Erstes Element einer Ordered List, aka das erste &lt;li&gt;&lt;/li&gt; im HTML-Code. Zweites Element einer Ordered List, aka das zweite &lt;li&gt;&lt;/li&gt; im HTML-Code. Unordered Sub-List. ACHTUNG: Man muss 2-mal auf die Tab-Taste drücken, sonst wird keine Einrückung gemacht! Bemerke, dass ich hier absichtlich wieder eine “1” im Code schreibe, weil ich dir zeigen will, dass die ordered List mit einem 3) beginnen wird! Ordered Sub-List. ACHTUNG: Auch hier muss man wieder 2-mal auf die Tab-Taste drücken, sonst wird keine Einrückung gemacht! Vierter Listenpunkt. Alternative für Unordered-Lists: Unordered list mit der Asterisk-Schreibweise. Oder mit einem Minus-Zeichen. Oder mit einem Plus-Zeichen. 1.3.2.4 Hyperlinks Häufig gibt es im Internet sehr gute Zusammenfassungen oder Konzepte, auf welche du referenzieren kannst, indem du einen Textausschnitt mit Hilfe von einem Hyperlink hervorhebst: [Dies ist zum Beispiel die Zusammenfassung](https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet#h4), auf welche ich mich basiere, um dir Markdown beizubringen. Ich kann so auch - zum Beispiel - auf die [Webseite von Tee](https://www.audiophil-dreams.com/) verlinken. Dies ist zum Beispiel die Zusammenfassung, auf welche ich mich basiere, um dir Markdown beizubringen. Ich kann so auch - zum Beispiel - auf die Webseite von Tee verlinken. 1.3.2.5 Bilder Wenn jemand eine Erklärung erhält, ist es unentbehrlich, auch Bilder zu verwenden, denn Visualisierungen von Konzepten sind oftmals extrem nützlich, um eine Intuition / Verständnis für Neues zu entwickeln. Bei einer Anleitung können zum Beispiel Screenshots verwendet werden, um die schriftlichen Erklärungen mit Bilder effizient zu unterstützen. Es gibt grundsätzlich vier Arten, um Bilder in ein Markdown-Dokument einzufügen: 1) Bild aus Internet, &quot;Inline-Style&quot;: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png &quot;Logo Title Text 1&quot;) 2) Bild aus Internet, &quot;Reference-Style&quot;: ![alt text][logo_1] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png 3) Bild aus Root-Directory (= Ordner, indem sich das R-Markdown-File &#39;ZF-Syntax&#39; befindet), &quot;Inline-Style&quot;: ![alt text](path-to-image-here) 4) Bild aus Root-Directory (= Ordner, indem sich das R-Markdown-File &#39;ZF-Syntax&#39; befindet), &quot;Reference-Style&quot;: ![alt text][logo_2] [logo]: path-to-image-here --- Bemerkung: &quot;Reference-Style&quot; ist sehr nützlich, insbesondere, weil du die References basically zuunterst in das R-Markdown Dokument einfügen kannst. Nehmen wir zunächst den Fall von einem Bild aus dem Internet: Die Inline-Methode ist einfach: Die Reference-Methode kann auch verwendet werden, um den gleichen Output zu generieren: Als nächstes, nehmen wir ein Bild, welches von meinem eigenen Computer stammt. Ich hatte ich VTX eine Frage bezüglich meiner Domain geschrieben und musst Ihnen einen Screenshot einer meiner E-Mails angeben: Mit Inline-Methode: Mit Reference-Methode: 1.3.2.6 Tabellen Für gewisse Konzepte, eignen sich Tabellen besonders gut, insbesondere wenn es um die übersichtliche Darstellung eines Konzeptes geht: | Produkt | Beschreibung | Preis | | ------------- |:-------------:| -----: | | Focal Sopra | Lautsprecher | CHF 12&#39;000 | | Focal Chorus | Lautsprecher | CHF 1&#39;200 | | B&amp;W 800 D3 | Lautsprecher | CHF 30&#39;000 | Alternativ kann auch mit weniger Zeichen gearbeitet werden, allerdings sieht die Tabelle - als Code - nicht besonders leserlich aus: Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Hier wäre die Muster-Tabelle für die Preise von Lautsprechern: Produkt Beschreibung Preis Focal Sopra Lautsprecher CHF 12’000 Focal Chorus Lautsprecher CHF 1’200 B&amp;W 800 D3 Lautsprecher CHF 30’000 Alternativ kann die gleiche Tabelle mit weniger Syntax erzielt werden: Produkt Beschreibung Preis Focal Sopra Lautsprecher CHF 12’000 Focal Chorus Lautsprecher CHF 1’200 B&amp;W 800 D3 Lautsprecher CHF 30’000 Bemerkung: Man kann innerhalb der Tabelle die Textausschnitte weiterhin mit Markdown erweitern, also - beispielsweise - fettgedurckt oder kursiv schreiben. 1.3.2.7 Blockquotes Zitate oder - in Emails - können Fragen der gegenüberstehenden Person besser hervorgehoben werden und tragen zum Verständnis bei: &gt; Herr Mayer, was sind die Vorteile des Optimize-Audio Konzeptes? &gt; &quot;Comerades&quot; isn&#39;t just a word. It refers to heart and believing in each other. - Mavis Herr Mayer, was sind die Vorteile des Optimize-Audio Konzeptes? Sie profitieren von Synergie-Effekten, welche die Leistung einer Anlage qualitätsmässig vervielfacht. “Regrets is a powerful poison. The more you harbor those feelings, the harder it is to move on.” Amamya 1.3.2.8 HTML in Markdown Wie bereits in einem oberen Kapitel gesehen, lässt sich R Markdown auch mit einfachem HTML kombinieren: &lt;ul&gt; &lt;li&gt;Definition list&lt;/li&gt; &lt;li&gt;Is something people use sometimes.&lt;/li&gt; &lt;li&gt;Does *not* work very **well**. Use HTML &lt;em&gt;tags&lt;/em&gt; instead.&lt;/li&gt; &lt;/ul&gt; Definition list Is something people use sometimes. Sometimes, does not seem to work very well with normal R Markdown. Use HTML tags instead. 1.3.2.9 Horizontale Linien Um Themen voneinander abzugrenden und schöneres Layout zu haben, empfiehlt sich, mit Linien zu arbeiten, sobald ein neues Thema beginnt. Es gibt drei Möglichkeiten, um horizontale Linien einzubauen: 1) Verwende 3 Hyphens --- 2) Verwende 3 Asterisks *** 3) Verwende 3 Underscores ___ Hyphens Asterisks Underscores 1.3.2.10 Videos einfügen In R Markdowngibt es keinen direkten Weg, um Videos direkt einzubetten. Am einfachsten geht es mittels HTML-Code, bei dem es grundsätzliche eine Möglichkeit gibt: Füge ein Bild mit einem Link zum Video hinzu: &lt;a href=&quot;http://www.youtube.com/watch?feature=player_embedded&amp;v=YOUTUBE_VIDEO_ID_HERE &quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg&quot; alt=&quot;IMAGE ALT TEXT HERE&quot; width=&quot;240&quot; height=&quot;180&quot; border=&quot;10&quot; /&gt;&lt;/a&gt; Mit der Bild-zum-Video-Methode erhalte ich: 1.3.2.11 LaTeX für mathematische Formeln Es gibt die Möglichkeit, mittels der Programmiersprache LaTeX mathematische Formeln aufzuschreiben. Indem man das Dollarzeichen $ verwendet, versteht R Markdown, dass man die Sprache LaTeX verwendet: - Für ein &quot;Block-Element&quot; - dh man beginnt auf einer neuen Zeile - verwendet man zwei Dollarzeichen: $$\\underbrace{z-score = \\frac{\\overbrace{x}^{observation}-\\overbrace{\\mu}^{population-mean}}{\\underbrace{\\sigma}_{population-sd}}}_{Standardisierungs-Formel}$$ - Für ein &quot;Inline-Element&quot;, verwendet man nur ein Dollarzeichen: Der Mittelwert wird oftmals als $\\muh$ bezeichnet und die Varianz $\\sigma$. Beispiel für ein “Block-Element”: \\[\\underbrace{z-score = \\frac{\\overbrace{x}^{observation}-\\overbrace{\\mu}^{population-mean}}{\\underbrace{\\sigma}_{population-sd}}}_{Standardisierungs-Formel}\\] Illustration des “Inline-Elements”: Der Mittelwert wird oftmals als \\(\\mu\\) bezeichnet und die Varianz \\(\\sigma\\). 1.3.2.12 Code Chunks Inputs / Outputs anzeigen / verstecken mit Eval, Echo, Results Bei Code Chunks, möchte man manchmal den Code absichtlich zeigen, aber manchmal auch - zum Beispiel - nur eine Graphik, ohne dass man den Code (= Input) sieht. Deshalb gibt es in RMarkdown diverse Befehle, die man als sogenannte Argumente (= Inputs) angeben kann: Wenn man nur den Input-Code anzeigen will, jedoch nicht den dazugehörigen Output (= z.B. um den Code anzuzeigen, wie man ein Bild exportiert, aber dieses Bild nicht effektiv exportieren will), verwendet man das Argument eval = FALSE. Hier ein Beispiel anhand meines Masterarbeit-Codes, welchen ich nur zeigen, aber nicht runnen möchte: ############### ## Step 1: Preliminary basic steps ###### 1) Load some packages, set working directory and clear the workspace. ###### rm(list = ls()) # clear workspace setwd(&quot;~/Uni/Masterstudium/Masterarbeit/final-code&quot;) # set wd # load some packages: library(dummies) library(foreign) library(stargazer) # important to create tables library(readstata13) # important to load the ddset library(haven) library(plyr)# to use functions like count() # Note: install.packages(&quot;readstata13&quot;) first before you can load them! Wenn man nur den Output des Codes zeigen möchte, aber nicht den dazugehörigen Input (= Code), dann verwendet man das Argument echo = FALSE. Eventuell müssen auch noch message=FALSE und warning=FALSE verwendet werden, falls zusätzliche Hinsweise beim evaluieren / runnen des Codes auftauchen (im unteren Beispiel ist dies der Fall!). Hier ein Beispiel, bei dem ich nur die erstellte Graphik zeigen möchte: Abschliessend gibt es noch den Fall, dass man weder den Input, noch den Output eines Code Chunks nicht zeigen möchte, aber der Code dennoch evaluiert wird. Dies tut man mittels des Arguments results = hide &amp; include = FALSE. Beispielsweise hast du in deiner MA ganz viele male den Code View = data verwendet. Dadurch gibt es ein Fenster, welches durch Pop-Up auftaucht. Oftmals ist dies praktisch, wenn man nach bugs im Code sucht, aber schlussendlich sollten diese im Endergebnis nicht angezeigt werden. Hier verwende ich einfach basic Mathe, um dir zu zeigen, dass es funktioniert, obwohl ich einen Code Chunk im RMarkdown-Dokument eingefügt habe: no code shows! 1.3.2.13 Hyperlinks Hier ein Link zu einem guten Youtube-Video. 1.3.2.14 Code-Snippets When writing Code, I recommend you to use .Rmd-Documents, which will allow you to combine both, text, as well as Code (in R, Python, or any other language of your choice). Actually, this document is written in .Rmd-Files, where I combine the text, with so-called “code-snippets”, like the follwing: This is a Code-Snippet When writing Code in R-Markdown, it will be useful for you to know when: to execute code wihtin a code-snippet and where not. 1.3.2.14.1 Hide Source Code This will be done with echo = FALSE: ## [1] 2 With figures, you need fig.show = 'hide' in R: plot(cars) To hide warning-messages, just use message = FALSE: message(&quot;You will not see the message.&quot;) 1.3.2.14.2 Execute a Code-Chunk without showing any Output You will get an output, e.g. the code will execute, but you will not show the code. 1.3.2.14.3 Do NOT execute a Code-Chunk If you want to show code for demonstration purposes - like on this Website - you will probably want to only show the code, but not execute it. This is also possible with eval=FALSE: 1.3.2.15 RMarkdown- VS. Markdown-Files Verwende &lt;ins&gt;&lt;/ins&gt; als HTML, um Texte zu Unterstreichen. Achtung : Der HTML-Tag &lt;u&gt;&lt;/u&gt; geht nicht in .rm-files. Lustigerweise funktioniert &lt;u&gt;&lt;/u&gt; jedoch in .Rmd (= R Markdown) 😂 1.3.2.16 Nützliche Hexadecimals Hexadecimal code for the left square-bracket = &amp;#91; –&gt; Ich muss das so machen mit den eckigen Klammern, weil [] wird für Links &amp; Bilder verwendet in Markdown  Hexadecimal code for the right square-bracket = &amp;#93; –&gt; Ich muss das so machen mit den eckigen Klammern, weil [] wird für Links &amp; Bilder verwendet in Markdown  1.4 Terminal-Commands: A terminal will be essential for your projects, since you will - oftentimes - install packages or move files around your repositories with it. Here, you will find the most useful things you should know when using the Terminal. 1.4.1 Aktuelle Position // Directory? For Mac: pwd = print working directory For Windows: dir = this is the same command as pwd, but dir is for Windows 1.4.2 Showing the child-directories inside the directory you are currently in? ls = prints all the child-directories (= one layer deeper of the path) from the parent-directory (= current directory you are in with your terminal) you are currently in. 1.4.3 Delete everything you wrote in your Terminal up until now? clear = clears the terminal 1.4.4 Change directory? cd = change directory cd .. = go back one directory. 1.4.5 Creating a new directory? mkdir new-folder-1 new-folder-2 new-folder-3 = This creates 3 new folder within the current (working-) directory you are currently in. 1.4.6 Create a new file? touch index.html app.css == This will create an index.html, as well as a app.css-file within the current (working-) directory you are currently in. 1.4.7 Remove files? rm index.html app.css capture.png = This will delete the index.html-, the app.css and the capture.png-files from the current (working-) directory you are currently in. 1.4.8 Open the current directory you are in? open . = opens the current directory you are in 1.4.9 Terminal Magic-Commands for being faster? Trick #1: hit the “Tab-Taste” == will automatically auto-fill the name of the file / directories etc. Example: Type cd Dok + “Tab”-Taste –&gt; auto-fill activates –&gt; im Terminal steht dann der automatisch ausgefüllte Name des Files / Directories, zum Beispiel cd Dokumente bzw. cd Name_Of_Child_Directory Trick #2: How to find a path of a directory that is situated very deeply in your local computer? Example: Type cd + drag-&amp;-drop the folder that is deep in your computer with the file in it. 1.4.10 Syntax im Terminal *-Zeichen == Represents “all files”. 1.4.10.1 Beispiel: Delete all Files in a folder that start with the letter a? To delete all files in a folder that start with the letter a, then you should write: /folder/a* .-Zeichen == use the . character to represent the current folder. ~-Zeichen == represents the “home directory”. 1.4.10.2 Beispiel: how to return to your home directory? You should use: cd ~ 1.5 How to activate &amp; use Python? Python can be executed on your local computer via a Jupyter Notebook, which can be accessed through an IDE. R, Visual Studio Code or PyCharm are examples of IDEs. Let’s assume, that we took PyCharm as our IDE. We do the following steps: Use the Terminal within PyCharm. Once you opened the PyCharm-Terminal, go to the directory that will be used for the Jupyter Notebook, by typing something as cd /some_folder_name. Finally, type in jupyter notebook in the Terminal to launch the Jupyter Notebook App. The notebook interface will appear in a new browser window or tab. 1.6 LaTeX This bookdown is created via a .Rmd-File. For a Markdown to be able to read some mathematical formulas, we will need the LaTeX-language. 1.6.1 What is the LaTeX-Code for \\(\\hat{y}\\)? Simply type: $\\hat{y}$ 1.6.2 Useful Websites Frequent Formulas as LaTeX-Code: http://www.malinc.se/math/latex/basiccodeen.php#:~:text=The%20code%20%5Ctimes%20is%20used,never%20be%20negative%20by%20definition. LaTeX-Converter: http://www.sciweavers.org/free-online-latex-equation-editor 1.7 Wörterbuch Jargon of a domain - like in Artificial Intelligence, Software Development or Economics - is one of the biggest challenges if you truly want tounderstand and master a new subject. Scientists love to create sophisticated new words for new discoveries, methods or conclusions that they came to uncover in their respecitve fields. The problem with that, is that it acts as a huge market-barrier to those that want to enter the world many years later. You can see this even for “non-scientific” things, like the Pokemon franchise, that grow from 150 Pokemon to almost 1’000 Pokemons. The problem is, that the audience does not nearly grow as much, as in the beginning and that the new generation is not so enthousiastic about it, partly because the field has become too large for noobies. That’s why it is form utmost importance to know each field’s jargon. I firmly believe that it is one of the biggest chalenges to overcome in order to be good at something. Because when you are able to “decode” the jargon, you can start “understand” the things. And when you understand, you can start applying, and that is where the fun truly begins! And this is why you will almost find a “Wörterbuch”-sub-chapter in every chapter of this bookdown :) 1.7.1 Data Science VS. mein Economics-Studium Was ist der Unterschied zwischen einem Data Scientist VS. was ich in meinem Economics-Studium gemacht habe? Erkenntnis: Machine Learning wird als eine “Black Box” betrachtet, wo man den X-Variablen des Modells keine grosse Beachtung schenkt: man bringt sie einfach ins Modell rein. Im Kontrast dazu, sind Ökonomen mehr dazu getrimmt, mit Hilfe eines theoretischen Modells, die “richtigen” X-Variablen zu selektieren. Man nennt dies dann auch “structural econometrics”. Erkenntnis 2: Bei uns im Studium wurde - VOR der Modellschätzung - der Schritt vom “Data-Splitting (into Train- &amp; Test-Set)” nie gemacht, da diese Simulations-Methoden schlichtweg sehr wenig verbreitet sind im Forschungs-Feld. What is artificial intelligence? Artificial intelligence is a branch of computer science that deals with the creation of “intelligent agents”, which are systems that can reason, learn, and act autonomously. In contrast, Machine Learning is a subset of artificial intelligence that helps you build AI-driven applications. Aditionally, Deep Learning is a subset of machine learning that uses vast volumes of data and complex algorithms to train a model. 1.8 Ausblick Hier liste ich alles auf, das ich in nächster Zeit ausprobieren möchte und - meiner Meinung nach - grossen Wert besitzt: Tutorial (Artikel von Towards-Datascience): Build Animated R-Visualizations "],["foundatios-of-programming.html", "Chapter 2 Foundatios of Programming 2.1 How to Program? 2.2 Tools to get started 2.3 Wörterbuch 2.4 Data-Types 2.5 Global Variables VS. Local Variables 2.6 Python-Basics 2.7 R as your IDE 2.8 Restful API 2.9 More advanced Topics", " Chapter 2 Foundatios of Programming 2.1 How to Program? In dieser ZF, werde ich mit Hilfe des Youtubers The Coding Train versuchen, meine Programmierkenntnisse zu erweitern. 2.2 Tools to get started Download the Program Processing: The Coding Train uses this to make all of his example. The programming language of Processing is Java. Processing Foundation Learning Processing Book 2.3 Wörterbuch 2.3.1 Synonyme inputs // arguments function // command cartesian coordinate system // “normales Koordinatensystem”, dh mit der X-Achse für die horizontale Linie &amp; Y-Achse für die vertikale Linie Source // Programming Language (any kind of) Properties of an object (Kontext: in Object Oriented Proggramming) // properties // variables // fields // instances // state Functionality of an object (in Object Oriented Programming) // methods // behaviour // operations initialization // declaration // specification dependencies // Abhängigkeitsprobleme fetch data // retrieve data cache data // store data in the computer’s memory (for future use) Rest API // Restful API __init__ // Custom-Constructor // Initialisierungs-Methode Taxonomie // Framework // (“State-of-the-Art”) Methodik // Systematik // Vorgehensweise 2.3.2 Programming What is an Algorithm?: An Algorithm is a list of instructions. What is Programming about?: Programming is not about hardcoding numbers into functions to create a picture. Programming is about creating a sequence of instructions (= a logic) to execute a task. What is a Command &amp; what are arguments?: The command is the function as a whole, while arguments are the inputs of a function // command The coordinate system of a computer VS. the coordinate system in mathematics?: The y-axis of a computer goes down starting the top-right corner, while the x-axis goes to the right starting the top-right corner What is an Array?: An Array is a List of Data. In a DataFrame-Object, you can think of a column OR a row to be arrays. It is a data structure, which contains N-objects within a list. Quelle: The Coding Train 3:10-3:22 What is an IDE? Abkürzung: IDE == Integrated Development Environment Definition: An integrated development environment (IDE) is software for building applications. It combines common developer tools into a single graphical user interface (GUI). Example: R, PyCharm oder Visual-Studio Code are all IDEs. Think of it as a modern Dream Weaver! :) What are Dependencies?: Das sind Probleme, die mit den Versionen der eingesetzten Packages - welche du in einem DS-Projekt verwendest - zu tun haben. Die Lösung wäre der Einsatz von einem Virtual Environment (oder ein Docker-Container). Beispiel, dass das Problem beschreibt: Du machst eine Datenanalyse und verwendest zum Zeitpunkt “t” bestimmte Versionen dieser Packages. Nun vergeht jedoch über 1 Jahr und du möchstest das Modell wieder zum laufen bringen, welches du damals konstruiert hattest. Ausserdem verwendest du dabei einen anderen Computer. Mit einem Memory-Stick überträgst du das Modell auf diesen neuen Computer. Allerdings funktioniert das Modell nicht mehr. Nach etlichen Recherchen findest du heraus, dass dein neuer Computer eine ge-updatete Version eines Package verwendet, welches du für deine Analyse benötigt hast. Dieses ge-updatete Package war allerdings nicht kompatibel mit einem anderen Package, das du ebenfalls für die Analyse benötigt hast. Es bestand also ein “unsichtbares” Abhängigkeitsproblem, was der Grund war, weshalb dein Modell auf dem neuen Computer nicht ge-runt hat! Remote: Das ist z.B. ein Server / eine Maschine, die du nicht lokal bedienst. instanziieren: Das Erzeugen eines Objekts in der objektorientierten Programmierung. Specification: This is the assignment of an initial value for a data object or variable. Dieser Schritt passiert gleich nach einer Instanzierung einer Variablen (siehe vorheriger Listenpunkt). Example in another Context: Plugging in some values, in order to get a prediction from an estimated model. Self: self refereziert den Zustand eines Objektes in Python –&gt; dieser kann durch methods verändert werden. Beispiel: ### Step 1: initialize a Python-Object &#39;x&#39; with the class &#39;LinearRegression&#39; x = linear_model.LinearRegression # important: this will create an instance of the class! ### Step 2: Now that we have specified a model-type (= linear model), we can estimate a linear regression x.fit() # &lt;-- THIS LINE IS KEY: &quot;hier&quot; wird der Zustand des Modells verändert! ### Step 3: Make a prediction x_pred = x.predict(x_test) Superklasse: Begriff, welcher in der “objektorientierten Programmierung” verwendet wird. Damit ist gemeint, dass eine Klasse von einer anderen Klasse abstammt und dessen Eigenschaften “inherited”. Analogie: Das ist dasselbe Konzept, wie ein Parent-Element in HTML, welcher die CSS-Styling-Eigenschaften den Kindern vererbt! 2.4 Data-Types In R oder Python ist es wichtig zu verstehen, dass gewisse Funktionen nur dann funktionieren, wenn die Inputs, die wir in die Funktion eingeben wollen, einen bestimmten Data-Type aufweisen müssen. On the website W3-Schools, I found this extremely good overview of all data-types, which is crucial concept to understand when doing data cleaning. Overview of different Data-Types Data-Types are a key-thing to understand. Otherwise, you won’t be able to apply some algorithms on your dataset! 2.5 Global Variables VS. Local Variables Variables that are created outside of a function are known as global variables. Global variables can be used by everyone, both inside of functions and outside. Example of a global variable: x = &quot;awesome&quot; def myfunc(): print(&quot;Python is &quot; + x) myfunc() ## Python is awesome In contrast, if you create a variable with the same name inside a function, this variable will be local, and can only be used inside the function. The global variable with the same name will remain as it was, global and with the original value. Example of a local variable: x = &quot;awesome&quot; def myfunc(): x = &quot;fantastic&quot; print(&quot;Python is &quot; + x) myfunc() print(&quot;Python is &quot; + x) Output of this: click here 2.6 Python-Basics Kernel: The kernel is the command line for a jupyter notebook. It always runs in the background when you run code in the jupyter notebook environment. You can think of it as the terminal on your mac, only that this time, it is for a jupyter notebook. This is how it looks like: This is the Kernel Variables: Eine Variable wird mittels dem =-Zeichen assigned. If-Else Statements: Die If-Else Statements bestehen aus zwei (oder mehr Bausteinen). “If-clause”: This is an If-Clause 2) &quot;Else-clause&quot;: ### Magic Commands in Python Das sind Tastatur-Shortcuts, wenn du Python bedienst. Es hilft dir, schneller &amp; produktiver beim Codieren zu sein. Mit Hilfe dieser “Magic Commands” in Python, kannst du unglaublich schnell herausfinden kannst, was eine Funktion überhaupt tut UND welche Inputs in eine Funktion gehören. Hier die Liste: shift + Tab → wenn du nicht weisst, was eine Funktion // Method tut Tab → Drücke die Tab-Taste, während sich dein Cursor innerhalb einer Funktion befindet, um eine Übersicht zu allen Inputs der Funktion zu erhalten! xD shift + control + _-Taste → Trennt eine Cell in mehrere, kleinere Cells, je nachdem, wo du den Cursor in der Cell platziert hast. Dieser dient dann als “Flag”, wodurch der Computer weiss: “Aha, hier muss ich die Cell in zwei teilen!” Wieso?: Dieser Keyword-Shortcut ist nützlich für das Debugging einer langen Funktion. Damit siehst du dann z.B. jeweils, wie lange jeweils eine Cell braucht, um ausgeführt zu werden. =) shift + M-Taste → Merge Cells, nachdem du eine Funktion debugged hast (mit dem Key-board-Shortcut shift + control + _) =) 2.6.1 Wörterbuch What is a “Cell”?: This is the green box which you can write your code into, when you use jupyter notebook: This is a Cell What is “the state” of a jupyter notebook?: To answer this question more precisely, look at this youtube video from 3:50-4:30. You can think of it as the internal memory of the jupyter notebook. You can erase the memory by restarting the kernel by clicking on the button I just marked in the image below: - Module: Python-File - PyPA: Python Packaging Authority - PyPI: Python Package Index 2.6.2 Tricks In den folgenden Zeilen Zeige ich dir, wie Programmierung effizienter geht in Python. 2.6.2.1 Swap the values of two variables Am besten schaust du dir dieses Youtube-Video ab 19:45-24:00 2.7 R as your IDE Write Markdown faster: https://thinkr-open.github.io/remedy/ Manage your Short-Cuts (to Code faster): Check out this Youtube-Video, from 2:21-2:51 2.7.1 Pipe-Operator in R What is the Pipe-Operator in R and what does it do? The Pipe-Operator in R looks like this: %&gt;%. It takes in an input and “transports” it into another function to use the input and produces an output. This verbal explanation can be best illustrated via a code-example in R: library(tidyverse) result &lt;- mtcars %&gt;% group_by(cyl) %&gt;% summarise(meanMPG = mean(mpg)) The Keyboard-Shortcut to use the Pipe-Operator would be: Shift + control + M. Quelle: A Guide to the Pipe in R 2.8 Restful API What are Rest APIs? Synonym: Restful APIs Defintion: API stands for Application Programming Interface. It is a way for 2 computers to “talk” // communicate with each other. During the talk, one computer sends a “request”, while the other sends the “response”. How is this possible? In order for you to use an API (e.g. to enable the “talking” between the computers), you need to write code to explicitly “request” data from a server // computer. Most APIs in the world are “restful”, e.g. they follow some set of rules // constraints known as “representational state transfer”, which is the “gold-standard” for API-development since the early 2000’s (invented by Roy Fielding, in its PhD dissertation). Example: If you are a “noob”, you can retrieve the NASA-images of asteroids by looking at their websites. However, you could also use NASA’s “Rest API” to get the data via a .json-file. How it works?: From a architecture point a view, a Rest API, organizes “data entities” // resources into a bunch of unique URLs. Well, technically speaking, they are NOT URLs, but rather “URIs” (aka “uniform resource identifier”), which identifies exactly each data-resource on the server. A client can then retrieve // get the data about a resource by making a request to the server // “endpoint” (over the “http”-protocol). This request has a very specific format, see this youtube-video (ab 1:03). “Stateless” Architecture (key to know): The two parties // computers don’t need to store any information about each other AND every “request-response”-cycle is independent from another “request-response”-cycle. These two characteristics are important, because it leads to an application that is predictable AND reliable. Quelle: Pokemon-API Zusammengefasst in meinen Worten: Ein API sind Befehle, die du gibst, um mit einer Datenbank (z.B. auf einem Server) zu kommunizieren ODER ein Package (in R oder Python) zum runnen zu bringen. Ganz einfach xD Background Client-Server Architecture: most of the applications these days, follow this architecture. Client == App itself == Front-End Server == Back-End Communication between the Sever &amp; the Client (= App) happens via API by using the Http Protocol. Example: if the App wants to access the particular data of a customer, it sends a request to the server via http-protocol. So, when does the Rest API comes into play? –&gt; the Rest API is a standard that established itself in the industry, when communaction between client &amp; server –&gt; these are the CRUD Operations, which are by definition: GET == getting the data from the server POST == creating data PUT == updating data DELETE == deleting data 2.8.0.1 API Example Beispiel eines API in Python? Das Modul // Package sktime verwendet einen ähnlichen API, wie die berühmte Machine-Learning sklearn-Library. Hierbei wäre der Programmierer (= Du) als Forecaster verstanden, welcher via sogannten methods (zum Beispiel die fit-method, um das Modell zu trainieren // estimaten) mit der application (= hier: Python) interagiert. 2.9 More advanced Topics 2.9.1 Data Pipeline Was ist eine Data-Pipeline? After streaming your data (in real time) // downloading your data from a provider, it’s basically a way to automate the process of data cleaning in order to be able to get the plots // models from your “dirty data” in a fraction of the time you would spend, if you would do the data cleaning “by hand” yourself. Youtube-Video: What is a Data-Pipeline? 2.9.2 Fundamental-Daten Was versteht man unter Fundamental-Daten? Fundamental-Daten sind effektiv messbare Daten, die über Datenbanken accessible sind und welche als Proxy - beispielsweise in Regressions-Analysen - verwendet werden können. 2.9.3 Data-Flow Was versteht man unter dem Data Flow? Mit dem Data Flow sollen folgende Fragen beantwortet werden: Welche Daten werden wo geholt &amp; wieso? Wie werden die Daten anschliessend verarbeitet? Was ist der End-Output, nachdem die Daten - beispielsweise - in einem ML-Modell verwedet wurde? Zur Illustration des Data Flows, gab es hierzu im Wissensaustausch auch eine Bild: Beispiel zum Data Flow 2.9.4 Docker Was ist Docker und was ist der Vorteil davon? Docker wird verwendet, um ein Virtual Environment zu bilden, welches - wie ein Container - dir punktgenaue Versionen von bestimmten Packages und Programmiersprachen (Python etc.) liefert. Docker läuft über Open Shift, welches eine Art Management-Programm für Docker ist (so viel ich das verstanden habe…). 2.9.4.1 Definitions Image - An image is an environment that has been built from a series of instructions called a DockerFile. Images can be prebuilt and hosted on DockerHub (similar to how GitHub hosts version controlled software files). The image is needed to run a Docker Container. Container - A container is a virtual environment that combines a Docker Image with software (files) to run an application in a controlled environment (a reproducible software environment created virtually from the Docker Image). DockerHub - An online community for storing and sharing container images. Has Public and Private repositories for image storage. It’s basically a Cloud, like Github. Fazit: DockerFile –&gt; Docker Image (can be pre-built and hosted on the DockerHub-Cloud) –&gt; Docker Container "],["foundations-of-data-science.html", "Chapter 3 Foundations of Data Science 3.1 Allgemeine Roadmap für Data-Science Projekte 3.2 Fragestellungen beantworten 3.3 Die “Kunst des Feature-Engineering” für gute Modellierung | Effizienter Modellieren 3.4 Allgemeine Schwächen von Data-Science 3.5 Statistics-Theory 3.6 Appendix for the Future", " Chapter 3 Foundations of Data Science In meinem Data Scientist Job werde ich häufig auf ähnliche Probleme stossen mit der Zeit. Hier habe ich eine Reihe an Fragen aufgelistet, welche ich fähig sein muss, zu beantworten, wenn ich effizient in meinem Beruf sein will! 3.1 Allgemeine Roadmap für Data-Science Projekte Die meisten Data-Science Projekte werden wie folgt ablaufen: - Define the problem that you are trying to solve. - Get the data you need to solve the problem - Start with &quot;understanding&quot; your data: - What &quot;X&quot;-variables do you have? In which &quot;Einheit&quot;? - What is the Y-Variable? In which &quot;Einheit&quot;? - Make some very BASIC summary-statistics // just screen the data a little bit. - Key: You need to see whether your data is good enough OR garbage. Because then you don&#39;t even need to put efforts into trying to solve the problem! xD - If the data is OK: start with &quot;Literatur-Recherche&quot; - Einlesen ins Thema - Are there researchers that faced similar problems? What methods did they use? - Clean the data - Split the data into training &amp; testing data - Estimate // Train your model (with the training data) - Evaluate your model: - How good did it &quot;learn&quot;? // Performance on the training-data (MAE, MAPE,...) - How good did it &quot;extrapolate&quot;? // Performance on the testing-data (MAE, MAPE...) - Conclusions - If you are happy with your model: Make a &quot;Minimum Viable Product&quot; (MVP): - For example, a dashboard on the web. Der erste und wichtigste Meilenstein ist vermutlich die Frage: ’Sind meine Daten “gut genug”, um das Problem zu lösen oder sind sie einfach nur Müll?\" Wenn du nämlich die obige Frage mit “NEIN” beantwortest, dann brauchst du schon gar nicht mit dem Projekt zu beginnen, denn es wäre ineffizient ;) 3.1.1 Find Data Der erste grosse Schritt bei allen Data-Science Projekten, ist der “Rohstoff” namens Daten! Ohne sie, kannst du jegliche DS-Projekte vergessen! Here is a list of websites, that offer free to use datasets: Kaggle UCI Machine Learning Repository awesome-public-datasets Google Dataset Search https://datasetsearch.research.google.com/ Wenn du deine eigenen Daten sammeln willst, empfehle ich dir auch die Library Beautiful-Soup. Eventuell gibt es auch einen Twitter-, Reddit- oder Youtube-Crawler API, mit denen du wertvolle Daten sammeln könntest! 3.1.2 After having found your Data: the Questions you need to have an answer on BEFORE you start writing your R-Scripts or Jupyter-Notebooks, you first need to think about several key-things &amp; -questions, because this will help you to come to the conclusion: “is my data good or bad?”: - Welche Datenbasis haben wir? - Was ist das Prognoseobjekt? - Welche Metrik hast du in deinen Notebooks verwendet? - Wie ist das Trainings- &amp; Validierungs-Dataset aufgebaut? - Welche Daten sind im Test-Set, welche für den Benchmark verwendet werden? - Welche (bereits bestehende) Prognoseansätze wurden angewendet? - Welche Daten sind effektiv genutzt &amp; welche sind verfügbar? - Etc… Datenbasis // Woher kommen die Daten?: ENTSOE exklusiv. Das ist der Dachverband der TSO (kennen Nachfrage und Angebot) Prognoseobjekt (= y-dach): Stündliche EUR/kWh für Folgetag. Metrik: Zeigt, wie gut das Modell “lernt” (= training error) &amp; “generalisiert” // exrapoliert (= generalization error). Im Notebook von Philipp werden 2 verschiedene Metriken verwendet, nämlich: - Mean Absolute Error, sowie - [Root] Mean Squared Error. Training Dataset: Daten von 2019. Validation Dataset: Random choice of 20% of the hours within this year. Mit welchem true y-value werden die Predictions verglichen? // Was ist der Benchmark? // Test-Set: EFFEX Spotpreise benutzt für Benchmark // true [y-]values. Struktur [der Analyse]: Output: Day ahead Strompreis CH Input: jeweils 24h für ca. 20 Prädiktoren. Reshape: K dimensionen = #Prediktors x 24h Was ist die Daten-Imputation Methode?: Missing Values mit Mittelwerten 3.1.3 After having concluded: “My data is OK!” | Wie geht es weiter? Hier mein Vorgehen, um jedes mögliche DS-Problem zu lösen,. The list below basically starts at the “Literatur-Recherche” point: - Make a list of Papers on the problem (Google Scholar), but only read the &quot;introduction&quot;! - Key here is &quot;intelligent copying&quot;-concept: You need to pick 2-3 papers that are the most relevant to your problem. Start with those. - For any topics that you don&#39;t understand enough: Find Youtube-Videos on the topic(s). Twitter-Threads is also great. Furthermore, give Reddit a try! - Next, choose which libraries are the most apropriate for the problem? - Key here: Your end-result will only be as good as your tools, with which you will be able to tackle the problem. Remember my MA-thesis: the libraries I needed were in Python, but I solved it (very painfully &amp; inefficiently) in R! - Start with the data cleaning. - Estimate your Model. - Evaluate the Model (with CV). - Conclusions? --&gt; Key: Can we do better? - Make a MVP, for example a dashboard. How to find Answers quickly, especially when a Concept is complicated? Aus Erfahrung weiss ich jetzt, dass Youtube bisher immer, die beste Quelle war, um mir etwas schnell &amp; effizient beizubringen. Twitter-Community von Wissenschaftlern sind ebenfalls sehr wertvoll. Als Beispiel wäre dieser Twitter-Post vom Prophet-Gründer. How to read Notebooks from other People? Es geht am Anfang um die Gesamtübersicht und noch nicht um die Details // Eigenheiten im Code oder im Datensatz! Am wichtigsten ist es, dass du diese Fragen zunächst beantworten kannst, wenn du das Notebook liest. 3.2 Fragestellungen beantworten In Data-Science - wie auch in der Wissenschaft - dreht sich alles um die Beantwortung von (relevanten) Fragen. Somit stellt die Datenanalyse schlussendlich nur das Werkzeug, mit welchem man eine bestimmte Frage(stellung) beantworten kann. In diesem Kapitel habe ich anhand von Beispielen versucht, eine Liste zu mit diversen Methoden &amp; konkreten Daten-Features zu erstellen, mit denen man interessante Fragen zu beantworten versucht. 3.2.1 Methoden | Liste an diverser Technologien, um Fragestellungen zu beantworten Difference-in-Differences Geeignet für: Kausaleffekte, wobei hier ein Vergleich zweier praktisch gleicher Gruppen über die Zeit stattfindet, um den Effekt eines “Treatments” (= treated OR not treated) zu messen. 3.3 Die “Kunst des Feature-Engineering” für gute Modellierung | Effizienter Modellieren Was “gute” Modelle von “Schlechten” unterscheidet, ist oftmals, wie du deine X-Variablen für das Modell kallibriest. Das wird in der Data-Science Branche als “Feature Engineering” bezeichnet. Hierbei geht es grundsätzlich um die Approximierung von “abstrakten Grössen” - zum Beispiel die Variable “Talent” bei der Prediction von “Student’s Test-Scores” - mittels tatsächlich zur Verfügung stehende Daten. Der Key-Point, den es hier zu verstehen gilt: “Talent” gibt es zwar schon, aber die Kunst besteht darin, eine echte, messbare Daten-Grösse zu haben, welche “Talent” approximiert. 3.3.1 Geographie Approx. von “Propensity to work in another country” OR “Propensity to buy cheap food abroad” Benötigte Variable: Distance to Boarder Themen: Wurde in Card &amp; Krueger (1992) Diff-in-Diff Methode verwendet 3.3.2 Finance Approx. von “Marktmacht” Benötigte Variable: Marktanteil within a country. Hierfür musst du die Unternehmen der Branchen &amp; deren Umsätze kennen… Berechnung: \\(Marktanteil-Firma_i= \\frac{Umsatz_i}{\\sum_{i=1}^N Umsatz_i}\\) Where to get the Data?: Beispielsweise aus Geschäftsberichten. Themen: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros &amp; Coop (wegen Krieg in der Ukraine, 2022). Approx. von “unternehmerischer Erfolg” Variable: Bruttomargen von Unternehmen. Definition &amp; Kontext: Sie messen, was Unternehmen vom Umsatz für sich behalten, nachdem die Zahlungen an die Zulieferer von Waren abgezogen worden sind. Die Bruttomargen spiegeln im Grossen und Ganzen den Kostenblock, den sich die Unternehmen leisten. Ist er gross, spiegelt sich das auch in höheren Endpreise wider. Laut Brancheninsidern sollte eine gut geführte Supermarktkette mit einer Bruttomarge von 25 Prozent des Umsatzes auskommen, um damit ihre Aufwendungen für Personal, Mieten, Verwaltung, Werbung, Abschreibungen auf Maschinen und Weiteres zu bestreiten. Wichtige Bemerkung: Viele Unternehmen machen aus ihren Bruttomargen ein grosses Geheimnis. publizieren ihre Bruttomargen nur auf Ebene des Gesamtkonzerns. Where to get the Data?: Beispielsweise aus Geschäftsberichten. Themen: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros &amp; Coop (wegen Krieg in der Ukraine, 2022). Approx. für “unternehmerische Effizienz” Variablen: Auf der Seite der Einnahmen: Umsatz pro Quadratmeter an Unternehmensfläche. Berechnung: \\(Einnahmen-Effizienz_{i,j}= \\frac{\\sum_{j=1}^NUmsatz_j}{\\sum_{j=1}^N Quadratmeter_j}\\), wobei i = Firma “i” &amp; j = Filiale “j” Auf der Seite der Ausgaben: Anzahl Mitarbeiter pro Quadratmeter an Unternehmensfläche. Berechnung: \\(Kosten-Effizienz_{i,j}= \\frac{\\sum_{j=1}^NMitarbeiter_j}{\\sum_{j=1}^N Quadratmeter_j}\\), wobei i = Firma “i” &amp; j = Filiale “j” Einheit: z.B. Mitarbeiter pro 1’000m2. Where to get the Data?: Beispielsweise aus Geschäftsberichten. Themen: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros &amp; Coop (wegen Krieg in der Ukraine, 2022). 3.4 Allgemeine Schwächen von Data-Science 3.4.1 Visualisierungen für Erklärungen verwenden Aus eigener Erfahrung habe ich gemerkt, wie schwierig es ist, extrem komplexe Themen innerhalb von wenigen Minuten einem Laien zu erklären. Das fällt jedoch nicht nur mir schwer. Das ist ein allgemeines Problem von der Data. Obwohl das Problem schwierig zu meistern ist, liegt hier jedoch eines der grössten Potentiale: denn häufig werden deine Cheffen Manager sein, die sich nicht gut in Data Science auskennen. Entsprechend ist es sehr wichtig, schnell &amp; effizient die richtigen Erklärungen zu finden, denn diese finanzieren das Projekt. Wenn sie nicht überzeugt sind, dann wird genau deine Karriere als Data Scientist sehr kurz enden! xD Lösung: Hier musst du insbesondere weiterhin auf deine Präsentations-Künste mittels Web-Development beharren. Das wird sich auszahlen! 3.4.2 Nur 20% der Data-Science Projekte haben “Erfolg” | Apropos Geld… Das ist eine absolut wahnsinnige Zahl! Dies zeigt, dass Data Science Projekte relativ riskant sind, aus der Perspektive von Unternehmen. Lösung: Ich bin weiterhin überzeugt, dass das Anbieten einer “Freelancing”-Option für Data-Science eine sehr attraktive Lösung darstellt. So kannst du den Unternehmen genügend Flexibilität anbieten (indem du für weniger Lohn arbeitest) und gleichzeitig gewinnst Erfahrung. 3.5 Statistics-Theory 3.5.1 P-Hacking Was ist p-hacking? In der Statistik gibt es den p-Wert ein: Man nimmt an die Hypothese sei wahr und berechnet dann die Wahrscheinlichkeit, dass die beobachtete Statistik mindestens so extrem ausfallen würde (für die Gegner von Wischi-Waschi hier die Wikipedia-Definition). Falls diese Wahrscheinlichkeit unter 5% liegt, dann sei das Resultat “statistisch signifikant” (yay!) und die Nullhypothese kann verworfen werden, was oftmals die Absicht ist. Das Problem ist nur: Hypothesen gibt es viele und z.T. auch recht ähnliche. Wenn man genug Hypothesen aufstellt - vor allem, nachdem man sich die Daten angeschaut hat - dann ist es durchaus möglich, dass man ein statistisch signifikantes Resultat erhält, unabhängig davon, ob das Resultat tatsächlich auch stimmt. Das nennt man p-Hacking. Es kommt häufig in der Forschung vor, aber es kommt sicher auch in der SBB vor (dennoch hier eine +1 für Hypothesen-basiertes arbeiten!). Wie einfach man in die “falsche Signifikanz Falle” tappen kann, wird hübsch in dieser Gallerie falscher Korrelationen illustriert. 3.6 Appendix for the Future Welche Zeitperiode sind am geeignetsten für Zeitreihenanalysen mit Machine Learning? "],["python-libraries.html", "Chapter 4 Python Libraries 4.1 List of Useful Python-Libraries 4.2 Python-Mustercodes für Data-Science 4.3 Basic Python 4.4 Pandas Library 4.5 Statsmodels 4.6 Time-Series Forecasting 4.7 Tensorflow", " Chapter 4 Python Libraries 4.1 List of Useful Python-Libraries numpy, for mathematical operations. pandas, for doing data analysis, for example: Read &amp; Save data. Drop missing values. Feature Engineering. statsmodels, for regression analysis in “econometrics-style”. Run a linear model (OLS). Run a binary-regression (logit or probit). Run a choice-model. sktime, for time-series analysis inkl. machine-learning. sklearn, for machine-learning in “computer-science style”. matplotlib, for data-visualization. seaborn, for data-visualization. datetime, for dealing with dates. beautifulsoup, for getting data. PyTorch, for deeplearning. TensorFlow, for deeplearning. darts, for dashboarding. 4.2 Python-Mustercodes für Data-Science Für eine übersichtliche Zusammenfassung, welche die Python-Basics bis hin zu Visualisierungen abdeckt, empfehle ich dir dieses Juypiter Notebook. 4.3 Basic Python In this part, I will give you some useful Python-Code, which can be helpful in any project. 4.3.1 Put a time-stamp on each Python-Cell The ipython-autotime library is particularly useful to get an overview of how long each Python-Cell takes to be executed. You can even use it for debugging in combination with the datetime-library in order to use print()-statements that gives you the run-time of - for example - each model-fiting iteration (in a loop). 4.3.1.1 Installation To install this wonderful option pip install ipython-autotime 4.3.1.2 Activation It uses a Python Magic-Command (= note the %) in order to be able to use this library: %load_ext autotime 4.4 Pandas Library 4.4.1 Padas DataFrame This is the overview of a Pandas DataFrame 4.4.2 How to display all the columns? pd.set_option('display.max_columns', None) 4.4.3 Drop Missing-values? df = df.dropna() # drop the missing values 4.4.4 loc- VS. iloc-Selection loc &amp; iloc are the “accessor operators” in the pandas-library. With these,, you can select rows and columns in your dataframe: For loc: Based on the name of the rows (= row-label // row-index) and columns. For iloc: Based on the position // where the specific rows &amp; columns are within the dataset. 4.4.4.1 Vorsicht: different Slicing for loc VS. iloc! When using iloc, the range 0:5 will select entries 0,...,4 that is: it indexes EXCLUSIVELY. On the other hand, loc, meanwhile, indexes INCLUSIVELY. So the same range 0:5 will select entries 0,...,5!!! Hence, if you want the SAME output with loc and iloc, you simply need to slightly change the range()-function. Example: ## Möglichkeit 1: with &#39;iloc&#39; iloc_test = dd.iloc[0:5,0] # row-position == 0:5 --&gt; first 5 rows; EXCLUDES &#39;5&#39; from the range &quot;0,1,2,3,4,5&quot; # --&gt; hence range(0:5) results in --&gt; &quot;0,1,2,3,4&quot; # column-position == 0 --&gt; 1st row --&gt; remember: indexing in # Python starts at &#39;0&#39;! # IMPORTANT: &#39;iloc&#39; uses the &#39;Python stdlib&#39; indexing scheme, where the first element of the range is # included and the last one excluded. So 0:5 will select entries 0,...,4 (= these are the first *5* # entries!!). ## Möglichkeit 2: to get the SAME output with &#39;loc&#39;, we need a slightly DIFFERENT range! loc_test = dd.loc[0:4,&#39;order_id&#39;] # row-position == 0:4 --&gt; first 5 rows; INCLUDES &#39;4&#39; # --&gt; hence range(0:4) results in --&gt; &quot;0,1,2,3,4&quot; ## check if the output are the same, even though &quot;range()&quot; has slightly different inputs? --&gt; yes! print(iloc_test) print(loc_test) 4.4.5 Drop a Column with the pop()-Method This method can be used, when you split your dataset into y (= the label) and the X-variables. Pop: “Pop” is simply a word that means “to drop something”. For example, df.pop('date') means that I am dropping the column called “data” from a dataframe called “df”. Fancily, if you write df.pop('date') and save it into another variable, then you can only select this dropped column from the ddset. Example: See the Pandas-Documentation 4.4.6 Convert into “Date-Time”-format with the to_datetime()-Method This method can be used when you need to convert a “Date”-Column into the correct Date-Time-Format in order to work with time-series data. df[&#39;Date-Time&#39;] = pd.to_datetime(df[&#39;Date-Time&#39;], format=&#39;%Y-%m-%d %H:%M:%S&#39;) df[&#39;Date-Time&#39;] # check, if it worked? 4.4.7 Umgekehrt: Use the strftime()-Method to convert a Date-Column back into a “String”-Format df[&#39;Date-Time&#39;] = df[&#39;Date-Time&#39;].apply(lambda x: x.strftime(&#39;%Y-%m-%d %H:%M:%S%z&#39;)) Why would we do this? Sometimes, there will be the problem, that we want to merge different time-series together, but they have different Date-Time-Formats to begin with! Hence, BEFORE we apply the UTC-universal Date-Time-Format, we will need to bring every separate data-frame into the same Date-Time-Format. Since Python is really good for handling transformations on the str-format, we can use this to our advantages and: Bring the Date-Time-String into the way we want it to be on all our separate Date-Time-Columns across those different datasets. And then, we will be able to convert them back into the UTC-Format! =) In order to see, how to manipulate strings, I strongly recommend you to look at the sub-chapter “String-Manipulation”. 4.5 Statsmodels This library should be used when you need to: Estimate an OLS-regression, Estimate a binary regression, like: A linear probability model, A logit regression OR, A probit regression. Estimate a choice model, such as: A multinomial logit model, A probit model OR, A logit model. Estimate a time-series regression, 4.5.1 Useful Nice-to-Knows Use statsmodels in combination with the patsy-library. For instance, if we have: some variable y, and we want to regress it against some other variables x, a, b, and the interaction of a and b, then we simply write: patsy.dmatrices(&quot;y ~ x + a + b + a:b&quot;, data) 4.5.2 Define the y-variable &amp; X-variables y, X = dmatrices(&#39;Lottery ~ Literacy + Wealth + Region&#39;, data=df, return_type=&#39;dataframe&#39;) Notice that dmatrices-function above does the following things: It creates Dummies: Split the categorical-variable Region into a set of indicator-variables (= Dummy-Variables). Added a constant (= Intercept-Column) to the exogenous regressors matrix. Returned pandas-DataFrames instead of simple numpy-arrays. 4.5.3 Estimate a Model model = sm.OLS(y, X) # Step 1) Choose the model type --&gt; here, we choose the model to be of the class &quot;OLS&quot; (= instantiate the model: which type of model do you want, for example OLS, GLM, RLM etc...?) results = model.fit() # Step 2: Fit the model print(results.summary()) # Step 3: Summarize model 4.5.3.1 Extract the \\(\\hat\\beta\\)-Parameters results.params 4.6 Time-Series Forecasting Here, I will explore different libraries that will allow you to make forecasts into the future. 4.6.1 Working with Date-Columns Since you will - most probably - often work with time-series data, you MUST learn how to handle this data-type well! For any project with time-series data, the following questions about your Date-Time-Column need to be answered: - What is the current &#39;dtype&#39; of your &#39;Date-Time&#39;-Column? - If you work with multiple time-series: are all time-series already in the UTC-format? If NOT, then you better do it, otherwise you have the wrong hours across different time-zones! 4.6.2 Facebook Prophet Library Aus einer didaktischen Perspektive ist die offizielle Dokumentation der Prophet-Library wunderbar, um die Unterschiede zwischen R &amp; Python (Objekt-Orientierte Programmierung) zu entdecken. Link zur offiziellen Dokumentation: https://facebook.github.io/prophet/docs/quick_start.html 4.6.3 SkLearn Library In some functions, you will need to set the option random_state. If you want reproducible results, how do you need to handle random_state? Just set random_state to an integer. Dieser integer kann beliebig gewählt werden, das ist SkLearn egal! 4.6.4 SkTime Library 4.6.4.1 SkTime API Überischt zum API in sktime? Grundsätzlich verwendet sktime fit-, predict-, und transform-class-methods. Um zu verstehen, was dies genau bedeutet, hier eine globalere Übersicht mit Hilfe einer Visualisierung veranschaulicht: API of sktime For estimator classes (a.k.a. model classes), sktime provides: a fit-method (= goal: training // estimating the model) and a predict-method (= goal: generate new predictions). For transformer classes, sktime provides: various fit-methods, various transform-methods to transform data that comes in series. 4.6.4.2 Reduction What does it mean: “Reduction is composable”? Ein Synonym wäre: “addieren” → E.g. you can split a difficult task into a bunch of smaller tasks (= reduction) and “add” them together to solve the bigger task at the end (= “reduction is composable”). 4.6.4.3 Forecasting-Horizon Welche Arten von Forecasting-Horizon gibt es? Es gibt einen relativen forecasting-horizon (FH), dh “relativ” zur Trainings-Periode ist hier gemeint! Es gibt auch einen absoluten FH. Hier werden die absolute time-points in the future verwendet für die Prediction. Quelle: Im “examples”-Repository auf Github zur Sktime-Library &gt; 01_forecasting.ipynd 4.6.4.4 Train-Test Split What do you need for parameters // inputs to write a train-test split function for time series data? You need 3 arguments // inputs for such a function in time series: the data the window length, e.g. how many periods the training dataset should have. the length of the forecasting horizon, e.g. how many periods into the future our validation-period go. Quelle: Youtube-Video ab 46:25 4.6.4.5 Konkurrenz-Libraries für Time-Series What are other useful libraries when working with Time Series in Python? Besides Sktime, there are 5 other libraries in Python which you can use for different purposes: Stop this Video at 5:35 um zu sehen, für welche Zwecke sie sich am besten eignen. 4.6.4.6 Example on Github In the example Repository on Github, was ist die Einheit der y-variable in 01_forecasting.ipynd? Einheit der y-Variable: in Monate 4.7 Tensorflow 4.7.1 Definition PyTorch and TensorFlow are far and away the two most popular Deep Learning frameworks today. While TensorFlow has a reputation for being an industry-focused framework and PyTorch has a reputation for being a research-focused framework. 4.7.2 Tensorflow VS. PyTorch Which one is more popular in the Deep-Learning Community? PyTorch VS. Tensorflow popularity As you can see, the adoption of PyTorch was extremely rapid and, in just a few years, grew from use in just about 7% to use in almost 80% of papers that use either PyTorch or TensorFlow. The researchers find that the majority of authors who used TensorFlow in 2018 migrated to PyTorch in 2019 (55%), while the vast majority of authors who used PyTorch in 2018 stayed with PyTorch 2019 (85%). This data is visualized in the Sankey diagram above, where the left side corresponds to 2018 and the right side to 2019. 4.7.3 Data-Windowing Konzept | Erklärung mittels Beispielen Wie erklärt man einem Computer, dass man gerne eine Vorhersage möchte von 1h, aber NICHT für die darauf folgende Stunde, sondern erst in 24h!? Mittels Data-Windowing lässt sich dieses Problem allgemein lösen: Data-Windowing anhand von 2 Beispielen erklärt. Es geht darum, den richtigen Prognose-Horizont festzulegen. 4.7.4 Wörterbuch Offset: Zeitlicher Versatz zwischen den Input-Variablen \\(X_t\\) und der zu vorhersehenden Variable \\(Y_{t+1}\\). Beispiel, anhand einer “single Prediction”, welche 24h weiter in der Zukunft liegt: Visualisierung des Offsets 4.7.4.1 Synonyme Offset // shift "],["visualization.html", "Chapter 5 Visualization 5.1 Graphs with Python 5.2 Statistics", " Chapter 5 Visualization When you analyze data, it is crucial to communicate with simplicity what evidence you found. Therefore, this chapter will focus on how to bring your evidence to your audience. 5.1 Graphs with Python When you work with Python, there are three main libraries you need to know: seaborn, which can plot extremely beautiful graphs, without much efforts. matplotlib, for more advanced stuff, this is the library you need to master. plotly, for implementations into websites. Noob. 5.1.1 Seaborn 5.1.1.1 Draw a Time-Series (= Line-Chart) A line-chart is often used to visualize a trend in the data over time. import seabron as sns sns.lineplot(data = spotify_data) 5.1.1.2 Draw a Barplot (= Barchart) import seabron as sns sns.barplot(x = data.Racing, y = data.index) 5.1.1.3 Draw a Heatmap import seabron as sns sns.heatmap(data = flight_data, annot = True) 5.1.1.4 Draw a Scatterplot (Streudiagramm // Punktwolke) You can check, whether you have enough variation in your data to predict the dependent variable. 5.1.1.4.1 With 2 variables (incl. regression-line) import seabron as sns sns.scatterplot(x = health_data[&#39;bmi&#39;], y = health_data[&#39;life_expectancy&#39;]) # include a regression-line: sns.regplot(x = health_data[&#39;bmi&#39;], y = health_data[&#39;life_expectancy&#39;]) 5.1.1.4.2 With 3 variables, where 2 are continuous &amp; 1 is categorical import seabron as sns sns.scatterplot(x = health_data[&#39;bmi&#39;], y = health_data[&#39;life_expectancy&#39;], hue = health_data[&#39;smoker&#39;]) # include a regression-line: sns.lmplot(x = health_data[&#39;bmi&#39;], y = health_data[&#39;life_expectancy&#39;], hue = health_data[&#39;smoker&#39;]) # note that it is not the same, as when you used 2 variables! 5.1.1.5 Draw a Histogram A histogram sorts the data, then it is putting the data into intervalls of data (= batches) and finally counts the frequency those batches occur. Useful to check the distribution of the data Why should we use this? For example, in order to see whether our data is normally distributed (= Glockenkurve). By definition - if a variable follows (roughly) a normal distribution - you can standardize the variable. If you want to do Deep Learning, then a transformation like normalisation (OR standardisation?) is required, that’s the reason why we care about the distribution! 5.1.1.5.1 Discrete Version import seabron as sns sns.distplot( a = iris_data[&#39;Petal Length (cm)&#39;], # a column you would like to plot label = &quot;Rose&quot;, kde = False # &quot;Kernel Density Estimate&quot; (kde): always provide this input when creating a histogram ) 5.1.1.5.2 Continuous Version (Kernel Density Estimate) import seabron as sns ### ---- Standard KDE-Plot sns.kdeplot( data = iris_data[&#39;Petal Length (cm)&#39;], shade = True # colors the area below the curve ) ### ---- 2-Dimensional KDE-Plot, when using more than 1 column sns.jointplot( x = iris_data[&#39;Petal Length (cm)&#39;, y = iris_data[&#39;Septal Width (cm)&#39;], kind = &quot;kde&quot; ) 5.1.1.6 Change Style of different Seaborn-Plots Key:Set the new style BEFORE you run your code for the actual graph! import seabron as sns ### ---- Example for changing the colors // styles with a new style &quot;dark&quot;: sns.set_style(&quot;dark&quot;) # Possibile styles to choose from: {&quot;darkgrid&quot;, &quot;whitegrid&quot;, &quot;dark&quot;, &quot;white&quot;, &quot;ticks&quot;} 5.1.2 Matplotlib 5.1.2.1 Set Width &amp; Height of Graph import matplotlib.pyplot as plt plt.figure(figsize=(10,6)) # 1. width, 2. height 5.1.2.2 Set title of Graph import matplotlib.pyplot as plt plt.title(&#39;Hello World&#39;) 5.1.2.3 Set Label of Axes import matplotlib.pyplot as plt plt.ylabel(&#39;Average Gaming Score&#39;) plt.xlabel(&#39;Date&#39;) 5.1.2.4 Force Legend to appear import matplotlib.pyplot as plt plt.legend() 5.2 Statistics 5.2.1 The Normal Distribution Wir wollen die Normal-Verteilung der Grösse aller Menschen abbilden. Dies ist möglich, wenn wir die Durchschnitts-Grösse \\(\\mu\\) &amp; die Standard-Abweichung \\(\\sigma\\) der Menschen-Grösse kennen, z.B.: \\(\\mu\\) = 170cm \\(\\sigma\\) = 10cm Diese Werte müssen nun in die Formel der Normalverteilung (= Funktion) eingesetzt werden: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}*e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\\) Nun müsste man bloss alle einzelne Werte der Grösse (inkl. \\(\\mu\\) &amp; \\(\\sigma\\)), um die Glockenkurve darstellen zu können. "],["improve-yourself-constantly.html", "Chapter 6 Improve Yourself Constantly 6.1 To Share", " Chapter 6 Improve Yourself Constantly Like everything in Software and Informatics: you constantly need to acquire new skills to get the job done. Here, I listed different examples of what you will need to master, in order to improve yourself as a data-scientist: Git Cheat-Sheet? PDF zum Git Cheat-Sheet: klicke hier Twitter crawler via R? Link zur Webseite Monte-Carlo Simulation explained &amp; how to implement in Python? Link zur Webseite How to create a „reproducable example“ in order to post it on Stackoverflow? Link zur Webseite 6.1 To Share Create a Website (Hugo): Link zur Webseite "],["git-versioncontrol.html", "Chapter 7 Git Versioncontrol 7.1 Wörterbuch 7.2 First time using Git &amp; Github 7.3 2 Key concepts in Git 7.4 Working with Branches 7.5 Set-Up an SSH-Key for Github 7.6 Create a .gitignore-file 7.7 Speicherplatz verwendet durch Git-History 7.8 Unstage everything 7.9 Stage only a certain type of file - for example - only .html-files", " Chapter 7 Git Versioncontrol If you work on projects, especially when collaborating in teams, it is very important to use a versioncontrol system, such as - for example - Git. In this chapter, I will give you a list of useful Git-commands, that you will be likely to use. 7.1 Wörterbuch pull request = “Take some changes from a particular branch and bring it into another branch.” Achtung: es ist eine Request, es wurde noch nichts gemerged! Für das brauch es noch merge als zusätzlichen Befehl. fork a repo = “Take someone else’s repo - because you love it 😊 - and put it into your own list of repos, in order to be able to edit it yourself without affectig the original repository of the owner.” commit = save hash = unique identifier in the history of files. A has is a huge string composed of characters (= Buchstaben) &amp; numbers and is used when using a version-control software, such as Git. git add = Der Befehl git add wird zu vielen verschiedenen Zwecken eingesetzt. Man verwendet ihn, um: neue Dateien zur Version-Control hinzuzufügen, Dateien für einen Commit vorzumerken, UND verschiedene andere Dinge – beispielsweise einen Konflikt aus einem Merge als aufgelöst zu kennzeichnen. Leider wird der Befehl git add oft missverstanden. Viele assoziieren damit, dass damit Dateien zum Projekt hinzugefügt werden. Wie Sie aber gerade gelernt haben, wird der Befehl auch noch für viele andere Dinge eingesetzt. Wenn Sie den Befehl git add einsetzen, sollten Sie das eher so sehen, dass Sie damit einen bestimmten Inhalt für den nächsten Commit vormerken (= also Punkt (2) ist vor allem relevant in der obigen Liste. How to tell the original owner you want to merge your changes that you made back into their orignal repo and implement them those changes into their original work // repo? Look at the youtube video from Coding Train ab 9:35-11:50 To see how to refer to issues &amp; bugs in your code directly via your commit-command, look at the youtube video ab 6:35-7:40 and to diretly close issues, because you resolved it, look look at the youtube video ab 7:40-8:55. What is a remote? A remote is a duplicate instance of your repository (on your local computer) that lives somewhere else on a remote server (like Github). 7.2 First time using Git &amp; Github There are specific Git-commands that you need to know, when you begin to start to work with Git and Gibthun for the first time. Note that all these Git-commands need to be typed within the Terminal on your local computer. git config --list = Sehr wichtig, wenn du Git zum ersten Mal via einem neuen Computer runst! Dieser Befehl zeigt dir, welchen Username &amp; Email du aktuell verwendest (schaue bei user.name &amp; user.email, ob es deine Github Anmelde-Daten sind). Es ist key - insbesondere, wenn du neu mit Git beginnst - dass diese Parameter mit deinen Github Anmelde-Daten übereinstimmen! Ansonsten musst du immer git clone machen und die ältere Version in einen “alt”-Ordner tun, was extrem mühsam ist. Wenn du noch keinen user.name hast, dann gebe folgenden Code in die Command-Line ein: git config --global user.name 'Dein_Github_UserName'. Beachte: Schreibe den Namen mit die Anführungszeichen! Wenn du noch keinen user.name hast, dann gebe folgenden Code in die Command-Line ein: git config --global user.name 'deineEmail@email.ch'. Beachte: Schreibe die Email ohne die Anführungszeichen! Check if it worked?: Gebe wieder den Befehl git config --list und schaue bei user.name &amp; user.email, ob dort deine Github Anmelde-Daten übernommen wurden. git push = this is the act of sending to Github. git pull = this is the act of receiving from Github. 7.3 2 Key concepts in Git Before starting to work with Git, you need to understand that there are 2 ways of starting a project: 1) Create a `remote` repository on Github and then `cloning` it - via Git - on your local computer to work from there. 2) Creating a repository `locally` on your computer and then - aftre a few months working on this repository - adding it to Github. Depending on which of those 2 different ways you choose to start a project (create a repo right from the get-go on Github VS. work locally and then - after some time - push everything to Github), the Git-Commands will slightly differ. 7.3.1 Start Project via Github | Remote-Possibility What are the Git-Commands, if you start your project directly by creating a Repo on Github (= possibility 1) above)? git clone https://github.com/joffreymayer/tageb.git == Will clone your remote directory tageb - which is currently on Github on your local computer, which is simpler // more comfy when working on projects =) git status = Assume that you worked on &amp; modified a file on your local computer that you previously had on Github (you cloned the directory with the file in it on your local computer). With the command git status, Git will check whether there is any changes between your local files VS. the files in the remote directory on the Github-Server // -Website. git commit -a -m \"Test comment for a commit\" = If you changed a file locally and you are happy with your results, you will need to make a commit (= save) and add all files (= this is why we have an input // argument -a; the concept of adding will be explored in the chapter below, where you want to put a local repo into Github after a few months) and you also want to document, what exactly you modified, if you need to go back to a previous version of your file (= this is why we have an input // argument -m \"comment is here...\"). git push origin master = If you have done some changes locally on your file, you can now push everything on the Github-Website. git log = see, locally, the history of your git commits. Achtung: When running this command, you might - accidentally - run into a dangerous environment called VIM, which is a terminal-based text-editor. The problem when you are in VIM, is that you might not be able to get out of it. -Solution: To get out of VIM, just type in :q and you will get out of it. git remote -v = This will tell you which URL is the remote on which your repository is hosted. Merke: The URL of your repo is assigned to the variable origin in Git. 7.3.2 Start Project via local computer | Local-Possibility What commands do you need, when you decide - after a few months working locally on your computer - to put everything on Github (= possibility 2)? git init == To get started, you need to go to your repository with your terminal - e.g. set the working directory with cd Joffrey\\dokumente\\my_project) first - and, then, transform your repository to a Git-Repository by just typing git init into your terminal. git add single_file.txt == After you initialized your repository, you will have an empty git-repository. Git will not track the files in your repository (= untracked files), unless you explicitly point them out via the git add command. If you want to add all files quickly // simultanoeusly: git add . Für genauere Theorie // Erklärung dahinter: Siehe Youtube-Video Coding Train ab 2:10-6:03 git commit -m \"Adding a new comment for my commit\": After having pointed out to Git, which files he needs to track, you can do a commit of the changes of the files you modified, like in possibility 1). Achtung, es gibt einen kleinen Unterschied zu possibility 1): das -a (siehe oben) ist verschwunden, weil wir hier add und commit als zwei separate Schritte betrachten. git remote add origin https://github.com/joffreymayer/new-repo = Because our repository is still currently not on Github, we first need to go on the Github Website and create an empty repository. After having done this, you need to tell Git - with the command git remote add origin + copy-pasting the URL “https://github.com/joffreymayer/new-repo” - that this is our local Check if it worked: Type the following into the Terminal git remote -v. It should output the variable name - usually called origin - Note if you want to be fancy: Within git remote add origin, the name origin can be changed to any word you like. This is just the variable name in which your Github-URL will be stored. If there is already a remote with the default name origin but you don’t like the name, you can change the name by: git remote remove origin –&gt; this will delete the remote git remote -v –&gt; just to check if step 1) worked –&gt; should not output anything git remote add noob https:\\\\github.com\\project-1 –&gt; now, re-name the remote and call it noob git push origin master == Finally, you will be able to put all your files into the freshly made remote-repository on Github. git pull origin master == Assume that you did changes remotly on Github but not yet locally on your computer. This does not matter, since you can just enter the command git pull origin master to be able to retrieve the changes that you did remotly on Github onto your local computer =) 7.4 Working with Branches Tutorial for branches? Look at youtube-videos from Coding Train. git branch new_branch == this will create a branch locally on your computer git checkout new_branch == this will tell Git: “ah, he wants to go into the branch called ‘new_branch’”. git branch == this will give you a list of all the branches you ave created locally. Furthermore, it will tell you on which branch you currently work on. How to merge the changes you made on a separate branch to the master-branch (= main branch)? git branch new_branch == this will create a branch called new_branch locally on your computer. git checkout new_branch == You will tell Git: “I now want to work on this newly created branch called ‘new_branch’”. git checkout master == After you are happy with the changes you did in new_branch you will need to prepare for the merging by switching to your main-branch, which is the master-branch. git merge new_branch == Since you currently are in the master-branch, Git will know that you want to merge new_branch into the master-branch. 7.5 Set-Up an SSH-Key for Github Whenever you will work with Git and Github, you will always need to type in your password, whenever you push something to the Github-Cloud. Therefore, you will absolutely need an shh-key, where Github gets the public-key and your computer has the private-key to establish an automatic connection when you work on your local computer and you want to push some changes to the Github-Cloud. In order to activate the connection between your local computer and the Github-Cloud, you will need to type in the following command into your terminal: git remote set-url origin git@github.com:Your_Github_UserName_here/Name_of_Your_Remote_Repository_on_Github.git Follow the instructions on this Youtube-Video from WebDevSimplified from WebDevSimplified in order to set-up your ssh-key =) That was it 😎 7.6 Create a .gitignore-file You will NEVER want to use version-control on every single file in your repository. That’s why you need a .gitignore-file, in which you can write - on each line - which paths can be ignored. Be sure to be in the root of your working directory when you create the .gitignore-file! touch .gitignore Example, of what to write into a .gitignore-file? It can be a path to a repository that you don’t want to stage: /bilder It can be all log-files: **.log 7.7 Speicherplatz verwendet durch Git-History Um den Status zu erhalten, brauchen wir bloss, den folgenden Code einzugeben: git count-objects -v. Der Output wird sein: git count-objects -v count: 7 size: 32 in-pack: 17 packs: 1 size-pack: 4868 prune-packable: 0 garbage: 0 size-garbage: 0 Der size-pack Eintrag gibt die Größe Ihrer Packdateien in Kilobyte an. Somit verwenden Sie fast 5 MB an Speicherplatz. Das ist oftmals zu gross, beispielsweise Bild-Dateien sind hier enthalten, die man nicht - wie zum Beispiel HTML-Files - ständig bearbeitet. Deshalb solltest du diese nicht alles in deinem Repository stagen! 7.8 Unstage everything Dieser Befehl enthält zwar den gefährlichen Befehel rm, doch hier werden keine Files deleted, sondern bloss die komplette git-history gelöscht: git rm --cached -r . Warum will man die gesamte Git-History löschen? Weil man muss nicht alles via Version-Control verwalten. Ein gutes Beispiel sind Bild-Dateien, welche man nicht - im Gegensatz zu HTML- oder CSS-Files - häufig bearbeitet. Ein zweiter Nachteil, wenn alles via Version-Control läuft, ist, dass es viel zu viel Speicher benötigt, obwohl Git Bilder zwar komprimiert, aber diese bleiben trotzdem grosse Dateien… 7.9 Stage only a certain type of file - for example - only .html-files git add *.html "],["econometrics.html", "Chapter 8 Econometrics 8.1 Synonyme: 8.2 English Words Synonyme 8.3 Allgemeines 8.4 Statistics Formulas 8.5 Definitionen 8.6 Different Tests // Vorgehen bei den Tests 8.7 Diverse Berechnungen 8.8 Accept // Reject Null-Hypothesis 8.9 Formulierungen 8.10 Coefficient Interpretation 8.11 Nice to Know 8.12 Allgemeine Regeln 8.13 Ommitted Variable Bias = OVB 8.14 Randomization 8.15 Dummy Variables 8.16 Implication of statistically significant coefficients 8.17 Linear probability model VS. Probit model: comparison 8.18 Heterogeneous treatment effects // interaction terms 8.19 Definitions of “bad controls” 8.20 Fixed effects // FE 8.21 Cross-sectional Data 8.22 Panel Data 8.23 Difference-in-Differences 8.24 Instrumental variables // IV 8.25 Regression discontinuity design // RDD 8.26 Time Series 8.27 Coded Algorithms &amp; R-Functions 8.28 Nützliche Funktionen R: 8.29 Useful Econometrics documents:", " Chapter 8 Econometrics 8.1 Synonyme: Dependent variable // y-variable // regressand // Y-intercept // outcome variable // response variable // label // ground truth X-variable // variable of interest // explanatory variable // regressor // covariates // independent variable // predictor // attribute regression of y on x // regress y on x slope coefficient // beta Controlling for // conditional on // holding fixed // holding constant // “certeris paribus” // statistically account Classical linear regression model // CLRM Linear probability model // LPM Probit model // probit regressions Logit model // logit regressions (Population) data generating process // Population regression function // PRF // „true“ regression (—&gt; very often unknown) Sample regression function // SRF // fitted model // estimated model Range // distribution // histogram Dummy variable // categorical variable Endogeneity // Omitted variable Bias // OVB // violation of assumption 2 (= the non-zero mean assumption) R squared // coefficient of determination RSS // SSR // residual sum of squares // sum of squared residuals Fitted value // predicted value // Y(hat) Validity // internal Validity // external Validity Randomized experiment // Random Assignment study // social experiment // randomized control trial // randomized trial Expected value // predicted value Heterogeneous treatment effects // interaction terms Fixed Effects // FE Difference-in-Differences // Diff-in-Diff // DiD Two stage least squares // 2SLS // Second Stage // IV-Regression NA // Censored Data // Latent Variable —&gt; can be estimated with a Tobit Model or Heckman Two stage Model (where the dependent variable is censored // has NAs, for example wages of people who don’t work cannot be observed // are censored) Observation // Row in R // samples // records (in computer science) Column in R // field (in computer science) Variable // Column in R Finite // Definite Infinite // indefinite Relativ zu // im Verhältnis zu Seasonal ARIMA // SARIMA // ARIMA(p,d,q)(P,D,Q) 8.2 English Words Synonyme Among = across Fixed = not random —&gt; an example would be the true parameters of the population would be fixed // not random, but you don’t know them —&gt; you can only approximate the true parameters by taking a random sample and estimating it’s coefficient to approach the true / fixed coefficient with a random (but estimated) coefficient, which resulted from a sample that the researcher took. With respect to … = in Bezug auf… = Auf der (z.B. klanglichen) Ebene… 8.3 Allgemeines Numerator = Zähler Denominator = Nenner Ditto = ebenfalls A priori = When dealing with ommitted variable bias, you need to ask yourself the question: what are the most likely sources of important omitted variable bias in this regression? Answering such a question requires applying economic theory &amp; expert knowledge, and should occur before you actually runy any regressions; because this step is done before analyzing the data, it is referred to as „a priori“ („before the fact“) reasoning. Non-Linearity in X examples: A regression is not linear in one regressor „x“, if - for example - the „x“ was logarithmic or is an interaction term. Non-linearity in „beta“ // parameters // coefficients: An example of a regression function being non-linear in the parameters // coefficients are: 1) logistic regressions, where the dependent variable is between 0 and 1 (you use an s-shape function that can match all x-values between -infinity to +infinity to a y-value between 0 and 1); or 2) negative exponential growth: Importantly, non-linearity in „beta“ cannot be estimated using OLS, but rather, with an extension of OLS called nonlinear least squareds. Independence assumption when dealing with probabilities &amp; variances: Independence has implications on how you calculate in statistics. For example, it affects the calculation of joint probabilities or variances with 2 random variables: 8.4 Statistics Formulas Formula for covariance: Formula for (population) variance: 8.5 Definitionen Beliefs: VL1, Yanazingawa, S.2 Evidence: VL1, Yanazingawa, S.2 Counterfactual: VL 1, Yanazingawa, S.5 Validity // internal Validity // external Validity: VL 1, Yanazingawa, S.7, S.12 Randomized experiment // Random Assignment study // social experiment // randomized control trial // randomized trial: VL1, Yanazingawa, S.7 Control Group: it mimics the counterfactual: VL1, Yanazingawa, S.7 Treatment Group: VL1, Yanazingawa, S.7 Random Assignment (= part of randomization): VL1, Yanazingawa, S.7 Random Sampling (= part of randomization): VL1, Yanazingawa, S.7 Why is randomization important?: VL1, Yanazingawa, S.7-8 Observational Study: VL1, Yanazingawa, S.10 (wichtig für MA, weil es genau in mein Kontext fällt!) Bivariate Regression: Vl2, Yanazingawa, S.2 Population Regression Funtion // Sample Regression Function: VL2, Yanazingawa, S.3 Dependent variable // y-variable // Regressand // Y-intercept: VL2, Y-intercept, S.3 Independent variable // x-variable // Regressor // Covariate // Explanatory Variable(s): VL2, Y-intercept, S.3 Slope Coefficient // Beta: VL2, Y-intercept, S.3 Interpretation of Slope Coefficient Bivariate Regression: VL2, Yanazingawa, S.4 Interpretation of Slope Coefficient Multivariate(!) Regression: VL2, Yanazingawa, S.10 Interpretation of Intercept Bivariate Regression: VL2, Yanazingawa, S.4 Interpretation of Intercept Multivariate(!) Regression: VL2, Yanazingawa, S.10 Expected Value // Predicted Value: VL2, Yanazingawa, S.4 Interpretation p-value (= it’s a bedingte(!!) W’keit): VL2, Yanazingawa, S.5 What does it mean to have a small Standard-Error?: VL2, Yanazingawa, S.6 Statistical Significance &amp; its Implication: VL2, Yanazingawa, S.7 Understanding “Holding Constant”: VL2, Yanazingawa, S.12 Positive // Negative Bias Table: VL3, Yanazingawa, S.5 Overstated // Understated Concept (Prof does not like the concept…): VL3, Yanazingawa, S.5 Dummy Variable // Categorical Variable: VL4, Yanazingawa, S.1 Dummy Variable Trap: VL4, Yanazingawa, S.3 &amp; S.6 Multicolinearity: VL4, Yanazingawa, S.3 Range // Distribution // Histogram: Ü1, Yanazingawa Scatterplot: Ü1, Yanazingawa Linear probability Model // LPM: VL5, Yanazingawa, S.2 Interpretation of Slope Coefficient in Linear Probability Model // Regression: VL4, Yanazingawa, S.2 Interpretation of Intercept Linear Probability Model // Regression: VL4, Yanazingawa, S.2 Advantages // Disadvantages of LPM: VL5, Yanazingawa, S.4 Advantages // Disadvantages of Logit Model: VL5, Yanazingawa, S.5 Probit Model // Probit Regressions: VL5, Yanazingawa, S.5 Logit Model // Logit Regressions: VL5, Yanazingawa, S.13 Implement a Probit Model with Code: Ü2, Yanazingawa, Question 6 Heterogeneous Treatment Effects // Interaction Terms: VL6, Yanazingawa, S.1, S.4 (first bullet) Coefficient Interpretation of Continuous-Dummy Interaction: VL6, Yanazingawa, S.4-7 Coefficient Interpretation of Dummy-Dummy Interaction: Vl6, Yanazingawa, S.8 Merge different Datasets together: Ü3, Yanazingawa, 5) c) Bad Controls: Ü3, Yanazingawa, 5) d), siehe Korrektur auf meinem gedruckten Problemset! Fixed Effects // FE: VL7, Yanazingawa, S.3 &amp; Zusammenfassung (ZF) auf letzter Seite des Handouts Difference-in-Differences // Diff-in-Diff // DiD: VL8, Yanazingawa, S.1 Parallel Trend Assumption: VL8, Yanazingawa, S.2 Interaction Terms between two FEs: Ü4, Yanazingawa, Aufgabe 3, Part I = a „country x time FE“ interaction controls for country-specific time-trends. However, to use it, you need 2 observations for each country and year (if we assume the time FE to be years…) Make Dummy Variables while having multiple Conditions: for example, the city needs to be between 25 km and 75 km away from the border = Ü4, Yanazingawa, Aufgabe 1, Part II Clustering the Standard Errors: Ü4, Aufgabe 1, Part II Endogeneous Regressor: VL9, Yanazingawa, S.1 Validity of an Instrument: VL9, Yanazingawa, S.1 Exclusion Restriction: VL9, Yanazingawa, S.2 Two Stage Least Squares // 2SLS // Second Stage: VL9, Yanazingawa, S.6 &amp; S.7 First Stage: VL9, Yanazingawa, S.6 Reduced Form: VL9, Yanazingawa, S.6 IV-Regression in R: Ü5, Yanazingawa, Aufgabe 1 a) Calculation of Mean while ignoring NAs: Ü5, Aufgabe 2 a) Standardize a Variable: Ü5, Audgabe 2 a) Interpretation with standardized Variables: Ü5, Audgabe 3 ATE, ATT, ATUT: Ü2, Biroli, Aufgabe 2 Selection Problem: Ü2, Biroli, Aufgabe 3 Histrogram: zeigt die an, wie häufig ein Wert auftaucht. Scatter-Plot: Plot, welcher die Korrelation zwischen zwei Variablen in einem Datensatz aufzeigen. Dichtefunktion: W’keit, einen Wert zwischen “Wart a” und “Wert b” zu erhalten (im Kontext von Zufallsvariablen) = verwendet man bei stetigen Massen, wie Gewicht oder Distanz etc… Cross-Sectional Data: Observation of an economic agent (for example individuals, firms, households etc.) collected at one point in time; you can have agent FEs, but not time FEs! Panel Data: Multiple observations on agents over time (—&gt; you have a time index, as well as an index for individuals!) —&gt; here you can add time FEs and also agent FEs Confounders // Mediator Variables: These are the unobservable factors in your regression // in the error term that correlate with your x-variable of interest, as well as your y-vaiable, thus inducing bias when you estimate your model Note: The term „Mediator“ is used for independent variables that cause a change in the y-variable. Note 2: The term „Moderator“ are used for interaction terms, e.g. that the effect of an x-variable is modified and depends on the second variable. Bivariate Regression: a regression with only one regressor // x-variable Multivariate Regression: a regression with multiple x-variables. 3 Common Reasons for Endogeneity: Simultaneous Causality: x causes a change in y but y also causes a change in x Correlated unobservable Variable: leads to OVB in your coefficient of interest Measurement Error in x: you will have attenuation bias, e.g. bias towards zero Note: There is also the possibility that you have measurement error in y. In this case, we do not have a bias in the coefficient of interest! —&gt; however, the standard error of the coefficient gets bigger if you have this kind of measurement error! Degrees of Freedom (d.o.f.): If we have „N“ observations and „K“ regressors in a regression, then we have: v = N - K „degrees of freedom“ R-Squared: measures the goodness of fit, e.g. how much of the variance of y can be explained by our model // it is the squared correlation between the population regression function and our predicted SRF. Problem with the measure R-Squared: if you rescale the y-axis in the regression, for example with a log-function, it will lead to a reduction of the variability in Y and increase the R squared without improving the regression. Overfitting the Data: This term is another problem that comes along with the measure „R squared“: the thing is, that you always increase the R squared when you add an additional regressors into your regression. With this characterstic, you could think of people adding regressors with almost no correlation to Y but still increasing the R squared without augmenting the precision of the estimated model! Unbalanced Groups (Kontext: RCTs pr Dummies): If the treatment and control group are - on average - different to begin with, then the groups are unbalanced! —&gt; your estimated treatment effect will likely to be biased BLUE = best (= efficient), linear, unbiased estimator Consistency: if you have an infinite number of observations for your sample, then the estimated coefficient will converge towards the true coefficient of the population with a smaller variance for the estimated coefficient distribution —&gt; we then say that the estimated coefficient is „consistent“ —&gt; Note: the statistical theorem that forms the basis for consistency is called the Law of Large Numbers. Attenuation Bias: This is the Bias, when we have measurement error, meaning that the coefficient of interest „beta“ will be biased towards zero —&gt; e.g. if beta &gt; 0, then beta has a negative bias (it is smaller than what it should be) or if beta &lt; 0, then beta has a positive bias (it is bigger than it should be) —&gt; note: If we have a measurement error in one particular „x“ of a multivariate regression, we will have a higher bias (in absolute value), which is worse than in a simple linear regression case —&gt; the bias towards zero gets bigger! (See the formula on page 90 of „sources of bias, Crawford) Partitioned Regression: you do a bivariate regression of the residuals of 1) a regression of x(i) on all other x(-k)] on 2) the residuals of a regression of y on all other x(-k)] Replication Samples // Monte Carlo Simulation: You (randomly!) draw different samples of size „N“ from the same population many times, for example say you draw them „R“ times. Then, you plot the distribution of the average of each sample —&gt; you will see that, by drawing larger sizes of samples „N“ holding fixed the number of draws „R“ —&gt; the average of the distributed average samples will converge to the true average of the population! Serial Correlation: correlation of a variable with a lagged version of itself —&gt; you need this because of the assumption 4 of the CLRM: residuals should not have a serial correlation with one another Selection Bias: when an individual selects himself into the treatment- / control-group, because he has an advantage of doing so. Compliers (in an Instrumental variable setting): You usually have compliers appearing, when an instrument is not assigning people randomly between treatment- and cotrol-groups. In this case, the compliers would be the people who choose to get treated (D=1), when they actually get assigned to treatment (Z=1) and decide not to get treated (D=0), when the instrument is not activated (Z=0). Never Takers (in an Instrumental variable setting): People who choose to never get treated, independently whether the instrument is switched on or off! —&gt; D = 0 (always!), if Z = 1 bzw. Z = 0 Always Takers (in an Instrumental variable setting): People who choose to always get treated, independently whether the instrument is switched on or off! —&gt; D = 1 (always!), if Z = 1 bzw. Z = 0 Synthetic Control: This method creates an appropriately weighted average of control units which best approximates the evolution of the outcome in the treated unit before treatment. —&gt; the key concept of using the synthetic control is that we construct an artificial control group to get a reasonable estimate for our missing counterfactual (= our treatment, which would not have been treated) RDD with a sharp Design: This is an RDD where the probability of treatment at the cutoff point changes from 0 to 1. RDD with a fuzzy Design: This is an RDD where the probability of getting treated changes (also) discontinuously, but less than 100%. Saturated Model: This is a model with a full set of dummies, e.g. in such a model, there is no constant! Type I-Error in statistical Testing: A type I error happens, when we wrongly conclude that the null hypothesis H(0) is false, when it‘s acutally true (Eselsbrücke —&gt; Jemand sagt die Wahrheit, aber alle meinen, er lügt!) The significance-level „alpha“ is representing the type I error. Ideally, we want to minimizes both errors: type I and type II Type II-Error in statistical Testing: A Type II error happens, when we wrongly conclude that the null hypothesis H(0) is true, when it is actually false (Eselsbrücke —&gt; Jemand erzählt eine Lüge, und alle glauben ihm diese!) Often, the type II error is referred to as „beta“: the probability of concluding that H(0) is true when it‘s actually false. Hence, (1-beta) is the probability of concluding that H(0) is false, when it is actually // truly false (probabilities sum up to 1, that‘s why we can do this!). This (1- beta) probability is also called the statistical power! Ideally, we want to minimizes both errors: type I and type II Statistical Power: read the first sub-point of type error II just above! Grundsätzlich gilt: a small sample size gives us little power to reject the null hypothesis (wahrscheinlich, because you have large standard errors // variance(beta-coefficient) is high), whereas a large sample size gives us more statistical power. —&gt; Usually, we want the power to be larger than 50%! —&gt; a power between 80 to 90%, e.g. the probability to reject H(0) when it is TRULY false. By the reverse probability, this would also imply that - with a statistical power of 80-90% - we would have a 10 to 20% probability to accept the H(0), even though it is false! Merk-Regel: A high statistical power is what you want, otherwise you would not conduct a study! Supplementary Analysis: supplementary analysis seeks to shed light on the credibility of the primary analysis (= this is for example a DiD method, or IV, or RDD) —&gt; an example of a supplementary analysis is placebo testing Objective Function: this is a general function that individuals seek to maximize —&gt; example: a lifetime utility function takes on many values. The individuals seek to choose the maximum value (sometimes the minimum value, if it’s a cost-funtion) out of all those values of this „objective function“. Resampling: If you have a classification problem, where you have a y-variable with 9000 cases of „obese“ and 1000 cases of „normal weight“, then you have a problem that „normal weight“ people are under-represented in your sample. Thus, you have imbalanced data and you need to use resampling techniques with - for example - the „imblearn“ library to have a dataset that is equally distributed, e.g. 1000 cases of „normal weight“ and 1000 cases of „obese“. A staggered Treatment (first time I heard of it was in the context of DiD): This means that - for example - an individual can choose to get treated, but once it gets treated, he cannot get out of the treatment (example: COVID vaccines are like that). 8.6 Different Tests // Vorgehen bei den Tests Check if random Assignment // Randomization into Treatment- &amp; Control-Group was successful: VL1, Yanazingawa, S. Placebo Testing: this test involves demonstrating that your effect does not exist when it really „should not“ exist —&gt; you pick a period where no treatment occured and try to see if your treatment group really did not react to „no treatment“ in this particular period! 8.7 Diverse Berechnungen Review of Hypothesis-Testing // Testing differences in Means: Vl1, Yanazingawa, S.9 &amp; S.11 Hypothesis testing “no effect” (= H0) VS. “there is an effect”: VL2, Yanazingawa, S.5 Bias = short Regression Coeff. - long Regression Coeff. = beta(2)*gamma [= corr(Y, omitted) * corr(X, omitted)] 8.8 Accept // Reject Null-Hypothesis t-statistic: beidseitiger Test: if t-stat (im Betrag) &gt; crit.-value –&gt; reject the null Note: we use Z(1- [alpha/2]) as the critical value, since we have a two-sided test. rechtsseitiger Test: if t-stat &gt; crit.-value –&gt; reject the null Note: use Z(1-alpha) as the crit value, since we have a one-sided test. linksseitiger Test: if t-stat &lt; critical value –&gt; reject the null Note: use Z(1-alpha) as the crit value, since we have a one-sided test. Important general fact about hypothesis: Just because you cannot reject a null-hypothesis does not mean that the null-hypothesis is true. It just means that you don’t have enough empirical evidence to prove that the alternative-hypothesis is true. p-value: Definition: Gives the probability that we would get our „sample-mean“ (= the mean that you just calculated from the data // sample you have drawn) IF the Null-hypothesis was true! Interpretation: This implies - when we have a very small p-value - that - GIVEN our sample - there is very small probability that we would get this sample mean if the nullhypothesis was true. —&gt; this is a good result for a researcher, since he / she seeks to reject the null-hypothesis! beidseitiger Test: if p-value &lt; alpha –&gt; reject the null rechtsseitiger Test: linksseitiger Test: Types of Errors in statistical Testing: 8.9 Formulierungen For “correlation”: “is correlated with”. “is associated with”. For “causation”: “lead to…” “has an effect on..” In Regressions: “hold constant” “certeris paribus” “holding fixed” “controlling for” “conditional on” “statistically account for” “Y is a function of X”: y(x) y = f(x) y depends on x y as a function of x 8.10 Coefficient Interpretation Lin-Lin: Lin-Log: a 1% increase in X will increase // decrase Y by (beta/100) units! Log-Lin: a unit change in X will increase // decrease Y by beta *100 percent! Log-Log: a 1% increase in X will increase // decrease Y by beta % Log-Dummy (dh logarithmierter dummy?): Here you have to be very careful (Fallunterscheidung!): If your coefficient on the dummy is very small, then you can do the normal „Lin-Log“ interpretation, it will be approximately correct. However, if your coefficient on the dummy is big, then you need to transform your coefficient before you do the interpretation! The formula would be: exp^(estimated beta) -1 —&gt; if you multiply this result by 100 then you get the correct %-units and you can procede to do the normal „Lin-Log“ interpretation! Coefficient Interpretation (multivariate Case): \"Beta 1 tells us the average change (, e.g. increase // decrease) in Y associated with a one-unit-change in X(1), holding constant X(2) and all other variables! Coefficient Interpretation (y-variable in Prozent): \"A unitary increase in X is associated with a change in Y of XYZ Percentage Points(!!!) Coefficient Interpretation (x-variable in percent): \"An 1 percentage point(!!) increase (= unitary increase when x-var. is in percent) is associated with a XY unitary change in y. Coefficient Interpretation of a Linear Probability Model: \"Being 1 year older (= x-variable is age [in years]) increases the probability of getting married. Pay attention: the increase is in percentage points (-&gt; y-variable is a dummy!!! [–&gt; dh the slope coefficient is the change in the probability in a LPM! –&gt; dh you need to multiply the number of the coefficient by 100 to get the result in the change in probability (in percentage points of course…)!]) Coefficient Interpretation of a Probit Model: you cannot interpret the magnitude (= Grösse des coeff.) directly, you need to transform it first (with a complicated formula for the s-shape function)! However, the sign AND the statistical significance can be directly interpreted from the coefficient of a probit model! Coefficient Interpretation of a dummy*continuous Interaction: for example the gender-wage gap –&gt; returns to education can vary depending if you are a man or a female. Hence, the coefficient represents the difference in slopes (for education, e.g. the continuous variable) for males and females. Coefficient Interpretation of a Dummy-Dummy Interaction Term (hier: female * black // gender * enthnicity): when you have 2 dummies, you have - in total - 4 different “states of the world” // 4 different possibilities. Hence, you have 2 different possibilities to do the coefficient interpretation, which is either the differences between black and white males // females, or the differences between white male and females // black male and females. –&gt; these two interpretations are equivalent, but often one is more interesting than the other! –&gt; very important: the dummy*dummy-coefficient = Diff-in-Diff estimator!! Coefficient Interpretation while “holding constant” FEs: Within(!!) districts, full RTML reception is associated with a XY unitary change in genocide cases, compared to zero reception. –&gt; Alternative: “holding constant all factors that vary across districts (= FEs)” Coefficient Interpretation with standardized Measurements in X and in Y: If corruption increases by one standard deviation, then child mortality will rise by 0.6259 standard deviations or 62.59% of a standard deviation of child mortality. 8.11 Nice to Know Why do we need statistics in econometrics? -&gt; Because it is the statistical theory that allows you to make statistical inference from your sample to your underlying population! Random VS. Non-Randomness of Mean &amp; Variance // Features: Features of the population are fixed, dh non-random (but unknown!), whereas features of the sample are random (but known!) —&gt; e.g. if you draw another sample from the population, it is very likely that you get different numbers than in your other sample! If you want to see, if an estimator is economically meaningful: This is done by looking at the magnitude of the coefficient. How to calculate the magnitude of a coefficient // whether to know if the effect is big or not? —&gt; formula: magnitude = coeff. of interest / average estimated Y —&gt; make a summary-statistic of your regression to find out this average estimated Y! Law of large numbers: The sample mean converges to the population mean as the sample size „N“ gets large. Bootstrapping: when you pick a random sample out of a population, you probably will get a different number than with another sample (of the exact same size &amp; exact same population). This exact method can be done with „Bootstrapping“ —&gt; the goal is to see, if your analysis gets completely different (—&gt; this would be bad…) or stays approximately the same (this is the outcome you want…). The 5 assumptions in a regression model that must be true (otherwise we get a biased estimator): The PRF is assumed to take a linear form (—&gt; that‘s why you need to sometimes take the logarithm of a non-linear relationship!) Key assumption for causal interpretation of the coefficient: Mean zero Error —&gt; the error term has an expected value of zero —&gt; E(error/X) = 0 implies that cov(error, X) = 0 —&gt; in other words: that there is no correlation between the coefficient of interest and „everthing else“ // all unobservables in a regression Homoskedasticity —&gt; dh the variance of the residuals should always stay the same —&gt; how to see if A3 is satisfied? —&gt; look at a scatterplot of the residuals (on y-axis) plotted against a regressor: you want to see „picasso on drugs“, e.g. a variation in the residuals on the y-axis that stays constant with increasing „x“ A4: No correlation between the lagged residuals —&gt; how to see if A4 is satisfied? —&gt; use the Autocorrelation-function in R to see if the residuals are correlated with each other (this is what you don‘t want!)- A5: Normality —&gt; residuals should be normally distributed —&gt; how to see if A5 is satisfied? —&gt; plot the density function of the residuals, it should look like a normal-distribution! If not, then the assumption is violated! What happens, if all the above 5 assumptions are satisfied? The coefficient of interest will be unbiased (= Unbiasedness) Efficiency —&gt; e.g. the variance of the estimated coefficient is the smallest (compared to non-OLS estimation), which is a good thing, because we will always have an estimated coefficient that will be the closest guess to the true beta-coefficient! Our estimated coefficient will be normally distributed (good for hypothesis testing etc…) What happens if A1-A5 are violated and how to fix it? Assumption 1 (= Linearity): This assumption is most certainly violated (the true PRF is very rarely linear) —&gt; fixing it is not such a big deal —&gt; either you can add non-linearity in x: log-transform your regressor Make a polynomial for x (often used in RDD) OR add interaction terms or add non-linearity in beta: Using nonlinear Least Squares (= these are non-linear models —&gt; look it up in a fat textbook!) Assumption 2 (= non-zero mean): This will lead to OVB // your regressor is endogeneous! —&gt; to fix it, you need to control for the unobservable factor OR to use some fancy methods like Fixed effects, IV, RDD or RCT that allows you to control for the unobserved factor! Assumption 3 (homoskedasticity): if your variance of the error term is heteroskedastic, your assumption 3 is violated —&gt; use a statistical test to see if your variance is homo- or heteroskedastic! —&gt; it‘s not a big deal if this assumption 3 is violated, because the estimated coefficient of interest will still be unbiased! But the standard errors will be wrong though —&gt; to fix it, you can tell your statistical software to account for heteroscedasticity! —&gt; when you account for heteroscedasticity, you will have to use so called „robust standard errors“! Assumption 4 (correlation between error terms is zero): if this assumption gets violated, it‘s also not a big deal, because (like A3) the estimated coefficient of interest will also still be unbiased! But the standard errors will again be wrong though! To fix this, you can tell your statistical software to cluster the standard errors. Assumption 5 (normally distributed errors): very uikely that this assumption is violated, because we have the very powerful „Central Limit theorem“ that backs it up! —&gt; prof crawford did not tell us how to fix it, because it is very rarely violated! Under which of the above assumptions is an estimated coefficient BLUE? —&gt; under the Assumptions 1-4 the estimated beta-coefficient will be BLUE! Why do we take correlations instead of covariances? —&gt; because correlations have no units (only a number between -1 and +1), while covariances can have strange units like „hours-gradepoints“ xD If you include an irrelevant variable in the regression, will there be a bias on your estimated coefficient of interest? —&gt; no, you coefficient stays unbiased, but the coefficient will be inefficient (not the lowest possible variance(beta_hat)) —&gt; if you are not sure whether to include a variable or not: better to include it, rather than ommit it (otherwise OVB in the worst case!) Dertermine the sign of bias (if you have OVB): where beta(2) is the correlation between the ommitted variable and the y-variable! Another formula to dertermine the sign of the bias: bias = short regression coeff. - long regression coeff. “Without loss of generality”? —&gt; it means that a statement // formula is always true! Check if an instrument is „strong“ or „weak“: —&gt; you can do an F-Test: if the instrument has an F-value that exceeds 10, you have approximately a strong instrument. If you only have one instrument, you can use the t-test. If the value exceeds 3.16, then you also have a „strong“ instrument (rule of thumb). When is the usage of the synthetic-control method optimal? —&gt; If you have a single unit that is treated (for example a country) and many other units that are not treated, all of which could be a control, but none of which is a perfect one. 8.12 Allgemeine Regeln “Larger sample sizes yield to smaller standard errors and thus less uncertainty // narrower confidence intervalls // preciser estimates.” “Larger residuals (= Differenz zwischen durchschnittliches Y und tatsächliches Y = noisier data) yield to larger standard errors and thus more uncertainty.” “Regress Y on X(1), X(2)…X(k)” “We regress Y on X(1), X(2), … X(k)” “The impact of X on Y…” 8.13 Ommitted Variable Bias = OVB Condition for ommitted variable bias: corr(Y, omm. var.) &gt; // &lt; 0 AND corr(X, omm. var.) &gt; // &lt; 0 –&gt; sign of the OVB Bias = short Regression Coeff. - long Regression Coeff. = beta(2)*gamma [= corr(Y, omitted) * corr(X, omitted)] Note: if beta(2) = 0 OR if gamma = 0, there is no bias! –&gt; the higher the magnitude of the bias, the less precise our sample regression funtion (SRF) relative to our population regression function (PRF), thus we have lower internal validity (dh likelyhood of estimating the true causal effect is low, since we have an unprecise SRF) 8.14 Randomization If you don’t randomize, there will be a selection problem, e.g. mean untreated outcomes will diﬀer from the mean of the treated outcomes. By randomizing, you will make it impossible for people to self-select in or out of a treatment! Implementation of “randomization” can be achieved by - for example - tossing a (fair) coin. –&gt; this is difficult to implement because not all people want // need a treatment to begin with (= there are often debates where such “randomized experiments” are seen as unethical!) 8.15 Dummy Variables You can also use other numbers than 0 or 1 to define a dummy. However, 0 and 1 is easier to interpret. If you estimate a model without(!!) a constant, then you can include all dummies. However, you cannot add all dummies from a category (for example you cannot add female &amp; male into one regression, when these two are the only gender-dummies), otherwise you will run into a multicolinearity problem. 8.16 Implication of statistically significant coefficients If a coefficient is statistically significant, it means that we have strong evidence for an association between X and Y! (very relevant for my Master-Thesis!) –&gt; thus, if you don’t find a statistically significant coefficient, you don’t have, you will never have enough evidence for a causal effect! 8.17 Linear probability model VS. Probit model: comparison A good rule-of-thumb is that the probit model’s prediction are similar to those of the linear probability model near the sample mean of X but can be very different far from the sample mean! 8.18 Heterogeneous treatment effects // interaction terms The three categories of different heterogeneous treatment effects are: Continuous * Dummy Dummy * Dummy Continuous * Continuous 8.19 Definitions of “bad controls” If the treatment can strongly influence some of your variables you control for, they are “bad controls”! 8.20 Fixed effects // FE Definition: By using fixed effects, one controlls for all the factors that differ across(!) groups, like districts (if you use for example “district fixed effects”) that are time-invariant!Therefore, any differences that vary across districts (and don’t change over time within a particular district) are controlled for. Instead, the regression exploits variation of the X-variable that vary within groups. Note: when you deal with cross-sectional data and use FEs, you controll for all the factors that differ across(!) groups. You cannot include time FE in a cross-section, since you only have observations for a particular point in time (not like panel data, where you have multiple observation for an agent over time, and thus there you can include time FE). Intuition, why you controll for all factors that vary across districts: in a regression, you want a counterfactual that - ideally - only differs from your population of interest by the treatment. FE is a powerful tool to construct a better counterfactual, because it allows you to group the data and thus make comparisons within those different groups! What does FEs not control for? –&gt; However, note that you need to control for all factors which vary within a district if you use FEs! Key requirement to use FEs: have multiple observations within a group! However, be aware that there will ALWAYS be other unobserved factors that could explain your results. The question is how important those other factors are, and whether they are correlated with the variable of interest (such as RTML reception in Rwanda example). Intuition for Time FEs: schlecht erklärt in Handouts, aber ich habe eine gute Visualisierung –&gt; stelle dir zwei benachbarte Regionen vor: ZH und Aargau –&gt; wenn ich jetzt Time-FEs habe in einer Regression, kontrolliere ich für folgendes: Alle Faktoren, die sich über die Zeit verändern ABER beide Regionen GLEICH STARK “hitten” (–&gt; zum Beispiel BIP-Growth Rates der SCHWEIZ oder global economic conditions, welche sich über die Zeit verändern, aber wahrscheinlich diese beiden Nachbar-Regionen gleich stark beeinflussen). Für was hier jedoch NICHT kontrolliert wird, sind zum Beispiel Faktoren wie REGIONALE BIP-Growthrates // REGIONALE crime rates // ein Meteoriteinschlag in Zürich, aber NICHT in Aargau. Deine Aufgabe als Forscher ist nun herauszufinden: welche dieser Faktoren für dein Modell jetzt relevant? –&gt; folglich weisst du auch, was die Faktoren sind, die durch Time FEs nicht kontrolliert werden und somit kannst du - hoffentlich - dafür kontrollieren! Intuition for Region FEs: schlecht erklärt in Handouts, aber ich habe eine gute Visualisierung: wie vorher: zwei Nachbar-Regionen, Zürich und Aargau –&gt; nun hat man region FEs in Regression. Diese kontrollieren für: Alle Faktoren, die sich zwischen Zürich und Aargau unterscheiden ABER über die Zeit sich nicht verändern // ZEITINVARIABEL sind (–&gt; zum Beispiel ein SCHWEIZER Gesetz, das nur auf den Aargau abzielt und welches sich über die (vom Forscher betrachtete // fixierte) Zeitperiode NICHT verändert hat oder die unterschiedliche Geographie der beiden Regionen (wird sich wohl kaum verändern über die betrachtete Zeit) oder meistens auch Bevölkerungszusammensetzung (unterscheiden sich zwischen ZH und AR, aber bleiben meist konstant über die Betrachtungsperiode). Indexing coefficients: when you use FEs, the index for the coefficients are the groups. —&gt; example: when you use time-FEs, the index will be time. Or, if you use region-FE, the index will be regions Usage of causal language: you can use causal language when interpreting the FEs coefficients if you are convinced that the Fixed effects have eliminated the main threats to internal validity! Controlling for Time FEs, how to make a coefficient interpretation? –&gt; when interpreting a coefficient while using Time FEs: On average and controlling for time trends(!!), a unitary increase in X is associated with an YZ-change in Y. 8.21 Cross-sectional Data Definition: Cross-sectional Data is data from one moment in time. Conditions to use FE for cross-sectional data: you can use FEs, if your data: contains groups AND if your groups have at least 2 observations 8.22 Panel Data Definition: Observing outcomes in the same units // of the same individuals // of the same states (etc.) in multiple moments in time (years). –&gt; note: you observe for example the beer-tax over multiple periods of times for many different states, e.g. we can do a cross-section analysis &amp; analysis over time! Fixed Effects with Panel Data: panel data allows us to: You can do Time Fixed effects, which control for all factors that differ over time but are constant across units, for example globalisation or price level –&gt; both vary over time but are the same across states // units You can do state fixed effects, e.g. you control for factors that differ across states but stay the same over time (= e.g. needs to be time invariant factors within the state // not change over time within the state). 8.23 Difference-in-Differences Key assumption that needs to be satisfied with DiD: parallel trend assumption –&gt; this means: in the absence of treatment(!) // policy change, the treatment group would have had the same mean change(!!) in outcomes (X) as the control group Wichtige Bemerkungen: The parallel assumtion cannot be tested, because we NEVER observe the counterfactual! BUT what you can do is to observe multiple data-points over time BEFORE treatment and look if the treatment- and control-group have similar trends –&gt; then you can look if they move in the same direction (this is what you want to have a good DiD). This assumption does not require the treatment group to have the same level(!!) of Y for the treatment and the control group, either before or after, the policy change! If the parallel trend assumption is satisfied, then the DiD technique allows you to control for variables that differ across the groups but are constant over time (within the group –&gt; for example district FEs) and also allows you to control for variables that differ over time but are contstant across the groups (within the determined time period –&gt; e.g. time FEs) Examples: other policies that don’t change over this period but vary across the districts (abgedeckt durch district FEs); global policies, that affect both regions the same (time FEs absorbs that); one example that you would absolutely need to control for in a Diff-in-Diff in the Card &amp; Kruger paper would be „distance to New Jersey\", because - after treatment - people near the border would start to head to New Jersey to gain more money because the policy changed rised it in New Jersey, but not in the neighboring state Pensylvania. Requirements to use Diff-in-Diff: you need to divide the world into 2 time periods: before &amp; after treatment. You need two groups: 1) treatment-group &amp; 2) control-group. Parallel trend assumption needs to hold, otherwise the results are biased. 8.24 Instrumental variables // IV Requirements to use IV: the instrument needs to be valid, which means: corr(Z, coefficient of interest X(1)) &gt; OR &lt; 0 &lt;–&gt; relevance assumption corr(Z, u) = 0 &lt;–&gt; exogeneity assumption // exclusion restriction (= Synonyme) Wichtige Bemerkung hier: it is NOT possible to test whether an instrument is valid. To be more precise: only the relevance assumption can be tested, but you cannot(!!) test the exogeneity assumption // exclusion restriction. —&gt; You can try and convince your readers and argue in favor of the exogeneity assumption by testing if the differences between two groups are not statistically significant from each other (and that the instrument is indeed a random process which is independent from every variable) Key (assumption): The instrument Z should affect the outcome Y ONLY through the variable X, and not through any other(!) channel (e.g. Z –&gt; u –&gt; Y darf nicht sein!) What is the First-Stage and why do you have to look at it? you want to show that the relevance assumption is satisfied, e.g. that there is indeed a correlation between your instrument and your coefficient of interest. you want a strong instrument, e.g. you want to see a statistical significance of at least 5% on your Z-variable. What is the Reduced-Form and why do you look at it? —&gt; You regress the outcome Y directly(!) on the instrument, without including your “X” in this regression. What is the “2 Stage-Least-Squares” regression? —&gt; this is the regression where you regress your Y on your estimated(!!) coefficient of interest (your statistical software skips the first stage). –&gt; by doing this you only use that part of the variation in your coefficient of interest “X”, that is due to the instrument “Z” which only correlates with “X” and NOT with other unobserved variables in the error-term “u”. —&gt; when you do an IV-Regression, you need to show this. It contains a First-Stage, where you regress your X (= coefficient of interest) on your instrument Z and a reduced form Usage of causal language? —&gt; at the 2SLS, you can use causal languange, if you assume the instrument to be valid (bzw. if you assume the exclusion restriction holds!) How to get the 2SLS-coefficient of interest if you only have the coefficients for the Reduced Form and the First Stage? –&gt; Berechnung: 2SLS = Reduced Form / First Stage LATE // Local average treatment effects –&gt; Estimates generated from instrumental variables are based on the individuals whose behavior is affected by the instrument. Thus, people refer to instrumental variable estimates as “local average treatment effects (= LATE)”, because these estimates are the average effects for a SUBSET (i.e. local part) of the population. Use of more than 1 instrument: You can use more than 1 instrument. However, all instrument need to be valid &amp; you need to include all instruments as explanatory variables in the estimation of the first stage. 8.25 Regression discontinuity design // RDD Requirement to RDD? –&gt; You need to have a “Cutoff Score” (= Assignment variable? –&gt; Synonyme?) which decides whether you get treated or not. This cutoff score needs to be a continuous variable. 3 Assumptions for the RDD that you need to check: Whether an individual gets treated should ONLY depend on the cutoff score! If - for example - an individual can still decide whether he participates in the treatment or not AFTER he knows his cutoff score, this assumotion is violated! The individuals cannot(!) perfectly control the cutoff score. The assignment variable // cutoff score cannot(!) be caused by the treatment or the outcome (of the treatment?) –&gt; reverse causality –&gt; Y-variable wird zur X-variable und X-variable zur Y-Variable… Which people do we compare in an RDD? —&gt; In a RDD, we only compare our individuals exactly above and below the treatment. That’s why we can say that - whether these indivuals receive the treatment or not - is basically random! The fact that those two individual being treated or not is random is nice, because the variable becomes basically independent (no correlation) from all other factors that we cannot conrtrol for. External validity with RDD? In an RDD, we compare the two individuals just above &amp; below the threshold value, e.g. we compare the very first person that does not get the treatment with the last person who gets treated. Generally, this generates precise and valid estimates (that are causal) –&gt; high internal validity. But because we look at two precise individuals when estimating our effect, the estimated effects are not generalizable, but rather: they estimate a local treatment effect, which can at most be applied to the individuals just around the threshold, but not for the rest of the sample! 8.26 Time Series 8.26.1 Wörterbuch Univariate Forecasting: Verwende nur die vergangenen Preise der (eigenen) Zeitreihe, um die Preise der Zukunft zu forecasten. Multivariate Forecasting: Verwende - nebst vergangenen Preise der (eigenen Zeitreihe) - auch noch andere Zeitreihen, um die Preise der Zukunft zu forecasten. Cross-Correlation: Das ist die Korrelation zwischen dem Preis “heute” und einem bestimmten Lag einer anderen Zeitreihe. Mathematisch ausgedrückt: Corr(\\(Y_t\\), \\(X_{t-n}\\)), wobei n = {1, 2, …, n} Bemerkung: Leider habe ich kein Python-Package gefunden, welches die partielle Cross-Correlation berechnet, was sehr schade ist! What is the “MA”-Term // q-Parameter in the ARIMA-Modell?: You make a forecast based on the past errors in the time series (= “error lags”). 8.26.2 Seasonality What is Seasonality? A repeating pattern within a year. - Example 1: You see a “M”-shape every day –&gt; Daily Seasonality Beispiel einer Tages-Saisonalität Example 2: You see an “Buckel” for each day of the weak –&gt; Weekly Seasonality Beispiel einer Wochen-Saisonalität Example 3: You see an the same “Abwärtsbewegung” for the first 3 months of the year(!) –&gt; Monthly Seasonality Beispiel einer Monats-Saisonalität How to get rid // account for Seasonality in the Data? There are 2 possibilities: Use already de-seasonalized data: for example, unemployment rates. Include all year-, month- or week-dummies (exclude one, otherwise: Dummy-Variable Trap…). 8.26.3 Cycles What is a Cycle? A cycle is NOT the same thing as seasonality! Cycles take place over the course of several year, rather than within a year for seasonality. Generally, cycles are not predictable. - Example: Given a time-seroes, there may be a 1st cycle that takes place over the course of 3 years, while another cycle can take 1.25 years to complete –&gt; there is no general rule, to quantify their duration! 8.26.4 Partial Auto-Correlation Definition? Misst den direkten Effekt zwischen dem Preis “jetzt” und dem Preis vor “n”-Stunden (= deshalb “auto”-Korrelation), nachdem der indirekte Effekt einer oder mehrerer Kontroll-Variablen (deshalb “partielle”-Korrelation) entfernt wurde. Visualisierung des direkten VS. indirekten Effektes Mathematisch ausgedrückt: Corr(\\(Y_t\\), \\(Y_{t-n}\\)), wobei n = {1, 2, …, n} Hier ein Beispiel eines partiellen Auto-Korrelations Plot 8.26.5 How to create a simple Benchmark Time-Series Model? Vorgehen? Erstelle ein simples Mono-Copy-Modell, welches einfach - beispielsweise, bei einer Zeitreihe in Stunden-Einheiten - jeweils die letzten 24 Werte aus der Vergangenheit nimmt, um einen Forecast zu erstellen. Monocopy-Modell illustriert 8.26.6 Machine Learning Data Splitting: Weil Zeitreihen-daten die Eigenschaft besitzen, geordnet zu sein, muss beim Train-Test Split aufpassen: it is done without shuffling (= randomisation), weil wir ansonsten ein Data Leakage Problem hätten! Darstellung des Data Leakage Problems bei Time Series: das Modell kann leichte Vorhersagen machen, weil es Teile der Zukunft sieht! xD ### Cross-Validation for Time-Series Berechnung der Validation-Errors? Visuelle Darstellung, wie der verschiedenen Validierungs-Fehler (MAPE, MAE oder RMSE) berechnet werden auf dem Trainingsset. Vergesse nicht, dass jeweils bloss ein durchschnittswert pro Tabular-Regression ausgerechnet wird! Berechnung der Test-Fehler? Visuelle Darstellung, wie die verschiedenen Test-Fehler (MAPE, MAE oder RMSE) berechnet werden. Vergesse nicht, dass jeweils bloss ein durchschnittswert pro Tabular-Regression ausgerechnet wird! 8.27 Coded Algorithms &amp; R-Functions Monte Carlo Simulation: PS1, empirical methods, File: „ExpoR“ Construct a Scatterplot (= Streudiagramm) for the residuals, where you have the residuals on the Y-axis and a certain regressor on the X-Axis (—&gt; in order to check if A3 is satisfied): PS1, empirical methods, File: „Cigs“: Zeilen 20-33 (—&gt; Bemerkung: viel Zusatz, aber nützlicher Zusatz!) Construct a normal distribution: PS1, empirical methods, File: „Cigs“: Zeilen 29-33 Do an F-Test to check joint hypothesis: PS2, empirical methods, „PS2“: Zeilen 5-9 &amp; 45-49 Make a confidence intervall out of a regression: PS3, empirical methods, „PS3“: Zeilen 15-28 Only take the coefficient from a summary(regression_1) code: PS4, empirical methods, „PS4“: Zeilen 35; Zeilen 44-46 Reformulate a dataset, such that the unit of observation is not „the twin“, but rather the „family“: —&gt; PS4, empirical methods, „PS4“: Zeilen 52 Sort a dataset and only display some information &amp; drop some observations: PS4, empirical methods, „PS4“: Zeilen 83-84 Ommit NAs by looking at one particular column, from which you want to eliminate every NAs: Replication of Bonjour stud, lifestyle seminar, „Replication“: Zeilen 81-82 Make a beautiful table with two vectors (which are transformed to a dataframe): Replication of Bonjour stud, lifestyle seminar, „Replication“: Zeilen 163-182 Randomly add a person to a treatment group &amp; others to the control group: PS2, PeCi , „PS2“, Zeilen 119-121, Rmarkdown file Calculate means for treatment and control groups &amp; apply a t-test to see if they differ from each other in their characteristics: PS2, PeCi , „PS2“, Zeilen 129-133 Breusch-Pagan Test to test for heteroscedastic variances: PS3, PeCi, Aufgabe 2 a), Zeilen 66-68 Implementation of heterogeneous Treatment effects: PS3, PeCi, Aufgabe 4 a), Zeilen 112-118 &amp; 130-133 Calculate the average (total) treatment out of (multiple) heterogeneous treatment effects: PS3, PeCi, Aufgabe 4 b), Zeilen 137-139 Comparison of means in a DiD setting // summary statistics of a dataset using stargazer: PS5, Aufgabe 3 a), Zeilen 189-206 8.28 Nützliche Funktionen R: Count number of observations: NROW(dataset): counts the number of rows (= Zeilen = Anzahl Beobachtungen), OR dim(dataset): the first number would be the number of observations, the second one is the number of variables in the dataset Summary statistics of each variable in the dataset: summary(dataset) Simple linear bivariate regression model: lm(data$cigs ~ data$educ) Simple linear regression without a constant: lm(data$cigs ~ data$educ + 0), wobei hier zu beachten gilt, dass lm(y ~ x) Simple coefficient summary of a regression model: summary(regression_model) Autocorrelation-function // ACF: acf(residual, main=„title of the plot), need this to check A4 of the CLRM Create a dummy with one condition: ifelse(data$educ &gt;= 16, 1, 0) Append new vector (of same length as the dataset) to a dataset: data.frame(dataset, dummy_university, log-educ) Build a subset of a dataset: subset &lt;- subset(data, data$female ==0) Make a scatterplot (= Streuungsdiagram): plot(x, y, options) Make a histogram: hist(X, options) Adding new variable to the dataset: data[„new-variable“] &lt;- data$educ*data$noob Select a particular coefficient in your regression: regression$coefficients[5], Achtung: bei der selection wird der intercept mit in den code berücksichtigt!! Man hat also scho +1 gemacht, das wäre also der coefficient der 4. Variable!! Create a confidence intervall vector: predict(regression, intervall=„confidence“, level=.95) Delete a whole column (= variable) from a dataset: dataset$female &lt;- NULL Tell R in which format a date (= Datum) is: dataset$arrest_date &lt;- as.Date(datase$arrest_date, format =„%m/%d/%Y“) Umbenennung einer Spalte mit einem Spalten-Namen, welcher mehr Sinn macht: data$white_crime &lt;- data$PERP_RACE_WHITE 8.29 Useful Econometrics documents: Datascience: Datascience cheat-sheat:https://storage.ning.com/topology/rest/1.0/file/get/1211570060?profile=original r-graph-gallery.com python-graph-gallery.com Different Statistical Tests &amp; Models: Common statistical tests are linear models: https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf Statistical Power Calculator: http://powerandsamplesize.com Difference-in-Differences // DiD: Revisitting parallel trend assumption: https://blogs.worldbank.org/impactevaluations/revisiting-difference-differences-parallel-trends-assumption-part-i-pre-trend Machine Learning: Fussball Machine Learning code in python: https://mariamsulakian.com/2018/02/01/machine-learning-predicting-the-2018-epl-matches/ "],["machine-learning-1.html", "Chapter 9 Machine Learning 9.1 Typen von Machine Learning 9.2 Supervised Learning 9.3 Methods 9.4 Wichtige Funktionen 9.5 Difference between Training-, Validation- &amp; Test-dataset 9.6 Concept of Stratification 9.7 Cross-Validation 9.8 Normalisierung der Daten 9.9 RNN 9.10 Data-Pipelines 9.11 Uni-Kurs Neural Networks &amp; Deep Learning 9.12 Wörterbuch 9.13 Quellen", " Chapter 9 Machine Learning In diesem Kapitel, werde ich vertiefter in das Thema “Machine Learning” eingehen. 9.1 Typen von Machine Learning Es gibt grundsätzlich 2 Kategorien, in denen die verschiedenen Methoden in Machine Learning eingeordnet werden: Supervised Learning Goal 1: Classification Image Classification Goal 2: Predictions with Regressions Translation (zum Beispiel angewendet in Deepl.com) Image Captioning (siehe Bild): Unsupervised Learning Clustering: Matching // K-Means Clustering Dimension Reduction: Principal Component Analysis (PCA) Outlier Detection 9.2 Supervised Learning Bei Supervised Learning wird die Annahme gemacht, dass man die Funktion ‘f’ kennt, um mittels den Werten der Zufallsvariablen ‘X’ die Werte der Zufallsvariablen ‘Y’ herauszufinden. Mathematisch ausgedrückt: f : X -&gt; Y. 9.3 Methods Hier soll eine Auflistung aller Vorgehen bei Machine Learning aufgelistet werden: Algorithm for “best fit” to find the weights: Guess some random weights. “Go downhill”, e.g. apply a learning rate when using the method gradient descent. Is the fitted line good enough? If the answer is “no”, then go back to point 2). 9.4 Wichtige Funktionen Da Machine Learning mittels Aktivierungsfunktionen funktioniert, folgt eine Übersicht zu verschiedenen Funktionen: Identity Function: damit ist die Funktion f(x) = x bzw. y = x gemeint: Logitstic Function // Sigmoid Function // S-Shaped Function: damit ist die Funktion \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) gemeint: 9.5 Difference between Training-, Validation- &amp; Test-dataset Since I started with Machine Learning, I was always confused about the concept of data splitting, e.g. which sub-set of the entire dataset is now considered to be the training dataset, which one is the validation dataset and which one is the testing dataset. In the graph below, you see how it is defined correctly: Training Set VS. Validation Set VS. Test Set Important to note: There is oftentimes confusion between the definition of validation dataset VS. testing dataset, because there is no consens about it. Therefore - and if we take the above picture as the “true” definition - some people will call the validation dataset the “test dataset” and vice versa, e.g. the test data as the “validation data”! xD 9.5.1 Validation-Set VS. Test-Set What is the difference between a Validation Dataset and a Testing Dataset? Es gibt keinen Konsens dafür, was nun das validation dataset und welches das test set genau ist. Nima (= mein Mentor bei der SBB) verwendet den Begriff test set für dasjenige Dataset, welches hold out // separat für die (spätere) Prediction verwendet wird. Regel zu Unterscheidung der beiden Terme: Wenn steht: we hold out [name of the set], dann ist es das testing dataset. 9.5.2 Reason for a Validation-Set Why do we need validation set? At the end, you want the best model possible. If you want to tune your hyperparameters, you will need your test set. The big problem here, however, is that - if we use the test-set more than once when wanting to find out the best hyperparameters - our model will know the data by heart and the predictions will be too good, but only on this dataset that you are currently using! The model will not generalize well on new data. That’s why we need this additional subsetting of the training-dataset! 9.6 Concept of Stratification Angenommen du hast eine kategorische Y-Variable, welche entweder Nullen oder Einsen als Werte annimmt. Nehme nun an, dass - im Training-Dataset - der Anteil der Nullen 60% beträgt. Wenn du nun ein stratified Sample willst, dann wird das Test-Dataset ebenfalls einen Anteil von 60% an Nullen enthalten! Quelle: Train-Test-Split in Sklearn and Cross-Validation 9.7 Cross-Validation The concept of cross-validation can be splitted into 2 parts: Step 1: Split your dataset into 1 training- &amp; 1 test-set. Rule of Thumb: Usually, the split is 70-90% training set and 10-30% for the test-set. Code Example: Example of Code for Train-Test Split Step 2: Now, we divide the training set further, such that it will contain - if we assume a 3-fold cross-validation as an example - 3 different validation sets and 3 training sets. Allgemeine Cross-Validation 9.7.1 Special Case: Cross-Validation for Time-Series Data Because time-series are ordered, since the flow of time is only going forward, the graph shown above for step 2 is not valid and we need another approach! Cross-Validation for Time Series This picture shows an example of a 3-fold cross-validation for a time-series. 9.7.1.1 The Expanding-Window Cross-Validation | Variante 1 One version of the special-case “CV for time-series” is the so-called Expanding-Window Cross-Validation. In this CV, we expand the number of training-data progressively, until we reach the end of the training-set. Visuelle Darstellung, wie der verschiedenen Validierungs-Fehler (MAPE, MAE oder RMSE) berechnet werden auf dem Trainingsset. Achtung: Vergesse nicht, dass jeweils bloss ein durchschnittswert pro Tabular-Regression ausgerechnet wird! _Wenn du also nicht zufriedenstellende Prognosen 9.7.1.2 The Sliding-Window Cross-Validation | Variante 2 In this CV, we start with a first BIG sub-training dataset. For the second fold - however we only take the most recent past data to predict the. the number of training-data progressively, until we reach the end of the training-set. Example of a 18-fold Sliding-Window Cross-Validation 9.7.1.3 What are the Advantages / Disadvantages of those two types of Cross-Validation? The Expanding Windows Splitter is **less volatile when a big macroeconomic shock occurs, like the COVID-19 crisis. However, the model reacts more slowly // is more sluggish (= träge), when it enters a “Regime-Wechsel” (= Zustandsänderung an den Märkten). The Sliding Windows Splitter, it is the opposite: **(much) more volatile when a big macroeconomic shock occurs, like the COVID-19 crisis. However, the model reacts more quickly, when it enters a “Regime-Wechsel” (= Zustandsänderung an den Märkten). Expanding-Window VS. Sliding-Window: Vergleiche jeweils hell-rot mit dunkel-rot ODER hell-blau mit dunkel-blau, um den Verlgeich zwischen Expanding- &amp; Sliding-Window zu sehen. 9.7.2 Why do we use Cross-Validation? There are 2 reasons: It allows us to compare the score (MAPE, MAE or RMSE) of different model set-ups, for example: CV-Scheme changes, e.g. a Time-Series Prophet-Model with a Sliding-Window of 4 Months VS. a Prophet-Model with an Expanding-Window (full past). OR changes in the covariates, e.g. a model that includes an important covariate, while the other model does not. This will allow you to draw conclusions regarding the selection of “the best” model. It allows you to assess the performance of different machine-learning methods. In a classification-setting for example, you can use the confusion-matrix. 9.8 Normalisierung der Daten Was versteht man unter Normalisierung? Was sollte man dabei beachten? Warum wird in einem Pre-Processing Schritt eine Normalisierung durchgeführt? Normalisierung == Subtrahiere den Mittelwert und dividiere durch die Standardabweichung jedes Merkmals. Es sollte beachtet werden, dass der Mittelwert und die Standardabweichung nur anhand der Trainings-Daten berechnet werden sollte, damit die Modelle keinen Zugriff auf die Werte in den Validierungs- und Testsätzen haben. Es ist wichtig, Features zu skalieren, bevor ein neuronales Netzwerk trainiert wird. Normalisierung ist eine gängige Methode für diese Skalierung. 9.9 RNN Was macht ein Reccurrent Neural Network (RNN)? Ein Beispiel für ein RNN wäre ein Long Short Term Memory Modell (LSTM Modell). Dabei nimmt das RNN zunächst ein ganz kleines Vergangenheits-Intervall, macht eine Modell-Estimation und dann - im nächsten Schritt - wird ein grösseres Vergangenheits-Intervall verwendet (inkl. predicted Y-Variable aus dem vorherigen Modell), um eine neue Modell-Estimation zu machen etc. Beispiel eines RNN: hier ein LSTM 9.10 Data-Pipelines Data Pipelines use an input to produce an output and then - on a second step - use the produced output as an input to produce another output etc… Visualisierung: Du kannst dir unter Data Pipelines nichts anderes als eine “Guetzli Fabrik” vorstellen, welche diverse Produktionsmaschinen verwendet - zum Beispiel einen Teig-Cutter, dann einen Butter-Schmierer, sowie einen fetten Ofen und einen Sortierer von “guten VS. schlechte Guetzli” - um die Inputs immer weiter zu verarbeiten, sodass schlussendlich ein Endprodukt (= die fertigen “Guetzli”) ensteht, dass man verkaufen kann. How to create good Data-Pipelines in Scikit-Learn? Ziel von Data-Pipeline: Write more clean // readable code, especially when you do data cleaning. A datapipeline is basically a way of standardizing your code. Warum sind Data-Pipelines so geil?: Because you can compare many different regression-models (Linear-Regression Vs. Logistic-Regression Vs. RandomForrest …), applying different “scaling-techniques” (= normalize a variable with mean 0 and standard-deviation of 1), as well as using “data cleaning techniques” (= reduce dimensions via PCA, reduce missing-values etc…). Another cool thing to note is, that you can choose the order, in which the cleaning, scaling and fitting occures! See also the summary of the guy on Youtube ab 10:00-11:00. Link to Github for an example: Jupyter-Notebook code-example on how to create a little Data-Pipeline by yourself 9.11 Uni-Kurs Neural Networks &amp; Deep Learning 9.11.1 Notation Neural Networks Machine Learning verwendet unterschiedliche Begriffe für diverse gleiche Konzepte &amp; Definitionen aus den Wirtschaftswissenschaften. Nun geht es darum, die korrekte Übersetzungen für diese Wörter aufzulisten: weights = Beta-Coefficients = parameters = a, b = neurons Input units = independent variables “x” inputs = Anzahl Observations in Total Output unit = Dependent variable “y” = labels Activation function (AF) = you need this function to plug in your estimated regression model. Examples of AF: logit, probit, relu, simple “threshold” AF etc… Perceptron = Linear Binary Classifier = usually, the perceptron is a linear separator (= line that separates group in a regression) = Perceptron is a single layer neural network Multi-layer perceptron == Neural Networks. Example: learning rate = how far to go in a particular direction features = inputs = independent variables “x” = Xki labels = this is the “true Y” you observe in the real world = output = dependent variable “going downhill” = this is the learning process that you get by using the method “gradient decent” (look youtube video of user called “the coding train” at ca. 17:30) &amp; applying a “learning rate” to it yk = this is the estimated regression function zk = “logits”, e.g. this is the whole sum of the weights multiplied by the x-variables (= entire regression), but this time we put this entire regression as an input into the logistic function –&gt; in other words: the same as yk but we then apply the specific activation function “logit function” to the estimated zk // \\(\\hat{y}\\) Input Layer = Layer 0 = very first set of Neuron Output Layer = Last Layer = last set of Neurons Hidden Layers = All Layers between the input &amp; output Layer input node = nodes at the input layer output node = nodes at the output layer hidden nodes = nodes, which are in the hidden layers or at the output layer &amp; don’t give out outputs Feed forward neural networks = connections only between layer i and layer i+1 Convolutional neural networks = a type of feed-foward network Recurrent neural networks = connections flow backwards to previous layers as well supervised learning = function estimation There are 2 different types: regression, classification unsupervised learning = structure the data into groups (very subjective) // detecting patterns. Can also be used for: data reduction, outlier detection loss-function = cost-function = [TRUE y - ESTIMATED \\(\\hat{Y}\\)]2 = error –&gt; we define the loss-function be the “least squares” identity function = y = x bzw. f(x) = x sigmoid function = logit function bias term = error term Note: Oftmals wird der “input” für den bias term als Zahl “1” angegeben (siehe Bild oben “Example of Multi-Layer Network”, wo der bias term als Zahl “1” angegeben ist.) Epoch = means we go through the whole data set once –&gt; default is ten epochs Net Input Function = Regressionsmodell als Ganzes = sum of all weighted inputs // “x” kernel = (starting) values for the weights regularizers = penalties that are used to reduce overfitting (of the starting values for the weights?) backpropagation = back pass = when we have our error-term, we can calculate the gradient and - if the error was too big - we can backpass the error-term with the help of the learning rate to a previous layer and estimate new // better weights breaking symmetry = principle, which says that you need to have different initial weights for hidden units with the same activation function and same inputs batches = these are smaller samples that you take from the whole dataset, e.g. you take only a fraction of the dataset –&gt; you use batches because the computation gets faster rather than putting the whole dataset into the machine learning “Apparat” –&gt; Rule: the higher the batch size, the better estimates you get decay = In Machine Learning, it has become kind of standard to make learning rates dynamic, e.g. first have bigger learning rates, because you can be very wrong at the beginning with the random weights, but then - towards the end of estimation - you adapt the learning rate only very smoothly, since you slowly go towards the optimum –&gt; typically, this decay will make the learning rate smaller as the training continues. momentum = makes learning rates dynamic –&gt; If you see that - in the history of gradients - the gradients point generally in the same direction, momentum will adjust the learning rate by increasing the step size hyperparameters = examples are: Learning rate, Learning Rate Decay, Momentum, Batch Size, Weight / Bias initialization Confusion Matrix = C = shows - in the diagonal of the matrix - how many times your predicted outcome was the same as the actual outcome. All the other numbers are saying that your model’s prediction was not in line with the actual outcome Precision = if i look at a guess // prediction, how many % my algorithm guessed correctly? –&gt; E.g. Anteil der predicted outcome \\(\\hat{y}\\), welche korrekt mit den ture outcomes übereinstimmen. Mathematisch ausgedrückt: Im Zähler die Anzahl an übereinstimmenden predicted outcomes \\(\\hat{y}\\) &amp; im Nenner Totale Anzahl an predicted Outcomes \\(\\hat{y}\\). Recall = if i look at an actual true outcome, how many % where guessed correctly? –&gt; E.g. Anteil der true outcome Y, welche korrekt vorhergesagt wurden Mathematisch ausgedrückt: Im Zähler die Anzahl an korrekt vorhergesehenen true outcomes &amp; im Nenner Totale Anzahl an True Outcomes. Training data = Training set is the one on which we train and fit our model basically to fit the parameters. Testing data = Testing Data is used only to assess performance of the model. Variance = Overfit = you use too much X’s // features in your model –&gt; you get too much variance in your predictions Bagging = Train the same architecture on different subsets of data Boosting = Train different model architectures on the same data data augmentation = get more data by adding noise on the input layer weight tying = Make the weights similar 9.11.1.1 Alphabetisch sortiert: Um die Begriffe noch leichter zu finden, habe ich sie hier noch alphabetisch sortiert: Activation function = you need this function to plug in your estimated regression model. Examples of AF = logit, probit, relu, simple “threshold” AF etc… backpropagation = back pass = when we have our error-term, we can calculate the gradient and - if the error was too big - we can backpass the error-term with the helplearning rate to a previous layer and estimate new // better weights batches = these are samples from the whole dataset –&gt; you use batches because the computation gets faster rather than putting the whole dataset into the machine learning “Apparat” –&gt; Rule: the higher the batch size, the better estimates you get bias term = error term breaking symmetry = principle, which says that you need to have different initial weights for hidden units with the same activation function and same inputs Confusion Matrix = C = shows - in the diagonal of the matrix - how many times your predicted outcome was the same as the actual outcome. All the other numbers are saying that your model’s prediction was not in line with the actual outcome Convolutional neural networks = a type of feed fowardnetwork decay = make learning rates dynamic –&gt; typically, this decay will make the learning rate smaller as the training continues. Epoch = means we go through the whole data set once –&gt; default is ten epochs features = inputs = independent variables “x” = X(ki) Feed forward neural networks = connections only between layer i and layer i+1 “going downhill” = this is the learning process that you get by using the method “gradient decent” (look youtube video by “the coding train” at ca. 17:30) &amp; applying a “learning rate” to it Hidden Layers = All Layers between the input &amp; output Layer hidden nodes = nodes, which are in the hidden layers or at the output layer &amp; don’t give out outputs hyperparameters = examples are: Learning rate, Learning Rate Decay, Momentum, Batch Size, Weight / Bias initialization identity function = y = x bzw. f(x) = x inputs = Anzahl Observations in Total Input Layer = Layer 0 = very first set of Neuron input node = nodes at the input layer Input units = independent variables “x” kernel = (starting) values for the weights labels = this is the “true Y” you observe in the real world = output = dependent variable learning rate = how far to go in a particular direction loss-function = cost-function = [TRUE y - ESTIMATED Y(hat)]^2 = error –&gt; we define the loss-function be the “least squares” Net Input Function = Regressionsmodell als Ganzes = sum of all weighted inputs // “x” momentum = makes learning rates dynamic –&gt; If you see that - in the history of gradients - the gradients point generally in the same direction, momentum will adjust the learning rate by increasing the step size Output Layer = Last Layer = last set of Neurons output node = node at the output layer Perceptron = Linear Binary Classifier = linear seperator (= line that separates group in a regression) = Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Precision = if i look at a guess // prediction, how many % my algorithm guessed correctly? Recall = if i look at an actual true outcome, how many % where guessed correctly Recurrent neural networks = connections flow backwards to previous layers as well regularizers = penalties that are used to reduce overfitting (of the starting values for the weights?) sigmoid function = logit function saling = pre-processing data supervised learning = function estimation –&gt; 2 different types: 1) regression; 2) classification Training data = Training set is the one on which we train and fit our model basically to fit the parameters. Testing data = Testing Data is used only to assess performance of the model. unsupervised learning = structure the data into groups (very subjective) // detecting patterns // data reduction weights = Beta-Coefficients = parameters = a, b = neurons y(k) = this is the estimated(!!) regresion function z(k) = “logits” –&gt; the same as y(k) but we then apply the specific activation function “logit function” to the estimated z(k) // y(hat) 9.12 Wörterbuch 9.12.1 Synonyme Training Dataset // Training Set Testing Dataset // Test Set generalize // extrapolate relativ zu // im Verhältnis zu feed in // plug in shape // dimension target // predicted y-value Treiber // x-Variablen Score // Error Cross-Validation Score // Validation Error Volume // Speicherplatz Weights // estimated coefficients Parameter Tuning // Grid-Search tweakable parameters // veränderbare Parameter (wenn du Parameter-Tuning) 9.12.2 Data Science Samples == number of rows // number of observations within a dataset. Instance: value within a cell –&gt; konkreter “x”-Wert, welcher angenommen wird und du - beispielsweise - für eine Prediction verwenden kannst. This is the overview of a Pandas DataFrame, where an instance would be nothing else but a “value”. Label: true y-value Forcasted data: These are the predictions that you do via the help of a model (–&gt; by plugging in concrete x-values), that you’ve built. Training Dataset: This is the sample, that you use to estimate your model. Testing Dataset: This is the hold out-set, which you use at the end, to check whether your model is able to generalize // extrapolate well to new data. “The model is learning”: What is meant by “learning” is –&gt; given some Datenpunkt-Wolke, the computer will try and fit the “best line” [oftentimes by minimizing the sum of the squared residuals, if you use OLS (= Ordinary Least Squared) OR by using the Gradient Descend Method, when you have more data] to construct the “best model” possible. Training: You are estimating a model on the trainig-dataset, such that the model “learns” from the data. Bias: When a modl displays “bias”, then it means that your model is too simpel (not enough X-variables included, for example) and - as a result - your estimated sample regression function is not able to approximate the (true) underlying (unknown) population regression function. Overfitting: The problem of overfitting occurs when a model is too complex and captures too much detail from the training data. This can lead to the model being unable to generalize to new, unseen data. Fazit: This is like the problem of “low external validity”. You may have a high “internal validity”, but external validity is low! Shuffle: englisches Wort für “mischen”. In the context of splitting the dataset into test- &amp; training-data, it is common practice to shuffle your observations within your dataset first. Reason why you shuffle?: Shuffling data serves the purpose of reducing variance AND making sure that models remain general AND overfit less. Merke: When dealing with time-series, you should not use shuffle when splitting the data into training- &amp; testing-data. Training Score OR Test Error: This error // score is the [oftentimes squared] difference between the true Y-variable and predicted Y-variable (= \\(\\hat{y}\\)). Note that the for the formula \\(\\sum_i^N{(y_i-\\hat{y_i})}\\), the \\(y_i\\) can be either related to: The test-set, when you are evaluating the final model-performance, OR The validation-dataset, when you are evaluating how well the model is “performing” with the help of Cross Validation, e.g. how the model performs with an increasingly bigger training dataset. Cross-Validation Score: This is the mean error // score that measures // evaluates how the model is “learning” over increasingly bigger training datasets. It is important to highlight the fact that this metric is only a mean, e.g. you don’t see the individual forecast that was being made. The main purpose of this metric is to be able to judge the model-performance very quickly: with only 1 single number, namely the mean error (by an increasing amount of sample-size). If you have a “bad” model - e.g. a high mean error - you will need to look at the individual errors (and not just the mean error), in order to understand what went wrong with the model! Fold == subset of the training dataset. Example: In a K-Fold Cross-Validation, you randomly split the training set into 10 distinct subsets, which are called folds. Grid: Das ist nichts anderes als die Optimierung von diversen “Modell-Parametern”. - Beispiel: Bei Random-Forrest Modellen kann man zum Beispiel die Tiefe eines Modells bestimmen, dh die relevante Frage, welche ein Forscher sich stellt, ist: wie viele Baum-Zweigungen die geeignesten Predictions bringen?. Mit Funktionen, wie zum Beispiel GridSearchCV() kann dieses Problem geregelt werden. Classifiers: These are simply Regression-Functions, where the Y-variable is binary, e.g. die Y-Variable kann nur Y = 0 oder Y= = 1 als Werte annehmen. Beispiele: Logit-Regression Probit-Regression Naive Bayes etc… Example: Logit- &amp; Probit-Regression, or Naiv Bayes, etc… feed in: Häufig im Zusammenhang mit Einsetzen von konkreten Werten für die x-Variablen in das geschätzte Modell, um Predictions zu erhalten. Batch: sample-size when you train your model with a dataset. Granularität (Beispiel): Angenommen, man möchte wissen, wie viele Tage, Stunden, Minuten und Sekunden innerhalb von 20’044 Sekunden enthalten sind → hier haben wir also Granularität von 4 → Lösung: 4 Tage, 18 Stunden, 37 Minuten, 44 Sekunden Kalibrierung des Modells: Wie wurde das Modell programmiert im Allgemeinen? → Dazu gehört - beim preisprog-Projekt der SBB - die Anpassung der Prognose. Extrapolation: Das ist eine Prediction, welche mittels X-Variablen ermittelt werden, welche zuvor nicht im Datensatz waren. Interpolation: Das ist eine Prediction, welche mittels X-Variablen ermittelt werden, welche bereits im Datensatz waren, als die Schätzung getätigt wurde. Pearson Korrelation == Lineare Korrelation Beispiel einer Pearson-Korrelations Matrix 9.12.3 Time-Series Sliding Window: This is just a way to tell python how to do a particular type of cross-validation and have equal lengths of time series where we can learn on. Forecasting Horizon: These are the points in time (in the future), for which you want to make a prediction (= \\(\\hat{y}\\)). This is something, you need to define, when you want to estimate your model in a time-series setting. Learning Task: Forecasting Task // Extrapolation [in Time Series] –&gt; not sure if this is correct… xD Time Heterogenous Data: These are different time-series datasets, that have different time stamps. Quelle: Ab 27:05 (Link: http://www.youtube.com/watch?v=Wf2naBHRo8Q&amp;t=27m05s) Seasonal Periodicity: The number of times per year, in which the forecaster expects to see a seasonal pattern. Concrete Example: In Philipp’s Notebook, he had a seasonal periodicity of 2, e.g. he said that in winter &amp; sommer, he expects a seasonal pattern. “Reduction is composable”: Synonym wäre “addieren” → E.g. you can split a difficult task into a bunch of smaller tasks (= reduction) and “add” them together to solve the bigger task at the end → “reduction is composable”… 9.13 Quellen https://blog.exxactcorp.com/lets-learn-the-difference-between-a-deep-learning-cnn-and-rnn/ https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/notes/Sonia_Hornik.pdf "],["mengenlehre.html", "Chapter 10 Mengenlehre 10.1 Wörterbuch 10.2 Mathematische “Schrift” lesen", " Chapter 10 Mengenlehre Du wirst ein bestimmtes Maß an Wissen in der Mengenlehre brauchen, denn dieser Teil der Mathematik ist besonders wichtig: für Statistik: weil in Statistik mit Mengen an possible outcomes gearbeitet wird. für Choice Theory: weil dort mit einer Menge an possible choices gearbeitet wird. Häufige Schwierigkeit, welche ich bei diesem Thema begegnet bin, ist das Nachvollziehen der mathematischen Schreibweise von Mengen, insbesondere in Kombination mit Funktionen! Wieso ist dies problematisch? Weil ich dann nicht einmal verstehe, was ich lese¨ Und das ist schlimm! Denn, ohne Verständnis, kann man gar nichts lernen. Man ist dann bloss jemand, der gut auswendig etwas nachplappert, was schlecht (&amp; gefährlich) ist! 10.1 Wörterbuch Mapping = Mapping is simply a (general) function between two methematical objects or structures. In many branches of mathematics, the term mapping (or map) is used to mean a function. Visualisierng: - Note: More than one x&lt;sub&gt;a&lt;/sub&gt; can point to a element of Y (= y&lt;sub&gt;a&lt;/sub&gt;), but every y&lt;sub&gt;a&lt;/sub&gt; is only associated with one unique x&lt;sub&gt;a&lt;/sub&gt;. 10.2 Mathematische “Schrift” lesen “Y is a function of X”: y(x) y = f(x) y depends on x y as a function of x Was heisst f: X &amp;#8594; Y? In Worten heisst dies: A function f from X to Y. Beispiel anhand einer “choice correspondance” (= C) definition: In Worten: A function C from the power set of X (= 2&lt;sup&gt;X&lt;/sup&gt; → das ist unser x in der Funktion C) (wihtout leere Menge) to the power set of X (= 2&lt;sup&gt;X&lt;/sup&gt; → das ist unser y in der Funktion C) (without leerer Menge). **Weitere Beispiele für f: X #&amp;8594; Y: ${delta}: Y #&amp;8594; [0, 1] In Worten: The function ${delta} - when applied to Y (= our X) - will assign a value in the intervall between 0 and 1 to Y, which will be the outcome. f: X x Y #&amp;8594; [0, 1], assume X is a vector, Y is a vector too In Worten: The function f - when applied to each combination of X and Y (= our inputs) - will assign a value in the intervall between 0 and 1 to each combination of X and Y, which will be the outcome… "],["business-die-welt-der-unternehmen.html", "Chapter 11 Business | Die Welt der Unternehmen 11.1 Wörterbuch 11.2 Projekt-Management 11.3 Meetings 11.4 Abschluss-Präsentation 11.5 Job-Interviews", " Chapter 11 Business | Die Welt der Unternehmen In diesem File werde ich alles “business-related” tun, was in Zusammenhang mit Data-Science steht. 11.1 Wörterbuch 11.1.1 IT &amp; Software-Development “Eine Stroy (mit Daten) haben”: Damit ist gemeint, dass du einen Erklärungsansatz hast, um z.B. ein “weirdes” Phänomen in den Daten zu erklären. agnostisch: Im IT-Umfeld hat das Wort agnostisch eine besondere Bedeutung. Es bezieht sich auf etwas, das soweit verallgemeinert wird, dass es auch unter verschiedenen Umgebungen funktionieren kann. Allgemeines Beispiel: Denke an mathematische Formeln, welche nur aus “allgemeinen Variablen” bestehen. Konkretes Software-Beispiel: Eine Plattform-agnostische Software läuft unter jeder Kombination aus Betriebssystemen und Prozessorarchitekturen. cached: stored away for future use. Beispiel: Think of a Browser, which can store - in his memory - the pictures and other HTML, CSS or JS-Files. This allows faster download of these resources, once a user visits a website on a later point in time in the future. Client: Damit ist häufig der Browser gemeint, wenn er Abfragen an den Server gibt. composite: custom End to End: Synonym für “es funktioniert”. Hierbei setzt man das Modell mit dem Code auf, führt es aus und man erhält abschliessend einen Output, ohne dass z.B. der Kernel abstürzt (wie es z.B. bei der AutoARIMA()-Methode bei sktime der Fall war…). 11.1.2 Organisation Was ist ein Kanban?: In der BWL-Sprache ist Kanban ein Vorgehensmodell zur Softwareentwicklung. Ziel #1: Das Meetings soll die Anzahl an paralleler Arbeiten, (= der Work in Progress (WiP)) minimiert // begrenzt werden und somit kürzere Durchlaufzeiten erreicht. Ziel #2: Das Meeting soll die Probleme – insbesondere Engpässe – schnell sichtbar machen. 11.2 Projekt-Management Sitzungen müssen gut dokumentiert werden! Insbesondere ist hervorzuheben, in welche Richtung das Projekt geht. 11.3 Meetings Was ist das Ziel eines Meetings? Das Ziel des Meetings ist es, dir &amp; deinem Team die nötige Basis zu geben, damit du in der folgenden Zeit bis zum nächsten Meeting genau weisst, was du zu tun hast. Am wichtigsten ist hier mit dem “Boss” zu reden und klar definieren, was er erreichen will im Projekt. Dies gibt die globale Richtung des Projektes an. Es ist exterm wichtig, mit dem Boss diese Ziele zu definieren, da die Planung des gesamten Projektes in dieser Phase hier noch definiert werden kann. Nachdem diese anfängliche Gliederung getätigt wurde, kann im Verlauf des Projektes nicht mehr viel davon abgewichen werden. Deshalb hängt die Gesampt-Performance eines Projektes sehr stark von der anfänglichen Planung ab! Während des Projektes gibt es dann immer wieder folgende Punkte zu prüfen: Sind die getätigten Schritte im Einklang mit dem Endziel des Bosses? Wichtig ist zu rechtfertigen, was du gerade am tun bist und warum genau du das tust. Aufklärung, damit alle auf dem gleichen Informationsstand sind Goal: Wo stehen wir? Wie sieht die Situation mit dem Notebook aus? Gibt es noch offene Lücken im Wissen eines Team-Mitglieds, welches von anderen Mitarbeitern gedeckt werden müsste? –&gt; Goal: Kein Mitglied isolieren, jeder muss sich integriert fühlen. Insbesondere Wert auf Papers &amp; Daten setzen hier Was sind weitere offene Optionen und inwiefern eignen sich diese Ansätze für unser Ziel? Goal: Aufzeigen, dass wir noch weitere Alternativen haben, als nur Phillip’s Notebooks Sonstige Unklarheiten / Bemerkungen / Inputs / Vorschläge? –&gt; Goal: letzte Informationsasymmetrien beheben. 11.4 Abschluss-Präsentation Folgender Aufbau eignet sich: 1) Wissenschatlicher ODER unternehmerische Relevanz deines Projektes? 2) Präsentation wichtiger Kernschritte - Ziele - Methodologie - wichtigste Ergebnisse pro Kernschritte 3) Thema a: zum Beispiel Datensammlung 4) Thema b: zum Beispiel Modellierung des &quot;besten Modelles&quot; 5) Thema c: Schlussfolgerungen // Präsentation des MVPs 6) Fragen &amp; Diskussion 11.5 Job-Interviews 11.5.1 Praxis Frage des Interviewers: “Wieso so kompliziert die Webseite deines Vaters? Wenn etwas einfacher geht, sollte es man so machen, nicht?” Meine Antwort: „Ja, da haben Sie recht, deshalb habe ich - in einem späteren Projekt - meine eigene Webseite mit Hugo kompliert, was circa 2h geht ;) 11.5.2 Vorteil von Data-Science Projekten | Punkte, auf die du unbedingt zu Sprechen kommen solltest… Enorm hohes langfristiges Ertrags-Potential: Sie haben potentiell hohe langfristige Erträge, weil die Modelle - wenn sie nach einem ersten PoC - noch deutlich verbessert werden können. Persistenz-Eigenschaft über die Zeit: das Projekt bleibt bestehen –&gt; keine Sandburg… 11.5.3 Quellen Vorbereitung Data-Science Interviews: Diese Webseite hat einen kompletten Guide mit Videos / Artikeln zu extrem vielen Key-Concepts gesammelt und zur Verfügung gestellt. "],["openai.html", "Chapter 12 OpenAI 12.1 What can OpenAI do? | The Output 12.2 How does the Model work? | The Blueprint 12.3 Terminology of OpenAI-API | Wörterbuch 12.4 Technology behind it | Quellen", " Chapter 12 OpenAI Here is the link in order for you to play around: https://beta.openai.com/playground Achtung Kostenpflichtig (ab sofort): Even if you play around on the “playground”-area, you will get charged! OpenAI treats “Playground” the same, as the usage from the regular API. You can see how much you are billed by navigating to this link: https://beta.openai.com/account/usage (you get 18$ and 3 months to spend them “for free”). Open AI is a non-profit organisation that uses NLP-Models (with Deep-Learning) to be able to understand the context of - for example - questions and generate content on its own! But it goes even further than that.: in his youtube-video, Fireship.io talks about OpenAI creating art, e.g. the computer is able to draw for you! It is awesome! The guy / team who created this, truly is / are admirable and has / have surely put an incredible amount of effort in it! :O 12.1 What can OpenAI do? | The Output Content generation Summarization Classification, categorization, and sentiment analysis Data extraction Translation Many more! 12.2 How does the Model work? | The Blueprint The model predicts which text is most likely to follow the text preceding it. 12.3 Terminology of OpenAI-API | Wörterbuch Prompt = Input-Text you give the model. For example, this can be a question like What is Economics? And what is Structural Economics?. Key: Adding a simple adjective to our “prompt” is able to change the resulting output completely! Designing your prompt is essentially how you “program” the model. Completions = This is the output of the model, for example, it would be the “prediction” // answer that the model provides for the question What is Economics? And what is Structural Economics? “Temperature” = This is a way to bring in variation in the prediction // suggested results of the model. Example: If you submit the same prompt multiple times when the “temperature” is set to 0, the model would always return identical or very similar completions // results. If you re-submit the same prompt a few times with temperature set to 1, you will notice that the proposed results will be different. Possible Range for “temperature”: You can set the parameter “temperature” with values between (and including) 0 and 1. Key to note: “Temperature” is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature towards 0 means it will take fewer risks when making a prediction. Faustregel: It’s usually best to set a low “temperature” for tasks where the desired output is well-defined. Higher “temperature” may be useful for tasks where variety OR creativity are desired. token = You can think of tokens as pieces of words used for natural language processing. For English text, 1 token is approximately 4 characters or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900’000 words or 1.2M tokens. “Wechselkurs” Token zu Wörter: 1 token = 0.75 words, also ein 1:0.75 Verhältnis, dh für 1 Token erhalte ich 0.75 Wörter bzw. für 1 Wort erhalte ich 1.333 Token. “Wechselkurs” Token zu Zeichen: 1 token = 4 Zeichen, also ein 1:4 Verhältnis, dh für 1 Token erhalte ich 4 Zeichen bzw. für 1 Zeichen erhalte ich 1/4-tel Token. 12.4 Technology behind it | Quellen Hierarchical Text-Conditional Image-Generation with CLIP-Latents | Ramesh, Dhariwal "]]
