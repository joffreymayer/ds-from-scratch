[["index.html", "Data Science from Scratch Code Chapter 1 Computer Set-Up 1.1 Mac Tricks 1.2 Virtual Environment 1.3 How to efficiently manage a Project? 1.4 Terminal-Commands: 1.5 How to activate &amp; use Python? 1.6 Git &amp; Github: 1.7 LaTeX 1.8 Wörterbuch 1.9 Synonyme 1.10 Projekt-Management 1.11 Job-Interviews 1.12 Ausblick", " Data Science from Scratch Code Joffrey Anthony 2022-05-16 Chapter 1 Computer Set-Up When building projects efficiently, you will need different set-ups in order to have everything in your code work properly, not only on your own machine, but also on other machines, for example of another team-member. 1.1 Mac Tricks Before we start with the whole set-up, you will also need to know how to work more efficiently with your Macbook-Computer. Those “tricks” will be useful for many other programs you will need when working in the field of datascience. 1.1.1 Öffne die Developer-Tools von Google-Chrome command + shift + i 1.1.2 Speichern command + s 1.1.3 Speichern Unter shift + option + command + s 1.1.4 Finder command + leerschlag 1.1.5 Programm schliessen command + q 1.1.6 Neuer Ordner shift + command + n 1.1.7 Tilde Zeichen Das ist nützlich für shortcuts im Terminal, wenn man das Working Directory festlegt: option + n 1.1.8 Backslash Useful for paths within the Directory: option + shift + 7 1.1.9 Screenshot vom Bildschirm command + shift + 3 1.1.10 Teil des Bildschirms Screen-Schoten shift + command + 4 1.1.11 Verlauf löschen in Chrome Browser shift + command + entfernen 1.1.12 Verlauf löschen Safari command + alt + e 1.1.13 Bildschirm Aufnahme Funktioniert nur, wenn QuickTime Player aktiv ist: option + command + n 1.1.14 Lesezeichen in Google Chrome command + d 1.1.15 Search Console Pop Up option + command + j 1.1.16 Format Kopieren einer Zelle (in Numbers) option + command + c 1.1.17 Format übertragen einer Zelle (in Numbers) option + command + v 1.1.18 Switch between Applications on your computer command + tab 1.1.19 Move Forward through Tabs control + tab 1.1.20 Zahl als Exponent Geht nur in Pages oder Tablett!: shift + control + command + „+“-Zeichen WÄHREND man die Zahl mit der Maus markiert Dann ist sie noch „normal“ und nicht tiefgestellt —&gt; also so: zum Beispiel: 2. 1.1.21 Source-Code einer Webseite aufschalten Option + CMD + u im Browser drücken 1.1.22 Interaktive Code-Ansicht für das Abchecken von Webseiten fn + F12-Taste auf Touch Bar drücken 1.1.23 Approximate Symbol ≈ option + x 1.2 Virtual Environment Because the libraries you work with in your projects will be updated over time (this is generally bad news, since this will cause all sorts of dependency problems across your libaries you use), it is crucial to understand that you will need a virtual environment (venv). There, you will install all the libraries you need. The major advantage here, is that you can control the version you install the library. Furthermore, you can send the venv to another computer and the people will download exactly the versions of each library. This allows that your code will always work, independently of the machine you will use! There will be no dependency problems anymore, which is a huge win.. :) 1.2.1 Create a venv Go to your terminal and plug in the following code: conda create -n YOUR-VENVIRONMENT-NAME-HERE python=3.6 1.2.2 Activate your newly created venv Weirdly though, you also need to activate the environment you created above. Plug in the following code: source activate YOUR-ENVIRONMENT-NAME Note that the code above can also be used to activate virtual environments you created in the past! =) 1.2.3 Install packages Now that you are in your new venv, you can start downloading some packages: python -m pip install SOME-PACKAGES 1.2.4 Overview of packages To check your packages within your venv, simply type: conda list 1.2.5 Overview of every venv To check all the venv I created, simply type: conda env list 1.2.6 Execute any python skript In order to execute a Python-script, you will need to head towards the directory that the .py-file is and - then - type in: cd go-to-the-dir-where-your-file-is python my-script.py 1.2.7 Deactivate the venv After you have completed what you wanted, you will need to shut-down the venv. Simply type: conda deactivate 1.2.8 Delete a venv Simply type: conda env remove -n my-new-env 1.3 How to efficiently manage a Project? In today’s world, it is from utmost importance to be able to explain &amp; document your work, otherwise you cannot convince your managers / higher-ups of your work. Ideally, your documentation should be done in an environment that can be accessed anywhere! And what better tool nowadays than a website, since everyone has access to a computer with a browser today! =) 1.3.1 Create a Website on Github (for free) Für mich: https://www.youtube.com/watch?v=xt3-JgAxWgE For visitors, I recommend you to watch this tutorial: https://www.youtube.com/watch?v=m5D-yoH416Y 1.3.2 R Markdown Syntax Es folgt eine Zusammenfassung der wichtigsten Markdown-Sytnax, damit ich schnell gute Anleitungen &amp; Tutorials für jegliche Themen meiner Wahl verfassen kann. 1.3.2.1 Headers Headers sind die &lt;h1&gt; bis &lt;h6&gt; in der HTML-Sprache. Diese werden hier mit Hashtags # geschrieben. Es gibt folgende Möglichkeiten: # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style: Alt-H1 ====== Alt-H2 ------ 1.3.2.2 Wichtige Textausschnitte markieren Wenn ein Autor einen Text verfasst, dann kann es sehr hilfreich für einen Leser sein, wenn die Hauptbotschaften hervorgehoben werden. Es gibt in Markdown grundsätzlich sieben Wege, um dies zu tun: Italics, aka kursiv, mit *asterisks* or _underscores_. Fettgedruckt, aka bold, mit **asterisks** or __underscores__. Kombiniere fett &amp; kursiv mit **asterisks and _underscores_**. Wörter unterstreichen mit HTML-Tag &lt;u&gt;dies ist wichtig&lt;/u&gt;! Durchgestrichen mit zwei tildes. ~~Scratch this.~~ (= drücke Option + n) Codes hervorheben mit Appostroph. `&lt;h1&gt;` (= drücke shift + ^-Taste) Ganze Code-Chunks mit drei Appostrophs. ```noob``` Als Output, sieht es so aus: Fettgedruckte Wörter Italics, dh kursiv Kombination aus fettgedrucktem und kursiv. Dies ist nicht korrekt gelöst! Du bisch en spasst, just kiddin du Esel ;) Codes im Text hervorheben, also zum Beispiel &lt;h1&gt;. Sogenannte Code-Chunks, wie oben verwendet, braucht einen neue Zeile, also: This is a Code Chunk, where you can write your R-Code, for example. 1.3.2.3 Listen Oftmals ist es von Vorteil, wenn jeder Schritt einer Erklärung oder Anleitung mit Hilfe von Listenpunkten heruntergebrochen wird. Ein Mensch versteht dann viel besser, was er zu tun hat, wenn er etwas Neues / zum ersten Mal macht. Wie du es bereits aus HTML kennst, gibt es sogenannte Unordered Lists (HTML-Code wäre ul) oder odered Lists (HTML-Code wäre ol). Ausserdem können Listen innerhalb von Listen erstellt werden, wenn ein Arbeitsschritt noch weiter heruntergebrochen werden kann: 1. Erstes Element einer Ordered List, aka das erste &lt;li&gt;&lt;/li&gt; im HTML-Code. 2. Zweites Element einer Ordered List, aka das zweite &lt;li&gt;&lt;/li&gt; im HTML-Code. * Unordered sub-list. ACHTUNG: Drücke 2-mal auf die Tab-Taste, sonst wird keine Einrückung gemacht! 1. Bemerke, dass ich hier absichtlich wieder bei &quot;1&quot; beginne, weil ich dir zeigen will: Actual numbers don&#39;t matter, just that it&#39;s a number 1. Ordered sub-list 2. Vierter Listenpunkt. Alternative für Unordered Lists: * Unordered list can use asterisks - Or minuses + Or pluses Erstes Element einer Ordered List, aka das erste &lt;li&gt;&lt;/li&gt; im HTML-Code. Zweites Element einer Ordered List, aka das zweite &lt;li&gt;&lt;/li&gt; im HTML-Code. Unordered Sub-List. ACHTUNG: Man muss 2-mal auf die Tab-Taste drücken, sonst wird keine Einrückung gemacht! Bemerke, dass ich hier absichtlich wieder eine “1” im Code schreibe, weil ich dir zeigen will, dass die ordered List mit einem 3) beginnen wird! Ordered Sub-List. ACHTUNG: Auch hier muss man wieder 2-mal auf die Tab-Taste drücken, sonst wird keine Einrückung gemacht! Vierter Listenpunkt. Alternative für Unordered-Lists: Unordered list mit der Asterisk-Schreibweise. Oder mit einem Minus-Zeichen. Oder mit einem Plus-Zeichen. 1.3.2.4 Hyperlinks Häufig gibt es im Internet sehr gute Zusammenfassungen oder Konzepte, auf welche du referenzieren kannst, indem du einen Textausschnitt mit Hilfe von einem Hyperlink hervorhebst: [Dies ist zum Beispiel die Zusammenfassung](https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet#h4), auf welche ich mich basiere, um dir Markdown beizubringen. Ich kann so auch - zum Beispiel - auf die [Webseite von Tee](https://www.audiophil-dreams.com/) verlinken. Dies ist zum Beispiel die Zusammenfassung, auf welche ich mich basiere, um dir Markdown beizubringen. Ich kann so auch - zum Beispiel - auf die Webseite von Tee verlinken. 1.3.2.5 Bilder Wenn jemand eine Erklärung erhält, ist es unentbehrlich, auch Bilder zu verwenden, denn Visualisierungen von Konzepten sind oftmals extrem nützlich, um eine Intuition / Verständnis für Neues zu entwickeln. Bei einer Anleitung können zum Beispiel Screenshots verwendet werden, um die schriftlichen Erklärungen mit Bilder effizient zu unterstützen. Es gibt grundsätzlich vier Arten, um Bilder in ein Markdown-Dokument einzufügen: 1) Bild aus Internet, &quot;Inline-Style&quot;: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png &quot;Logo Title Text 1&quot;) 2) Bild aus Internet, &quot;Reference-Style&quot;: ![alt text][logo_1] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png 3) Bild aus Root-Directory (= Ordner, indem sich das R-Markdown-File &#39;ZF-Syntax&#39; befindet), &quot;Inline-Style&quot;: ![alt text](path-to-image-here) 4) Bild aus Root-Directory (= Ordner, indem sich das R-Markdown-File &#39;ZF-Syntax&#39; befindet), &quot;Reference-Style&quot;: ![alt text][logo_2] [logo]: path-to-image-here --- Bemerkung: &quot;Reference-Style&quot; ist sehr nützlich, insbesondere, weil du die References basically zuunterst in das R-Markdown Dokument einfügen kannst. Nehmen wir zunächst den Fall von einem Bild aus dem Internet: Die Inline-Methode ist einfach: Die Reference-Methode kann auch verwendet werden, um den gleichen Output zu generieren: Als nächstes, nehmen wir ein Bild, welches von meinem eigenen Computer stammt. Ich hatte ich VTX eine Frage bezüglich meiner Domain geschrieben und musst Ihnen einen Screenshot einer meiner E-Mails angeben: Mit Inline-Methode: Mit Reference-Methode: 1.3.2.6 Tabellen Für gewisse Konzepte, eignen sich Tabellen besonders gut, insbesondere wenn es um die übersichtliche Darstellung eines Konzeptes geht: | Produkt | Beschreibung | Preis | | ------------- |:-------------:| -----: | | Focal Sopra | Lautsprecher | CHF 12&#39;000 | | Focal Chorus | Lautsprecher | CHF 1&#39;200 | | B&amp;W 800 D3 | Lautsprecher | CHF 30&#39;000 | Alternativ kann auch mit weniger Zeichen gearbeitet werden, allerdings sieht die Tabelle - als Code - nicht besonders leserlich aus: Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Hier wäre die Muster-Tabelle für die Preise von Lautsprechern: Produkt Beschreibung Preis Focal Sopra Lautsprecher CHF 12’000 Focal Chorus Lautsprecher CHF 1’200 B&amp;W 800 D3 Lautsprecher CHF 30’000 Alternativ kann die gleiche Tabelle mit weniger Syntax erzielt werden: Produkt Beschreibung Preis Focal Sopra Lautsprecher CHF 12’000 Focal Chorus Lautsprecher CHF 1’200 B&amp;W 800 D3 Lautsprecher CHF 30’000 Bemerkung: Man kann innerhalb der Tabelle die Textausschnitte weiterhin mit Markdown erweitern, also - beispielsweise - fettgedurckt oder kursiv schreiben. 1.3.2.7 Blockquotes Zitate oder - in Emails - können Fragen der gegenüberstehenden Person besser hervorgehoben werden und tragen zum Verständnis bei: &gt; Herr Mayer, was sind die Vorteile des Optimize-Audio Konzeptes? &gt; &quot;Comerades&quot; isn&#39;t just a word. It refers to heart and believing in each other. - Mavis Herr Mayer, was sind die Vorteile des Optimize-Audio Konzeptes? Sie profitieren von Synergie-Effekten, welche die Leistung einer Anlage qualitätsmässig vervielfacht. “Regrets is a powerful poison. The more you harbor those feelings, the harder it is to move on.” Amamya 1.3.2.8 HTML in Markdown Wie bereits in einem oberen Kapitel gesehen, lässt sich R Markdown auch mit einfachem HTML kombinieren: &lt;ul&gt; &lt;li&gt;Definition list&lt;/li&gt; &lt;li&gt;Is something people use sometimes.&lt;/li&gt; &lt;li&gt;Does *not* work very **well**. Use HTML &lt;em&gt;tags&lt;/em&gt; instead.&lt;/li&gt; &lt;/ul&gt; Definition list Is something people use sometimes. Sometimes, does not seem to work very well with normal R Markdown. Use HTML tags instead. 1.3.2.9 Horizontale Linien Um Themen voneinander abzugrenden und schöneres Layout zu haben, empfiehlt sich, mit Linien zu arbeiten, sobald ein neues Thema beginnt. Es gibt drei Möglichkeiten, um horizontale Linien einzubauen: 1) Verwende 3 Hyphens --- 2) Verwende 3 Asterisks *** 3) Verwende 3 Underscores ___ Hyphens Asterisks Underscores 1.3.2.10 Videos einfügen In R Markdowngibt es keinen direkten Weg, um Videos direkt einzubetten. Am einfachsten geht es mittels HTML-Code, bei dem es grundsätzliche eine Möglichkeit gibt: Füge ein Bild mit einem Link zum Video hinzu: &lt;a href=&quot;http://www.youtube.com/watch?feature=player_embedded&amp;v=YOUTUBE_VIDEO_ID_HERE &quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg&quot; alt=&quot;IMAGE ALT TEXT HERE&quot; width=&quot;240&quot; height=&quot;180&quot; border=&quot;10&quot; /&gt;&lt;/a&gt; Mit der Bild-zum-Video-Methode erhalte ich: 1.3.2.11 LaTeX für mathematische Formeln Es gibt die Möglichkeit, mittels der Programmiersprache LaTeX mathematische Formeln aufzuschreiben. Indem man das Dollarzeichen $ verwendet, versteht R Markdown, dass man die Sprache LaTeX verwendet: - Für ein &quot;Block-Element&quot; - dh man beginnt auf einer neuen Zeile - verwendet man zwei Dollarzeichen: $$\\underbrace{z-score = \\frac{\\overbrace{x}^{observation}-\\overbrace{\\mu}^{population-mean}}{\\underbrace{\\sigma}_{population-sd}}}_{Standardisierungs-Formel}$$ - Für ein &quot;Inline-Element&quot;, verwendet man nur ein Dollarzeichen: Der Mittelwert wird oftmals als $\\muh$ bezeichnet und die Varianz $\\sigma$. Beispiel für ein “Block-Element”: \\[\\underbrace{z-score = \\frac{\\overbrace{x}^{observation}-\\overbrace{\\mu}^{population-mean}}{\\underbrace{\\sigma}_{population-sd}}}_{Standardisierungs-Formel}\\] Illustration des “Inline-Elements”: Der Mittelwert wird oftmals als \\(\\mu\\) bezeichnet und die Varianz \\(\\sigma\\). 1.3.2.12 Code Chunks Inputs / Outputs anzeigen / verstecken mit Eval, Echo, Results Bei Code Chunks, möchte man manchmal den Code absichtlich zeigen, aber manchmal auch - zum Beispiel - nur eine Graphik, ohne dass man den Code (= Input) sieht. Deshalb gibt es in RMarkdown diverse Befehle, die man als sogenannte Argumente (= Inputs) angeben kann: Wenn man nur den Input-Code anzeigen will, jedoch nicht den dazugehörigen Output (= z.B. um den Code anzuzeigen, wie man ein Bild exportiert, aber dieses Bild nicht effektiv exportieren will), verwendet man das Argument eval = FALSE. Hier ein Beispiel anhand meines Masterarbeit-Codes, welchen ich nur zeigen, aber nicht runnen möchte: ############### ## Step 1: Preliminary basic steps ###### 1) Load some packages, set working directory and clear the workspace. ###### rm(list = ls()) # clear workspace setwd(&quot;~/Uni/Masterstudium/Masterarbeit/final-code&quot;) # set wd # load some packages: library(dummies) library(foreign) library(stargazer) # important to create tables library(readstata13) # important to load the ddset library(haven) library(plyr)# to use functions like count() # Note: install.packages(&quot;readstata13&quot;) first before you can load them! Wenn man nur den Output des Codes zeigen möchte, aber nicht den dazugehörigen Input (= Code), dann verwendet man das Argument echo = FALSE. Eventuell müssen auch noch message=FALSE und warning=FALSE verwendet werden, falls zusätzliche Hinsweise beim evaluieren / runnen des Codes auftauchen (im unteren Beispiel ist dies der Fall!). Hier ein Beispiel, bei dem ich nur die erstellte Graphik zeigen möchte: Abschliessend gibt es noch den Fall, dass man weder den Input, noch den Output eines Code Chunks nicht zeigen möchte, aber der Code dennoch evaluiert wird. Dies tut man mittels des Arguments results = hide &amp; include = FALSE. Beispielsweise hast du in deiner MA ganz viele male den Code View = data verwendet. Dadurch gibt es ein Fenster, welches durch Pop-Up auftaucht. Oftmals ist dies praktisch, wenn man nach bugs im Code sucht, aber schlussendlich sollten diese im Endergebnis nicht angezeigt werden. Hier verwende ich einfach basic Mathe, um dir zu zeigen, dass es funktioniert, obwohl ich einen Code Chunk im RMarkdown-Dokument eingefügt habe: no code shows! 1.3.2.13 Hyperlinks Hier ein Link zu einem guten Youtube-Video. 1.3.2.14 Code-Snippets When writing Code, I recommend you to use .Rmd-Documents, which will allow you to combine both, text, as well as Code (in R, Python, or any other language of your choice). Actually, this document is written in .Rmd-Files, where I combine the text, with so-called “code-snippets”, like the follwing: This is a Code-Snippet When writing Code in R-Markdown, it will be useful for you to know when: to execute code wihtin a code-snippet and where not. 1.3.2.14.1 Hide Source Code This will be done with echo = FALSE: ## [1] 2 With figures, you need fig.show = 'hide' in R: plot(cars) To hide warning-messages, just use message = FALSE: message(&quot;You will not see the message.&quot;) 1.3.2.14.2 Execute a Code-Chunk without showing any Output You will get an output, e.g. the code will execute, but you will not show the code. 1.3.2.14.3 Do NOT execute a Code-Chunk If you want to show code for demonstration purposes - like on this Website - you will probably want to only show the code, but not execute it. This is also possible with eval=FALSE: 1.3.2.15 RMarkdown- VS. Markdown-Files Verwende &lt;ins&gt;&lt;/ins&gt; als HTML, um Texte zu Unterstreichen. Achtung : Der HTML-Tag &lt;u&gt;&lt;/u&gt; geht nicht in .rm-files. Lustigerweise funktioniert &lt;u&gt;&lt;/u&gt; jedoch in .Rmd (= R Markdown) 😂 1.3.2.16 Nützliche Hexadecimals Hexadecimal code for the left square-bracket = &amp;#91; –&gt; Ich muss das so machen mit den eckigen Klammern, weil [] wird für Links &amp; Bilder verwendet in Markdown  Hexadecimal code for the right square-bracket = &amp;#93; –&gt; Ich muss das so machen mit den eckigen Klammern, weil [] wird für Links &amp; Bilder verwendet in Markdown  1.4 Terminal-Commands: A terminal will be essential for your projects, since you will - oftentimes - install packages or move files around your repositories with it. Here, you will find the most useful things you should know when using the Terminal. 1.4.1 Aktuelle Position // Directory? For Mac: pwd = print working directory For Windows: dir = this is the same command as pwd, but dir is for Windows 1.4.2 Showing the child-directories inside the directory you are currently in? ls = prints all the child-directories (= one layer deeper of the path) from the parent-directory (= current directory you are in with your terminal) you are currently in. 1.4.3 Delete everything you wrote in your Terminal up until now? clear = clears the terminal 1.4.4 Change directory? cd = change directory cd .. = go back one directory. 1.4.5 Creating a new directory? mkdir new-folder-1 new-folder-2 new-folder-3 = This creates 3 new folder within the current (working-) directory you are currently in. 1.4.6 Create a new file? touch index.html app.css == This will create an index.html, as well as a app.css-file within the current (working-) directory you are currently in. 1.4.7 Remove files? rm index.html app.css capture.png = This will delete the index.html-, the app.css and the capture.png-files from the current (working-) directory you are currently in. 1.4.8 Open the current directory you are in? open . = opens the current directory you are in 1.4.9 Terminal Magic-Commands for being faster? Trick #1: hit the “Tab-Taste” == will automatically auto-fill the name of the file / directories etc. Example: Type cd Dok + “Tab”-Taste –&gt; auto-fill activates –&gt; im Terminal steht dann der automatisch ausgefüllte Name des Files / Directories, zum Beispiel cd Dokumente bzw. cd Name_Of_Child_Directory Trick #2: How to find a path of a directory that is situated very deeply in your local computer? Example: Type cd + drag-&amp;-drop the folder that is deep in your computer with the file in it. 1.4.10 Syntax im Terminal *-Zeichen == Represents “all files”. 1.4.10.1 Beispiel: Delete all Files in a folder that start with the letter a? To delete all files in a folder that start with the letter a, then you should write: /folder/a* .-Zeichen == use the . character to represent the current folder. ~-Zeichen == represents the “home directory”. 1.4.10.2 Beispiel: how to return to your home directory? You should use: cd ~ 1.5 How to activate &amp; use Python? Python can be executed on your local computer via a Jupyter Notebook, which can be accessed through an IDE. R, Visual Studio Code or PyCharm are examples of IDEs. Let’s assume, that we took PyCharm as our IDE. We do the following steps: Use the Terminal within PyCharm. Once you opened the PyCharm-Terminal, go to the directory that will be used for the Jupyter Notebook, by typing something as cd /some_folder_name. Finally, type in jupyter notebook in the Terminal to launch the Jupyter Notebook App. The notebook interface will appear in a new browser window or tab. 1.6 Git &amp; Github: pull request = “Take some changes from a particular branch and bring it into another branch.” Achtung: es ist eine Request, es wurde noch nichts gemerged! Für das brauch es noch merge als zusätzlichen Befehl. fork a repo = “Take someone else’s repo - because you love it 😊 - and put it into your own list of repos, in order to be able to edit it yourself without affectig the original repository of the owner.” commit = save hash = unique identifier in the history of files. A has is a huge string composed of characters (= Buchstaben) &amp; numbers and is used when using a version-control software, such as Git. git add = Der Befehl git add wird zu vielen verschiedenen Zwecken eingesetzt. Man verwendet ihn, um: neue Dateien zur Version-Control hinzuzufügen, Dateien für einen Commit vorzumerken, UND verschiedene andere Dinge – beispielsweise einen Konflikt aus einem Merge als aufgelöst zu kennzeichnen. Leider wird der Befehl git add oft missverstanden. Viele assoziieren damit, dass damit Dateien zum Projekt hinzugefügt werden. Wie Sie aber gerade gelernt haben, wird der Befehl auch noch für viele andere Dinge eingesetzt. Wenn Sie den Befehl git add einsetzen, sollten Sie das eher so sehen, dass Sie damit einen bestimmten Inhalt für den nächsten Commit vormerken (= also Punkt (2) ist vor allem relevant in der obigen Liste. How to tell the original owner you want to merge your changes that you made back into their orignal repo and implement them those changes into their original work // repo? Look at the youtube video from Coding Train ab 9:35-11:50 To see how to refer to issues &amp; bugs in your code directly via your commit-command, look at the youtube video ab 6:35-7:40 and to diretly close issues, because you resolved it, look look at the youtube video ab 7:40-8:55. What is a remote? A remote is a duplicate instance of your repository (on your local computer) that lives somewhere else on a remote server (like Github). 1.6.1 First time using Git &amp; Github There are specific Git-commands that you need to know, when you begin to start to work with Git and Gibthun for the first time. Note that all these Git-commands need to be typed within the Terminal on your local computer. git config --list = Sehr wichtig, wenn du Git zum ersten Mal via einem neuen Computer runst! Dieser Befehl zeigt dir, welchen Username &amp; Email du aktuell verwendest (schaue bei user.name &amp; user.email, ob es deine Github Anmelde-Daten sind). Es ist key - insbesondere, wenn du neu mit Git beginnst - dass diese Parameter mit deinen Github Anmelde-Daten übereinstimmen! Ansonsten musst du immer git clone machen und die ältere Version in einen “alt”-Ordner tun, was extrem mühsam ist. Wenn du noch keinen user.name hast, dann gebe folgenden Code in die Command-Line ein: git config --global user.name 'Dein_Github_UserName'. Beachte: Schreibe den Namen mit die Anführungszeichen! Wenn du noch keinen user.name hast, dann gebe folgenden Code in die Command-Line ein: git config --global user.name 'deineEmail@email.ch'. Beachte: Schreibe die Email ohne die Anführungszeichen! Check if it worked?: Gebe wieder den Befehl git config --list und schaue bei user.name &amp; user.email, ob dort deine Github Anmelde-Daten übernommen wurden. git push = this is the act of sending to Github. git pull = this is the act of receiving from Github. 1.6.2 2 Key concepts in Git Before starting to work with Git, you need to understand that there are 2 ways of starting a project: 1) Create a `remote` repository on Github and then `cloning` it - via Git - on your local computer to work from there. 2) Creating a repository `locally` on your computer and then - aftre a few months working on this repository - adding it to Github. Depending on which of those 2 different ways you choose to start a project (create a repo right from the get-go on Github VS. work locally and then - after some time - push everything to Github), the Git-Commands will slightly differ. 1.6.2.1 Start Project via Github (remote-possibility) What are the Git-Commands, if you start your project directly by creating a Repo on Github (= possibility 1) above)? git clone https://github.com/joffreymayer/tageb.git == Will clone your remote directory tageb - which is currently on Github on your local computer, which is simpler // more comfy when working on projects =) git status = Assume that you worked on &amp; modified a file on your local computer that you previously had on Github (you cloned the directory with the file in it on your local computer). With the command git status, Git will check whether there is any changes between your local files VS. the files in the remote directory on the Github-Server // -Website. git commit -a -m \"Test comment for a commit\" = If you changed a file locally and you are happy with your results, you will need to make a commit (= save) and add all files (= this is why we have an input // argument -a; the concept of adding will be explored in the chapter below, where you want to put a local repo into Github after a few months) and you also want to document, what exactly you modified, if you need to go back to a previous version of your file (= this is why we have an input // argument -m \"comment is here...\"). git push origin master = If you have done some changes locally on your file, you can now push everything on the Github-Website. git log = see, locally, the history of your git commits. Achtung: When running this command, you might - accidentally - run into a dangerous environment called VIM, which is a terminal-based text-editor. The problem when you are in VIM, is that you might not be able to get out of it. -Solution: To get out of VIM, just type in :q and you will get out of it. git remote -v = This will tell you which URL is the remote on which your repository is hosted. Merke: The URL of your repo is assigned to the variable origin in Git. 1.6.2.2 Start Project via local computer (local-possibility) What commands do you need, when you decide - after a few months working locally on your computer - to put everything on Github (= possibility 2)? git init == To get started, you need to go to your repository with your terminal - e.g. set the working directory with cd Joffrey\\dokumente\\my_project) first - and, then, transform your repository to a Git-Repository by just typing git init into your terminal. git add single_file.txt == After you initialized your repository, you will have an empty git-repository. Git will not track the files in your repository (= untracked files), unless you explicitly point them out via the git add command. If you want to add all files quickly // simultanoeusly: git add . Für genauere Theorie // Erklärung dahinter: Siehe Youtube-Video Coding Train ab 2:10-6:03 git commit -m \"Adding a new comment for my commit\": After having pointed out to Git, which files he needs to track, you can do a commit of the changes of the files you modified, like in possibility 1). Achtung, es gibt einen kleinen Unterschied zu possibility 1): das -a (siehe oben) ist verschwunden, weil wir hier add und commit als zwei separate Schritte betrachten. git remote add origin https://github.com/joffreymayer/new-repo = Because our repository is still currently not on Github, we first need to go on the Github Website and create an empty repository. After having done this, you need to tell Git - with the command git remote add origin + copy-pasting the URL “https://github.com/joffreymayer/new-repo” - that this is our local Check if it worked: Type the following into the Terminal git remote -v. It should output the variable name - usually called origin - Note if you want to be fancy: Within git remote add origin, the name origin can be changed to any word you like. This is just the variable name in which your Github-URL will be stored. If there is already a remote with the default name origin but you don’t like the name, you can change the name by: git remote remove origin –&gt; this will delete the remote git remote -v –&gt; just to check if step 1) worked –&gt; should not output anything git remote add noob https:\\\\github.com\\project-1 –&gt; now, re-name the remote and call it noob git push origin master == Finally, you will be able to put all your files into the freshly made remote-repository on Github. git pull origin master == Assume that you did changes remotly on Github but not yet locally on your computer. This does not matter, since you can just enter the command git pull origin master to be able to retrieve the changes that you did remotly on Github onto your local computer =) 1.6.3 Working with Branches Tutorial for branches? Look at youtube-videos from Coding Train. git branch new_branch == this will create a branch locally on your computer git checkout new_branch == this will tell Git: “ah, he wants to go into the branch called ‘new_branch’”. git branch == this will give you a list of all the branches you ave created locally. Furthermore, it will tell you on which branch you currently work on. How to merge the changes you made on a separate branch to the master-branch (= main branch)? git branch new_branch == this will create a branch called new_branch locally on your computer. git checkout new_branch == You will tell Git: “I now want to work on this newly created branch called ‘new_branch’”. git checkout master == After you are happy with the changes you did in new_branch you will need to prepare for the merging by switching to your main-branch, which is the master-branch. git merge new_branch == Since you currently are in the master-branch, Git will know that you want to merge new_branch into the master-branch. 1.6.4 Set-Up an SSH-Key for Github Whenever you will work with Git and Github, you will always need to type in your password, whenever you push something to the Github-Cloud. *That’s why you should start using an SSH-Key, giving Github your public-key and your computer the private-key. This Youtube-Video from WebDevSimplified will show you how to do it =) That was it 😎 1.7 LaTeX This bookdown is created via a .Rmd-File. For a Markdown to be able to read some mathematical formulas, we will need the LaTeX-language. 1.7.1 What is the LaTeX-Code for \\(\\hat{y}\\)? Simply type: $\\hat{y}$ 1.8 Wörterbuch Jargon of a domain - like in Artificial Intelligence, Software Development or Economics - is one of the biggest challenges if you truly want tounderstand and master a new subject. Scientists love to create sophisticated new words for new discoveries, methods or conclusions that they came to uncover in their respecitve fields. The problem with that, is that it acts as a huge market-barrier to those that want to enter the world many years later. You can see this even for “non-scientific” things, like the Pokemon franchise, that grow from 150 Pokemon to almost 1’000 Pokemons. The problem is, that the audience does not nearly grow as much, as in the beginning and that the new generation is not so enthousiastic about it, partly because the field has become too large for noobies. That’s why it is form utmost importance to know each field’s jargon. I firmly believe that it is one of the biggest chalenges to overcome in order to be good at something. Because when you are able to “decode” the jargon, you can start “understand” the things. And when you understand, you can start applying, and that is where the fun truly begins :) 1.8.1 Allgemein Overfitting = The problem of overfitting occurs when a model is too complex and captures too much detail from the training data. This can lead to the model being unable to generalize to new, unseen data. Fazit: This is like the problem of “low external validity”. You may have a high “internal validity”, but external validity is low! 1.8.2 Synonyme Source == Programming Language (any kind of) 1.8.3 Data Science VS. mein Economics-Studium Was ist der Unterschied zwischen einem Data Scientist VS. was ich in meinem Economics-Studium gemacht habe? Erkenntnis: Machine Learning wird als eine “Black Box” betrachtet, wo man den X-Variablen des Modells keine grosse Beachtung schenkt: man bringt sie einfach ins Modell rein. Im Kontrast dazu, sind Ökonomen mehr dazu getrimmt, mit Hilfe eines theoretischen Modells, die “richtigen” X-Variablen zu selektieren. Man nennt dies dann auch “structural econometrics”. What is artificial intelligence? Artificial intelligence is a branch of computer science that deals with the creation of “intelligent agents”, which are systems that can reason, learn, and act autonomously. 1.8.4 IDE explained What is an IDE? Abkürzung: IDE == Integrated Development Environment Definition: An integrated development environment (IDE) is software for building applications. It combines common developer tools into a single graphical user interface (GUI). Example: R, PyCharm oder Visual-Studio Code are all IDEs. Think of it as a modern Dream Weaver! :) Was ist IDE PyCharm? Das ist das RStudio von Python. 1.8.5 Restful API In a Rest API, every What are Rest APIs? Synonym: Restful APIs Defintion: API stands for Application Programming Interface. It is a way for 2 computers to “talk” // communicate with each other. During the talk, one computer sends a “request”, while the other sends the “response”. How is this possible? In order for you to use an API (e.g. to enable the “talking” between the computers), you need to write code to explicitly “request” data from a server // computer. Most APIs in the world are “restful”, e.g. they follow some set of rules // constraints known as “representational state transfer”, which is the “gold-standard” for API-development since the early 2000’s (invented by Roy Fielding, in its PhD dissertation). Example: If you are a “noob”, you can retrieve the NASA-images of asteroids by looking at their websites. However, you could also use NASA’s “Rest API” to get the data via a .json-file. How it works?: From a architecture point a view, a Rest API, organizes “data entities” // resources into a bunch of unique URLs. Well, technically speaking, they are NOT URLs, but rather “URIs” (aka “uniform resource identifier”), which identifies exactly each data-resource on the server. A client can then retrieve // get the data about a resource by making a request to the server // “endpoint” (over the “http”-protocol). This request has a very specific format, see this youtube-video (ab 1:03). “Stateless” Architecture (key to know): The two parties // computers don’t need to store any information about each other AND every “request-response”-cycle is independent from another “request-response”-cycle. These two characteristics are important, because it leads to an application that is predictable AND reliable. Quelle: Pokemon-API Zusammengefasst in meinen Worten: Ein API sind Befehle, die du gibst, um mit einer Datenbank (z.B. auf einem Server) zu kommunizieren ODER ein Package (in R oder Python) zum runnen zu bringen. Ganz einfach xD Background Client-Server Architecture: most of the applications these days, follow this architecture. Client == App itself == Front-End Server == Back-End Communication between the Sever &amp; the Client (= App) happens via API by using the Http Protocol. Example: if the App wants to access the particular data of a customer, it sends a request to the server via http-protocol. So, when does the Rest API comes into play? –&gt; the Rest API is a standard that established itself in the industry, when communaction between client &amp; server –&gt; these are the CRUD Operations, which are by definition: GET == getting the data from the server POST == creating data PUT == updating data DELETE == deleting data 1.8.5.1 API Example Beispiel eines API in Python? Das Modul // Package sktime verwendet einen ähnlichen API, wie die berühmte Machine-Learning sklearn-Library. Hierbei wäre der Programmierer (= Du) als Forecaster verstanden, welcher via sogannten methods (zum Beispiel die fit-method, um das Modell zu trainieren // estimaten) mit der application (= hier: Python) interagiert. 1.8.6 Magic Commands in Python “Magic Commands” in Python, mit denen du unglaublich schnell herausfinden kannst, was eine Funktion überhaupt tut UND welche Inputs in eine Funktion gehören? shift + Tab → wenn du nicht weisst, was eine Funktion // Method tut Tab → innerhalb einer Funktion, um eine Übersicht zu allen Inputs zu erhalten! xD 1.8.7 Pipe-Operator in R What is the Pipe-Operator in R and what does it do? The Pipe-Operator in R looks like this: %&gt;%. It takes in an input and “transports” it into another function to use the input and produces an output. This verbal explanation can be best illustrated via a code-example in R: library(tidyverse) result &lt;- mtcars %&gt;% group_by(cyl) %&gt;% summarise(meanMPG = mean(mpg)) Quelle: A Guide to the Pipe in R 1.8.8 Docker Was ist Docker und was ist der Vorteil davon? Docker wird verwendet, um ein Virtual Environment zu bilden, welches - wie ein Container - dir Punktgenaue Versionen von bestimmten Packages und Programmiersprachen (Python etc.) liefert. Docker läuft über Open Shift, welches eine Art Management-Programm für Docker ist (so viel ich das verstanden habe…). 1.8.8.1 Definitions Image - An image is an environment that has been built from a series of instructions called a DockerFile. Images can be prebuilt and hosted on DockerHub (similar to how GitHub hosts version controlled software files). The image is needed to run a Docker Container. Container - A container is a virtual environment that combines a Docker Image with software (files) to run an application in a controlled environment (a reproducible software environment created virtually from the Docker Image). DockerHub - An online community for storing and sharing container images. Has Public and Private repositories for image storage. It’s basically a Cloud, like Github. Fazit: DockerFile –&gt; Docker Image (can be pre-built and hosted on the DockerHub-Cloud) –&gt; Docker Container 1.8.9 Tensorflow and PyTorch Definition? PyTorch and TensorFlow are far and away the two most popular Deep Learning frameworks today. While TensorFlow has a reputation for being an industry-focused framework and PyTorch has a reputation for being a research-focused framework. Which one is more popular in the Deep-Learning Community? PyTorch VS. Tensorflow popularity As you can see, the adoption of PyTorch was extremely rapid and, in just a few years, grew from use in just about 7% to use in almost 80% of papers that use either PyTorch or TensorFlow. The researchers find that the majority of authors who used TensorFlow in 2018 migrated to PyTorch in 2019 (55%), while the vast majority of authors who used PyTorch in 2018 stayed with PyTorch 2019 (85%). This data is visualized in the Sankey diagram above, where the left side corresponds to 2018 and the right side to 2019. 1.8.10 Data Pipeline Was ist eine Data-Pipeline? After streaming your data (in real time) // downloading your data from a provider, it’s basically a way to automate the process of data cleaning in order to be able to get the plots // models from your “dirty data” in a fraction of the time you would spend, if you would do the data cleaning “by hand” yourself. Youtube-Video: What is a Data-Pipeline? 1.8.11 Fundamental-Daten Was versteht man unter Fundamental-Daten? Fundamental-Daten sind effektiv messbare Daten, die über Datenbanken accessible sind und welche als Proxy - beispielsweise in Regressions-Analysen - verwendet werden können. 1.8.12 Data-Flow Was versteht man unter dem Data Flow? Mit dem Data Flow sollen folgende Fragen beantwortet werden: Welche Daten werden wo geholt &amp; wieso? Wie werden die Daten anschliessend verarbeitet? Was ist der End-Output, nachdem die Daten - beispielsweise - in einem ML-Modell verwedet wurde? Zur Illustration des Data Flows, gab es hierzu im Wissensaustausch auch eine Bild: Beispiel zum Data Flow 1.9 Synonyme 1.9.1 Data Science bias: batch: sample-size when you train your model with a dataset Granularität (Beispiel) = Angenommen, man möchte wissen, wie viele Tage, Stunden, Minuten und Sekunden innerhalb von 20’044 Sekunden enthalten sind → hier haben wir also Granularität von 4 → Lösung: 4 Tage, 18 Stunden, 37 Minuten, 44 Sekunden Kalibrierung des Modells == Wie wurde das Modell programmiert im Allgemeinen? → Dazu gehört - bei uns - die Anpassung der Prognose. label == true y-value Pop == “Pop” is simply a word that means “to drop something”. For example, df.pop('date') means that I am dropping the column called “data” from a dataframe called “df”. Fancily, if you write df.pop('date') and save it into another variable, then you can only select this dropped column from the ddset. Example: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pop.html relativ zu == im Verhältnis zu shape == dimension target == predicted y-value Treiber == x-Variablen Volume == Speicherplatz weights == estimated coefficients 1.9.2 Python Module == Python-File PyPA == Python Packaging Authority PyPI == Python Package Index 1.9.3 IT &amp; Software-Development Abhängigkeitsprobleme == 1) Probleme bei den Versionen des Packages; 2) Packages werden vorausgesetzt, sodass gewisser Code dann einen Error printed, wenn man ihn nach 2 Jahren wiederverwenden will. agnostisch == Im IT-Umfeld hat das Wort agnostisch eine besondere Bedeutung. Es bezieht sich auf etwas, das soweit verallgemeinert wird, dass es auch unter verschiedenen Umgebungen funktionieren kann. Beispiel: Eine Plattform-agnostische Software läuft unter jeder Kombination aus Betriebssystemen und Prozessorarchitekturen. cached == store away for future use Client == Damit ist häufig der Browser gemeint, wenn er Abfragen an den Server gibt. composite == custom fetch data == retrieve data Remote == Das ist z.B. ein Server / eine Maschine, die du nicht lokal bedienst. Rest API == Restful API instanziieren == Das Erzeugen eines Objekts in der objektorientierten Programmierung. 1.10 Projekt-Management Sitzungen müssen gut dokumentiert werden! Insbesondere ist hervorzuheben, in welche Richtung das Projekt geht. 1.10.1 Meetings Was ist das Ziel eines Meetings? Das Ziel des Meetings ist es, dir &amp; deinem Team die nötige Basis zu geben, damit du in der folgenden Zeit bis zum nächsten Meeting genau weisst, was du zu tun hast. Am wichtigsten ist hier mit dem “Boss” zu reden und klar definieren, was er erreichen will im Projekt. Dies gibt die globale Richtung des Projektes an. Es ist exterm wichtig, mit dem Boss diese Ziele zu definieren, da die Planung des gesamten Projektes in dieser Phase hier noch definiert werden kann. Nachdem diese anfängliche Gliederung getätigt wurde, kann im Verlauf des Projektes nicht mehr viel davon abgewichen werden. Deshalb hängt die Gesampt-Performance eines Projektes sehr stark von der anfänglichen Planung ab! Während des Projektes gibt es dann immer wieder folgende Punkte zu prüfen: Sind die getätigten Schritte im Einklang mit dem Endziel des Bosses? Wichtig ist zu rechtfertigen, was du gerade am tun bist und warum genau du das tust. Aufklärung, damit alle auf dem gleichen Informationsstand sind Goal: Wo stehen wir? Wie sieht die Situation mit dem Notebook aus? Gibt es noch offene Lücken im Wissen eines Team-Mitglieds, welches von anderen Mitarbeitern gedeckt werden müsste? –&gt; Goal: Kein Mitglied isolieren, jeder muss sich integriert fühlen. Insbesondere Wert auf Papers &amp; Daten setzen hier Was sind weitere offene Optionen und inwiefern eignen sich diese Ansätze für unser Ziel? Goal: Aufzeigen, dass wir noch weitere Alternativen haben, als nur Phillip’s Notebooks Sonstige Unklarheiten / Bemerkungen / Inputs / Vorschläge? –&gt; Goal: letzte Informationsasymmetrien beheben Was ist ein Kanban? In der BWL-Sprache ist Kanban ein Vorgehensmodell zur Softwareentwicklung. Ziel #1: Das Meetings soll die Anzahl an paralleler Arbeiten, (= der Work in Progress (WiP)) minimiert // begrenzt werden und somit kürzere Durchlaufzeiten erreicht. Ziel #2: Das Meeting soll die Probleme – insbesondere Engpässe – schnell sichtbar machen. 1.11 Job-Interviews Vorbereitung Data-Science Interviews: Diese Webseite hat einen kompletten Guide mit Videos / Artikeln zu extrem vielen Key-Concepts gesammelt und zur Verfügung gestellt. 1.12 Ausblick Hier liste ich alles auf, das ich in nächster Zeit ausprobieren möchte und - meiner Meinung nach - grossen Wert besitzt: Tutorial (Artikel von Towards-Datascience): Build Animated R-Visualizations "],["foundatios-of-programming.html", "Chapter 2 Foundatios of Programming 2.1 How to Program? 2.2 Tools to get started", " Chapter 2 Foundatios of Programming 2.1 How to Program? In dieser ZF, werde ich mit Hilfe des Youtubers The Coding Train versuchen, meine Programmierkenntnisse zu erweitern. 2.2 Tools to get started Download the Program Processing: The Coding Train uses this to make all of his example. The programming language of Processing is Java. Processing Foundation Learning Processing Book 2.2.1 Definitionen What is an Algorithm?: An Algorithm is a list of instructions. What is Programming about?: Programming is not about hardcoding numbers into functions to create a picture. Programming is about creating a sequence of instructions (= a logic) to execute a task. What is a Command &amp; what are arguments?: The command is a function, while arguments are the inputs of a function // command The coordinate system of a computer VS. the coordinate system in mathematics?: The y-axis of a computer goes down starting the top-right corner, while the x-axis goes to the right starting the top-right corner 2.2.2 Synonyme inputs == arguments function == command cartesian coordinate system == “normales Koordinatensystem”, dh mit der X-Achse für die horizontale Linie &amp; Y-Achse für die vertikale Linie "],["foundations-of-data-science.html", "Chapter 3 Foundations of Data Science 3.1 Find Data 3.2 Questions you need to answer when starting a new Project 3.3 Effective Research: How to find answers quickly? 3.4 How to read Notebooks from other People? 3.5 Machine-Learning Theory 3.6 Programming Theory 3.7 Statistics-Theory 3.8 Appendix for the Future", " Chapter 3 Foundations of Data Science In meinem Data Scientist Job werde ich häufig auf ähnliche Probleme stossen mit der Zeit. Hier habe ich eine Reihe an Fragen aufgelistet, welche ich fähig sein muss, zu beantworten, wenn ich effizient in meinem Beruf sein will! 3.1 Find Data In order to work with data, you also need to have the supply. Here is a list of websites, that offer free to use datasets: Kaggle UCI Machine Learning Repository awesome-public-datasets Google Dataset Search https://datasetsearch.research.google.com/ 3.2 Questions you need to answer when starting a new Project BEFORE you start writing your R-Scripts or Jupyter-Notebooks, you first need to think about several key-things &amp; -questions: - Welche Datenbasis haben wir? - Was ist das Prognoseobjekt? - Welche Metrik hast du in deinen Notebooks verwendet? - Wie ist das Trainings- &amp; Validierungs-Dataset aufgebaut? - Welche Daten sind im Test-Set, welche für den Benchmark verwendet werden? - Welche Prognoseansätze wurden angewendet? - Welche Daten sind effektiv genutzt &amp; welche sind verfügbar? - Etc… Datenbasis // Woher kommen die Daten?: ENTSOE exklusiv. Das ist der Dachverband der TSO (kennen Nachfrage und Angebot) Prognoseobjekt (= y-dach): Stündliche EUR/kWh für Folgetag. Metrik: Zeigt, wie gut das Modell “lernt” (= training error) &amp; “generalisiert” // exrapoliert (= generalization error). Im Notebook von Philipp werden 2 verschiedene Metriken verwendet, nämlich: - Mean Absolute Error, sowie - [Root] Mean Squared Error. Training Dataset: Daten von 2019. Validation Dataset: Random choice of 20% of the hours within this year. Mit welchem true y-value werden die Predictions verglichen? // Was ist der Benchmark? // Test-Set: EFFEX Spotpreise benutzt für Benchmark // true [y-]values. Struktur [der Analyse]: Output: Day ahead Strompreis CH Input: jeweils 24h für ca. 20 Prädiktoren. Reshape: K dimensionen = #Prediktors x 24h Was ist die Daten-Imputation Methode?: Missing Values mit Mittelwerten 3.3 Effective Research: How to find answers quickly? How to find Answers quickly, especially when a Concept is complicated Aus Erfahrung weiss ich jetzt, dass Youtube bisher immer, die beste Quelle war, um mir etwas schnell &amp; effizient beizubringen. Twitter-Community von Wissenschaftlern sind ebenfalls sehr wertvoll. Als Beispiel wäre dieser Twitter-Post vom Prophet-Gründer. 3.4 How to read Notebooks from other People? Es geht am Anfang um die Gesamtübersicht und noch nicht um die Details // Eigenheiten im Code oder im Datensatz! Am wichtigsten ist es, dass du diese Fragen zunächst beantworten kannst, wenn du das Notebook liest. 3.5 Machine-Learning Theory 3.5.1 Difference between Training-, Validation- &amp; Test-dataset Since I started with Machine Learning, I was always confused about the concept of data splitting, e.g. which sub-set of the entire dataset is now considered to be the training dataset, which one is the validation dataset and which one is the testing dataset. In the graph below, you see how it is defined correctly: Training Set VS. Validation Set VS. Test Set Important to note: There is oftentimes confusion between the definition of validation dataset VS. testing dataset, because there is no consens about it. Therefore - and if we take the above picture as the “true” definition - some people will call the validation dataset the “test dataset” and vice versa, e.g. the test data as the “validation data”! xD 3.5.1.1 Validation-Set VS. Test-Set What is the difference between a Validation Dataset and a Testing Dataset? Es gibt keinen Konsens dafür, was nun das validation dataset und welches das test set genau ist. Nima (= mein Mentor bei der SBB) verwendet den Begriff test set für dasjenige Dataset, welches hold out // separat für die (spätere) Prediction verwendet wird. Regel zu Unterscheidung der beiden Terme: Wenn steht: we hold out [name of the set], dann ist es das testing dataset. 3.5.1.2 Reason for a Validation-Set Why do we need validation set? At the end, you want the best model possible. If you want to tune your hyperparameters, you will need your test set. The big problem here, however, is that - if we use the test-set more than once when wanting to find out the best hyperparameters - our model will know the data by heart and the predictions will be too good, but only on this dataset that you are currently using! The model will not generalize well on new data. That’s why we need this additional subsetting of the training-dataset! 3.5.2 Concept of Stratification Angenommen du hast eine kategorische Y-Variable, welche entweder Nullen oder Einsen als Werte annimmt. Nehme nun an, dass - im Training-Dataset - der Anteil der Nullen 60% beträgt. Wenn du nun ein stratified Sample willst, dann wird das Test-Dataset ebenfalls einen Anteil von 60% an Nullen enthalten! Quelle: Train-Test-Split in Sklearn and Cross-Validation 3.5.3 Cross-Validation The concept of cross-validation can be splitted into 2 parts: Step 1: Split your dataset into 1 training- &amp; 1 test-set. Rule of Thumb: Usually, the split is 70-90% training set and 10-30% for the test-set. Code Example: Example of Code for Train-Test Split Step 2: Now, we divide the training set further, such that it will contain - if we assume a 3-fold cross-validation as an example - 3 different validation sets and 3 training sets. Allgemeine Cross-Validation 3.5.3.1 Special Case: Cross-Validation for Time-Series Data Because time-series are ordered, since the flow of time is only going forward, the graph shown above for step 2 is not valid and we need another approach: 3.5.3.2 Why do we use Cross-Validation? There are 2 reasons: It allows us to compare the score (MAPE, MAE or RMSE) of different model set-ups, for example: CV-Scheme changes, e.g. a Time-Series Prophet-Model with a Sliding-Window of 4 Months VS. a Prophet-Model with an Expanding-Window (full past). OR changes in the covariates, e.g. a model that includes an important covariate, while the other model does not. This will allow you to draw conclusions regarding the selection of “the best” model. It allows you to assess the performance of different machine-learning methods. In a classification-setting for example, you can use the confusion-matrix. Cross-Validation for Time Series This picture shows an example of a 3-fold cross-validation for a time-series. 3.5.4 Normalisierung der Daten Was versteht man unter Normalisierung? Was sollte man dabei beachten? Warum wird in einem Pre-Processing Schritt eine Normalisierung durchgeführt? Normalisierung == Subtrahiere den Mittelwert und dividiere durch die Standardabweichung jedes Merkmals. Es sollte beachtet werden, dass der Mittelwert und die Standardabweichung nur anhand der Trainings-Daten berechnet werden sollte, damit die Modelle keinen Zugriff auf die Werte in den Validierungs- und Testsätzen haben. Es ist wichtig, Features zu skalieren, bevor ein neuronales Netzwerk trainiert wird. Normalisierung ist eine gängige Methode für diese Skalierung. 3.5.5 RNN Was macht ein Reccurrent Neural Network (RNN)? Ein Beispiel für ein RNN wäre ein Long Short Term Memory Modell (LSTM Modell). Dabei nimmt das RNN zunächst ein ganz kleines Vergangenheits-Intervall, macht eine Modell-Estimation und dann - im nächsten Schritt - wird ein grösseres Vergangenheits-Intervall verwendet (inkl. predicted Y-Variable aus dem vorherigen Modell), um eine neue Modell-Estimation zu machen etc. Beispiel eines RNN: hier ein LSTM 3.5.6 Data-Pipelines Data Pipelines use an input to produce an output and then - on a second step - use the produced output as an input to produce another output etc… Visualisierung: Du kannst dir unter Data Pipelines nichts anderes als eine “Guetzli Fabrik” vorstellen, welche diverse Produktionsmaschinen verwendet - zum Beispiel einen Teig-Cutter, dann einen Butter-Schmierer, sowie einen fetten Ofen und einen Sortierer von “guten VS. schlechte Guetzli” - um die Inputs immer weiter zu verarbeiten, sodass schlussendlich ein Endprodukt (= die fertigen “Guetzli”) ensteht, dass man verkaufen kann. How to create good Data-Pipelines in Scikit-Learn? Ziel von Data-Pipeline: Write more clean // readable code, especially when you do data cleaning. A datapipeline is basically a way of standardizing your code. Warum sind Data-Pipelines so geil?: Because you can compare many different regression-models (Linear-Regression Vs. Logistic-Regression Vs. RandomForrest …), applying different “scaling-techniques” (= normalize a variable with mean 0 and standard-deviation of 1), as well as using “data cleaning techniques” (= reduce dimensions via PCA, reduce missing-values etc…). Another cool thing to note is, that you can choose the order, in which the cleaning, scaling and fitting occures! See also the summary of the guy on Youtube ab 10:00-11:00. Link to Github for an example: Jupyter-Notebook code-example on how to create a little Data-Pipeline by yourself 3.6 Programming Theory 3.6.1 Data-Types In R oder Python ist es wichtig zu verstehen, dass gewisse Funktionen nur dann funktionieren, wenn die Inputs, die wir in die Funktion eingeben wollen, einen bestimmten Data-Type aufweisen müssen. On the website W3-Schools, I found this extremely good overview of all data-types, which is crucial concept to understand when doing data cleaning. Data-Types are a key-thing to understand. Otherwise, you won’t be able to apply some algorithms on your dataset! 3.6.2 Global Variables VS. Local Variables Variables that are created outside of a function are known as global variables. Global variables can be used by everyone, both inside of functions and outside. Example of a global variable: x = &quot;awesome&quot; def myfunc(): print(&quot;Python is &quot; + x) myfunc() &gt;&gt;&gt; Python is awesome In contrast, if you create a variable with the same name inside a function, this variable will be local, and can only be used inside the function. The global variable with the same name will remain as it was, global and with the original value. Example of a local variable: x = &quot;awesome&quot; def myfunc(): x = &quot;fantastic&quot; print(&quot;Python is &quot; + x) myfunc() print(&quot;Python is &quot; + x) Output of this: click here 3.6.3 Array What is an array? An Array is a List of Data. In a DataFrame-Object, you can think of a column or a row to be arrays. It is a data structure, which contains “n” objects within a list. - Quelle: The Coding Train 3:10-3:22 3.7 Statistics-Theory 3.7.1 P-Hacking Was ist p-hacking? In der Statistik gibt es den p-Wert ein: Man nimmt an die Hypothese sei wahr und berechnet dann die Wahrscheinlichkeit, dass die beobachtete Statistik mindestens so extrem ausfallen würde (für die Gegner von Wischi-Waschi hier die Wikipedia-Definition). Falls diese Wahrscheinlichkeit unter 5% liegt, dann sei das Resultat “statistisch signifikant” (yay!) und die Nullhypothese kann verworfen werden, was oftmals die Absicht ist. Das Problem ist nur: Hypothesen gibt es viele und z.T. auch recht ähnliche. Wenn man genug Hypothesen aufstellt - vor allem, nachdem man sich die Daten angeschaut hat - dann ist es durchaus möglich, dass man ein statistisch signifikantes Resultat erhält, unabhängig davon, ob das Resultat tatsächlich auch stimmt. Das nennt man p-Hacking. Es kommt häufig in der Forschung vor, aber es kommt sicher auch in der SBB vor (dennoch hier eine +1 für Hypothesen-basiertes arbeiten!). Wie einfach man in die “falsche Signifikanz Falle” tappen kann, wird hübsch in dieser Gallerie falscher Korrelationen illustriert. 3.8 Appendix for the Future Welche Zeitperiode sind am geeignetsten für Zeitreihenanalysen mit Machine Learning? "],["python-libraries.html", "Chapter 4 Python Libraries 4.1 List of Useful Python-Libraries 4.2 Basic Python 4.3 Pandas 4.4 Statsmodels 4.5 String-Manipulation 4.6 Working with Date-Columns 4.7 Time-Series Forecasting", " Chapter 4 Python Libraries Dieses Juypiter Notebook ZF. 4.1 List of Useful Python-Libraries numpy, for mathematical operations. pandas, for handling data, for example: Drop missing values. Feature Engineering. statsmodels, for regression analysis in “econometrics-style”. Run a linear model (OLS). Run a binary-regression (logit or probit). Run a choice-model. sktime, for time-series analysis inkl. machine-learning. sklearn, for machine-learning in “computer-science style”. matplotlib, for data-visualization. seaborn, for data-visualization. datetime, for dealing with dates. 4.2 Basic Python In this part, I will give you some useful Python-Code, which can be helpful in any project. 4.2.1 Create a DataFrame The following DataFrame was build with many Series-Objects: df = pd.DataFrame({&#39;LoadCH&#39;: ts_loadFc[time_index_hourly], &#39;LoadD&#39;: ts_DE_loadFc.resample(&#39;H&#39;).mean()[time_index_hourly], &#39;LoadF&#39;: ts_FR_loadFc[time_index_hourly], &#39;LoadIT&#39;: ts_IT_loadFc[time_index_hourly], &#39;GenCH&#39;: ts_genFc[time_index_hourly], &#39;GenD&#39;: ts_DE_genFc[time_index_hourly], &#39;GenF&#39;: ts_FR_genFc[time_index_hourly], &#39;GenIT&#39;: ts_IT_genFc[time_index_hourly], &#39;RenGenCH&#39;: ts_RenGenAllCH[time_index_hourly], # we take the aggregated renewable generation &#39;RenGenD&#39;: ts_RenGenAllDE[time_index_hourly], &#39;RenGenF&#39;: ts_RenGenAllFR[time_index_hourly], &#39;RenGenIT&#39;: ts_RenGenAllIT[time_index_hourly], &#39;TransFromDach&#39;: ts_TransFromDACH[time_index_hourly], &#39;TransToDach&#39;: ts_TransToDACH[time_index_hourly], &#39;TransToIT&#39;: ts_TransToIT[time_index_hourly], &#39;SeasonAndProduct&#39;: ts_season_prod[time_index_hourly], # TO-DO: re-name this column, since its name is confusing! &#39;tInfoDaySin&#39; : ts_timeInfoDayI_4forecast[time_index_hourly], &#39;tInfoDayCos&#39; : ts_timeInfoDayII_4forecast[time_index_hourly], &#39;tInfoYearSin&#39; : ts_timeInfoYearI_4forecast[time_index_hourly], &#39;tInfoYearCos&#39; : ts_timeInfoYearII_4forecast[time_index_hourly], &#39;PricesCH&#39;: ts_DA_Price[time_index_hourly]}) 4.2.2 Apply a function on a Column This can be useful, when you need to do some transformation to a column of a df: # Step 1: Define any function, that you will need to be applied, in order to transform def my_function(): ... # hier kommen die verschiedenen Anweisungen ... # hier kommen die verschiedenen Anweisungen return some_variable # Step 2: Apply this newly specified function on your column of your data-frame, that you wish to transform df[&#39;new_column&#39;] = df[&#39;column_1&#39;].apply(my_function) 4.2.3 Save a DataFrame df.to_csv(&#39;Your_FileName.csv&#39;, index = True) # this will be saved into your &#39;wd&#39; 4.2.4 Put a time-stamp on each Python-Cell The ipython-autotime library is particularly useful to get an overview of how long each Python-Cell takes to be executed. You can even use it for debugging in combination with the datetime-library in order to use print()-statements that gives you the run-time of - for example - each model-fiting iteration (in a loop). 4.2.4.1 Installation To install this wonderful option pip install ipython-autotime 4.2.4.2 Activation It uses a Python Magic-Command (= note the %) in order to be able to use this library: %load_ext autotime 4.3 Pandas 4.3.1 How to display all the columns? pd.set_option('display.max_columns', None) 4.3.2 Drop Missing-values? df = df.dropna() # drop the missing values 4.3.3 loc- VS. iloc-Selection loc &amp; iloc are the “accessor operators” in the pandas-library. With these,, you can select rows and columns in your dataframe: For loc: Based on the name of the rows (= row-label // row-index) and columns. For iloc: Based on the position // where the specific rows &amp; columns are within the dataset. 4.4 Statsmodels 4.4.1 Useful Nice-to-Knows Use statsmodels in combination with the patsy-library. For instance, if we have: some variable y, and we want to regress it against some other variables x, a, b, and the interaction of a and b, then we simply write: patsy.dmatrices(&quot;y ~ x + a + b + a:b&quot;, data) 4.4.2 Define the y-variable &amp; X-variables y, X = dmatrices(&#39;Lottery ~ Literacy + Wealth + Region&#39;, data=df, return_type=&#39;dataframe&#39;) Notice that dmatrices-function above does the following things: It creates Dummies: Split the categorical-variable Region into a set of indicator-variables (= Dummy-Variables). Added a constant (= Intercept-Column) to the exogenous regressors matrix. Returned pandas-DataFrames instead of simple numpy-arrays. 4.4.3 Estimate a Model model = sm.OLS(y, X) # Step 1) Choose the model type --&gt; here, we choose the model to be of the class &quot;OLS&quot; (= instantiate the model: which type of model do you want, for example OLS, GLM, RLM etc...?) results = model.fit() # Step 2: Fit the model print(results.summary()) # Step 3: Summarize model 4.4.3.1 Extract the \\(\\hat\\beta\\)-Parameters results.params 4.5 String-Manipulation Handling Data that are in string-Format is important, since knowledge &amp; human language is mostly based on “writing”, and therefore comes in as string-formatted data. 4.5.1 Split a String at a specific point timestamps = time_series_str_column.str.split(&quot;+&quot;, expand=True) # &quot;+&quot; is the Buchstabe, at which we will split the string. # &quot;expand = True&quot; says that we want to keep both parts of the string that was being split. 4.5.2 Cut a String in half Here, we define 2 functions, which will cut the string in half: def splitstring_p1(value): string1, string2 = value[:len(value) // 2], value[len(value) // 2:] return string1 def splitstring_p2(value): string1, string2 = value[:len(value) // 2], value[len(value) // 2:] return string2 4.6 Working with Date-Columns Since you will - most probably - often work with time-series data, you MUST learn how to handle this data-type well! For any project with time-series data, the following questions about your Date-Time-Column need to be answered: - What is the current &#39;dtype&#39; of your &#39;Date-Time&#39;-Column? - If you work with multiple time-series: are all time-series already in the UTC-format? If NOT, then you better do it, otherwise you have the wrong hours across different time-zones! 4.6.1 Define a Range of Dates # Step 1: We need a starting &amp; endpoint for the range of dates the_start_date = &#39;2019-01-01&#39; the_end_date = str(datetime.today().date() + timedelta(days = 2)) # this is simply &quot;today&quot; + 2 days into the future # Step 2: Let&#39;s create the range of dates based on the starting- &amp; end-point date_range_of_interest = pd.date_range(the_start_date, # starting date-point (= is fixed) the_end_date, # end date-point (= can change dynamically // updates with new data from entsoe) freq = &#39;h&#39;, # we want to have &#39;hours&#39; closed = &#39;left&#39;, # The input &#39;closed&#39; let&#39;s you choose whether you want to include the start and end dates. # --&gt; &#39;left&#39; will exclude the end-date, which is good, since this date is in the &quot;future&quot; (= today + 2)! tz = &#39;Europe/Berlin&#39;) # this is the time-zone 4.6.1.1 Date-Range in 15-Minutes Abstand date_range_15Min = pd.date_range(the_start_date, the_end_date, freq = &#39;15Min&#39;, # only thing that changes --&gt; will be used for Germany, since DE has quarterly-houred observations closed = &#39;left&#39;, tz = &#39;Europe/Berlin&#39;) 4.6.2 Convert a string-Date Column INTO a date-time-format We can use pandas for this: df[&#39;Date-Time&#39;] = pd.to_datetime(df[&#39;Date-Time&#39;], format=&#39;%Y-%m-%d %H:%M:%S&#39;) df[&#39;Date-Time&#39;] # check, if it worked? 4.6.3 Umgekehrt: Convert a date-time-Column, into a str-format df[&#39;Date-Time&#39;] = df[&#39;Date-Time&#39;].apply(lambda x: x.strftime(&#39;%Y-%m-%d %H:%M:%S%z&#39;)) Why would we do this? Sometimes, there will be the problem, that we want to merge different time-series together, but they have different Date-Time-Formats to begin with! Hence, BEFORE we apply the UTC-universal Date-Time-Format, we will need to bring every separate data-frame into the same Date-Time-Format. Since Python is really good for handling transformations on the str-format, we can use this to our advantages and: Bring the Date-Time-String into the way we want it to be on all our separate Date-Time-Columns across those different datasets. And then, we will be able to convert them back into the UTC-Format! =) In order to see, how to manipulate strings, I strongly recommend you to look at the sub-chapter “String-Manipulation”. 4.6.4 For UTC-Conversions: Keep a copy of the “Local”-Time Here, we create a new column Local-Time, in order to have both, the “local” &amp; the “UTC” time next to each other: df[&#39;Local-Time&#39;] = df[&#39;Date-Time&#39;].apply(lambda x: x.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)) 4.6.5 Create a unique ID based on a Date-Column df = df.reset_index().rename({&#39;index&#39;: &#39;Date-Time-ID&#39;}, axis=&#39;columns&#39;) 4.7 Time-Series Forecasting Here, I will explore different libraries that will allow you to make forecasts into the future. 4.7.1 Facebook Prophet Aus einer didaktischen Perspektive ist die offizielle Dokumentation der Prophet-Library wunderbar, um die Unterschiede zwischen R &amp; Python (Objekt-Orientierte Programmierung) zu entdecken. Link zur offiziellen Dokumentation: https://facebook.github.io/prophet/docs/quick_start.html 4.7.2 SkLearn In some functions, you will need to set the option random_state. If you want reproducible results, how do you need to handle random_state? Just set random_state to an integer. Dieser integer kann beliebig gewählt werden, das ist SkLearn egal! 4.7.3 SkTime 4.7.3.1 SkTime API Überischt zum API in sktime? Grundsätzlich verwendet sktime fit-, predict-, und transform-class-methods. Um zu verstehen, was dies genau bedeutet, hier eine globalere Übersicht mit Hilfe einer Visualisierung veranschaulicht: API of sktime For estimator classes (a.k.a. model classes), sktime provides: a fit-method (= goal: training // estimating the model) and a predict-method (= goal: generate new predictions). For transformer classes, sktime provides: various fit-methods, various transform-methods to transform data that comes in series. 4.7.3.2 Reduction What does it mean: “Reduction is composable”? Ein Synonym wäre: “addieren” → E.g. you can split a difficult task into a bunch of smaller tasks (= reduction) and “add” them together to solve the bigger task at the end (= “reduction is composable”). 4.7.3.3 Forecasting-Horizon Welche Arten von Forecasting-Horizon gibt es? Es gibt einen relativen forecasting-horizon (FH), dh “relativ” zur Trainings-Periode ist hier gemeint! Es gibt auch einen absoluten FH. Hier werden die absolute time-points in the future verwendet für die Prediction. Quelle: Im “examples”-Repository auf Github zur Sktime-Library &gt; 01_forecasting.ipynd 4.7.3.4 Train-Test Split What do you need for parameters // inputs to write a train-test split function for time series data? You need 3 arguments // inputs for such a function in time series: the data the window length, e.g. how many periods the training dataset should have. the length of the forecasting horizon, e.g. how many periods into the future our validation-period go. Quelle: Youtube-Video ab 46:25 4.7.3.5 Konkurrenz-Libraries für Time-Series What are other useful libraries when working with Time Series in Python? Besides Sktime, there are 5 other libraries in Python which you can use for different purposes: Stop this Video at 5:35 um zu sehen, für welche Zwecke sie sich am besten eignen. 4.7.3.6 Example on Github In the example Repository on Github, was ist die Einheit der y-variable in 01_forecasting.ipynd? Einheit der y-Variable: in Monate "],["visualization.html", "Chapter 5 Visualization 5.1 Graphs with Python", " Chapter 5 Visualization When you analyze data, it is crucial to communicate with simplicity what evidence you found. Therefore, this chapter will focus on how to bring your evidence to your audience. 5.1 Graphs with Python When you work with Python, there are three main libraries you need to know: seaborn, which can plot extremely beautiful graphs, without much efforts. matplotlib, for more advanced stuff, this is the library you need to master. plotly, for implementations into websites. Noob. 5.1.1 Seaborn 5.1.1.1 Draw a Time-Series (= Line-Chart) A line-chart is often used to visualize a trend in the data over time. import seabron as sns sns.lineplot(data = spotify_data) 5.1.1.2 Draw a Barplot (= Barchart) import seabron as sns sns.barplot(x = data.Racing, y = data.index) 5.1.1.3 Draw a Heatmap import seabron as sns sns.heatmap(data = flight_data, annot = True) 5.1.1.4 Draw a Scatterplot (Streudiagramm // Punktwolke) You can check, whether you have enough variation in your data to predict the dependent variable. 5.1.1.4.1 With 2 variables (incl. regression-line) import seabron as sns sns.scatterplot(x = health_data[&#39;bmi&#39;], y = health_data[&#39;life_expectancy&#39;]) # include a regression-line: sns.regplot(x = health_data[&#39;bmi&#39;], y = health_data[&#39;life_expectancy&#39;]) 5.1.1.4.2 With 3 variables, where 2 are continuous &amp; 1 is categorical import seabron as sns sns.scatterplot(x = health_data[&#39;bmi&#39;], y = health_data[&#39;life_expectancy&#39;], hue = health_data[&#39;smoker&#39;]) # include a regression-line: sns.lmplot(x = health_data[&#39;bmi&#39;], y = health_data[&#39;life_expectancy&#39;], hue = health_data[&#39;smoker&#39;]) # note that it is not the same, as when you used 2 variables! 5.1.1.5 Draw a Histogram A histogram sorts the data, then it is putting the data into intervalls of data (= batches) and finally counts the frequency those batches occur. Useful to check the distribution of the data 5.1.1.5.1 Discrete Version import seabron as sns sns.distplot( a = iris_data[&#39;Petal Length (cm)&#39;], # a column you would like to plot label = &quot;Rose&quot;, kde = False # &quot;Kernel Density Estimate&quot; (kde): always provide this input when creating a histogram ) 5.1.1.5.2 Continuous Version (Kernel Density Estimate) import seabron as sns ### ---- Standard KDE-Plot sns.kdeplot( data = iris_data[&#39;Petal Length (cm)&#39;], shade = True # colors the area below the curve ) ### ---- 2-Dimensional KDE-Plot, when using more than 1 column sns.jointplot( x = iris_data[&#39;Petal Length (cm)&#39;, y = iris_data[&#39;Septal Width (cm)&#39;], kind = &quot;kde&quot; ) 5.1.1.6 Change Style of different Seaborn-Plots Key:Set the new style BEFORE you run your code for the actual graph! import seabron as sns ### ---- Example for changing the colors // styles with a new style &quot;dark&quot;: sns.set_style(&quot;dark&quot;) # Possibile styles to choose from: {&quot;darkgrid&quot;, &quot;whitegrid&quot;, &quot;dark&quot;, &quot;white&quot;, &quot;ticks&quot;} 5.1.2 Matplotlib 5.1.2.1 Set Width &amp; Height of Graph import matplotlib.pyplot as plt plt.figure(figsize=(10,6)) # 1. width, 2. height 5.1.2.2 Set title of Graph import matplotlib.pyplot as plt plt.title(&#39;Hello World&#39;) 5.1.2.3 Set Label of Axes import matplotlib.pyplot as plt plt.ylabel(&#39;Average Gaming Score&#39;) plt.xlabel(&#39;Date&#39;) 5.1.2.4 Force Legend to appear import matplotlib.pyplot as plt plt.legend() "],["improve-yourself-constantly.html", "Chapter 6 Improve Yourself Constantly 6.1 To Share", " Chapter 6 Improve Yourself Constantly Like everything in Software and Informatics: you constantly need to acquire new skills to get the job done. Here, I listed different examples of what you will need to master, in order to improve yourself as a data-scientist: Git Cheat-Sheet? PDF zum Git Cheat-Sheet: klicke hier Twitter crawler via R? Link zur Webseite Monte-Carlo Simulation explained &amp; how to implement in Python? Link zur Webseite How to create a „reproducable example“ in order to post it on Stackoverflow? Link zur Webseite 6.1 To Share Create a Website (Hugo): Link zur Webseite "],["git-versioncontrol.html", "Chapter 7 Git Versioncontrol 7.1 Useful Git-Commands", " Chapter 7 Git Versioncontrol If you work on projects, especially when collaborating in teams, it is very important to use a versioncontrol system, such as - for example - Git. 7.1 Useful Git-Commands 7.1.1 Activate the automatic Password: from local Computer to Github If you use Github, you will absolutely need an shh-key, where Github gets the public-key and your computer has the private-key to establish an automatic connection when you work on your local computer and you want to push some changes to the Github-Cloud. In order to activate the connection between your local computer and the Github-Cloud, you will need to type in the following command into your terminal: git remote set-url origin git@github.com:Your_Github_UserName_here/Name_of_Your_Remote_Repository_on_Github.git 7.1.2 Create a .gitignore-file You will NEVER want to use version-control on every single file in your repository. That’s why you need a .gitignore-file, in which you can write - on each line - which paths can be ignored. Be sure to be in the root of your working directory when you create the .gitignore-file! touch .gitignore Example, of what to write into a .gitignore-file? It can be a path to a repository that you don’t want to stage: /bilder It can be all log-files: **.log 7.1.3 Speicherplatz verwendet durch Git-History Um den Status zu erhalten, brauchen wir bloss, den folgenden Code einzugeben: git count-objects -v. Der Output wird sein: git count-objects -v count: 7 size: 32 in-pack: 17 packs: 1 size-pack: 4868 prune-packable: 0 garbage: 0 size-garbage: 0 Der size-pack Eintrag gibt die Größe Ihrer Packdateien in Kilobyte an. Somit verwenden Sie fast 5 MB an Speicherplatz. Das ist oftmals zu gross, beispielsweise Bild-Dateien sind hier enthalten, die man nicht - wie zum Beispiel HTML-Files - ständig bearbeitet. Deshalb solltest du diese nicht alles in deinem Repository stagen! 7.1.4 Unstage everything Dieser Befehl enthält zwar den gefährlichen Befehel rm, doch hier werden keine Files deleted, sondern bloss die komplette git-history gelöscht: git rm --cached -r . Warum will man die gesamte Git-History löschen? Weil man muss nicht alles via Version-Control verwalten. Ein gutes Beispiel sind Bild-Dateien, welche man nicht - im Gegensatz zu HTML- oder CSS-Files - häufig bearbeitet. Ein zweiter Nachteil, wenn alles via Version-Control läuft, ist, dass es viel zu viel Speicher benötigt, obwohl Git Bilder zwar komprimiert, aber diese bleiben trotzdem grosse Dateien… 7.1.5 Stage only a certain type of file - for example - only .html-files git add *.html "],["econometrics.html", "Chapter 8 Econometrics 8.1 Synonyme: 8.2 English Words Synonyme 8.3 Allgemeines 8.4 Statistics Formulas 8.5 Definitionen 8.6 Different Tests // Vorgehen bei den Tests 8.7 Diverse Berechnungen 8.8 Accept // Reject Null-Hypothesis 8.9 Formulierungen 8.10 Coefficient Interpretation 8.11 Nice to Know 8.12 Allgemeine Regeln 8.13 Ommitted Variable Bias = OVB 8.14 Randomization 8.15 Dummy Variables 8.16 Implication of statistically significant coefficients 8.17 Linear probability model VS. Probit model: comparison 8.18 Heterogeneous treatment effects // interaction terms 8.19 Definitions of “bad controls” 8.20 Fixed effects // FE 8.21 Cross-sectional Data 8.22 Panel Data 8.23 Difference-in-Differences 8.24 Instrumental variables // IV 8.25 Regression discontinuity design // RDD 8.26 Coded Algorithms &amp; R-Functions 8.27 Nützliche Funktionen R: 8.28 Useful Econometrics documents:", " Chapter 8 Econometrics 8.1 Synonyme: Dependent variable // y-variable // regressand // Y-intercept // outcome variable // response variable // label // ground truth X-variable // variable of interest // explanatory variable // regressor // covariates // independent variable // predictor // attribute regression of y on x // regress y on x slope coefficient // beta Controlling for // conditional on // holding fixed // holding constant // “certeris paribus” // statistically account Classical linear regression model // CLRM Linear probability model // LPM Probit model // probit regressions Logit model // logit regressions (Population) data generating process // Population regression function // PRF // „true“ regression (—&gt; very often unknown) Sample regression function // SRF // fitted model // estimated model Range // distribution // histogram Dummy variable // categorical variable Endogeneity // Omitted variable Bias // OVB // violation of assumption 2 (= the non-zero mean assumption) R squared // coefficient of determination RSS // SSR // residual sum of squares // sum of squared residuals Fitted value // predicted value // Y(hat) Validity // internal Validity // external Validity Randomized experiment // Random Assignment study // social experiment // randomized control trial // randomized trial Expected value // predicted value Heterogeneous treatment effects // interaction terms Fixed Effects // FE Difference-in-Differences // Diff-in-Diff // DiD Two stage least squares // 2SLS // Second Stage // IV-Regression NA // Censored Data // Latent Variable —&gt; can be estimated with a Tobit Model or Heckman Two stage Model (where the dependent variable is censored // has NAs, for example wages of people who don’t work cannot be observed // are censored) Observation // Row in R // samples // records (in computer science) Column in R // field (in computer science) Variable // Column in R Finite // Definite Infinite // indefinite Relativ zu // im Verhältnis zu 8.2 English Words Synonyme Among = across Fixed = not random —&gt; an example would be the true parameters of the population would be fixed // not random, but you don’t know them —&gt; you can only approximate the true parameters by taking a random sample and estimating it’s coefficient to approach the true / fixed coefficient with a random (but estimated) coefficient, which resulted from a sample that the researcher took. With respect to … = in Bezug auf… = Auf der (z.B. klanglichen) Ebene… 8.3 Allgemeines Numerator = Zähler Denominator = Nenner Ditto = ebenfalls A priori = When dealing with ommitted variable bias, you need to ask yourself the question: what are the most likely sources of important omitted variable bias in this regression? Answering such a question requires applying economic theory &amp; expert knowledge, and should occur before you actually runy any regressions; because this step is done before analyzing the data, it is referred to as „a priori“ („before the fact“) reasoning. Non-Linearity in X examples: A regression is not linear in one regressor „x“, if - for example - the „x“ was logarithmic or is an interaction term. Non-linearity in „beta“ // parameters // coefficients: An example of a regression function being non-linear in the parameters // coefficients are: 1) logistic regressions, where the dependent variable is between 0 and 1 (you use an s-shape function that can match all x-values between -infinity to +infinity to a y-value between 0 and 1); or 2) negative exponential growth: Importantly, non-linearity in „beta“ cannot be estimated using OLS, but rather, with an extension of OLS called nonlinear least squareds. Independence assumption when dealing with probabilities &amp; variances: Independence has implications on how you calculate in statistics. For example, it affects the calculation of joint probabilities or variances with 2 random variables: 8.4 Statistics Formulas Formula for covariance: Formula for (population) variance: 8.5 Definitionen Beliefs: VL1, Yanazingawa, S.2 Evidence: VL1, Yanazingawa, S.2 Counterfactual: VL 1, Yanazingawa, S.5 Validity // internal Validity // external Validity: VL 1, Yanazingawa, S.7, S.12 Randomized experiment // Random Assignment study // social experiment // randomized control trial // randomized trial: VL1, Yanazingawa, S.7 Control Group: it mimics the counterfactual: VL1, Yanazingawa, S.7 Treatment Group: VL1, Yanazingawa, S.7 Random Assignment (= part of randomization): VL1, Yanazingawa, S.7 Random Sampling (= part of randomization): VL1, Yanazingawa, S.7 Why is randomization important?: VL1, Yanazingawa, S.7-8 Observational Study: VL1, Yanazingawa, S.10 (wichtig für MA, weil es genau in mein Kontext fällt!) Bivariate Regression: Vl2, Yanazingawa, S.2 Population Regression Funtion // Sample Regression Function: VL2, Yanazingawa, S.3 Dependent variable // y-variable // Regressand // Y-intercept: VL2, Y-intercept, S.3 Independent variable // x-variable // Regressor // Covariate // Explanatory Variable(s): VL2, Y-intercept, S.3 Slope Coefficient // Beta: VL2, Y-intercept, S.3 Interpretation of Slope Coefficient Bivariate Regression: VL2, Yanazingawa, S.4 Interpretation of Slope Coefficient Multivariate(!) Regression: VL2, Yanazingawa, S.10 Interpretation of Intercept Bivariate Regression: VL2, Yanazingawa, S.4 Interpretation of Intercept Multivariate(!) Regression: VL2, Yanazingawa, S.10 Expected Value // Predicted Value: VL2, Yanazingawa, S.4 Interpretation p-value (= it’s a bedingte(!!) W’keit): VL2, Yanazingawa, S.5 What does it mean to have a small Standard-Error?: VL2, Yanazingawa, S.6 Statistical Significance &amp; its Implication: VL2, Yanazingawa, S.7 Understanding “Holding Constant”: VL2, Yanazingawa, S.12 Positive // Negative Bias Table: VL3, Yanazingawa, S.5 Overstated // Understated Concept (Prof does not like the concept…): VL3, Yanazingawa, S.5 Dummy Variable // Categorical Variable: VL4, Yanazingawa, S.1 Dummy Variable Trap: VL4, Yanazingawa, S.3 &amp; S.6 Multicolinearity: VL4, Yanazingawa, S.3 Range // Distribution // Histogram: Ü1, Yanazingawa Scatterplot: Ü1, Yanazingawa Linear probability Model // LPM: VL5, Yanazingawa, S.2 Interpretation of Slope Coefficient in Linear Probability Model // Regression: VL4, Yanazingawa, S.2 Interpretation of Intercept Linear Probability Model // Regression: VL4, Yanazingawa, S.2 Advantages // Disadvantages of LPM: VL5, Yanazingawa, S.4 Advantages // Disadvantages of Logit Model: VL5, Yanazingawa, S.5 Probit Model // Probit Regressions: VL5, Yanazingawa, S.5 Logit Model // Logit Regressions: VL5, Yanazingawa, S.13 Implement a Probit Model with Code: Ü2, Yanazingawa, Question 6 Heterogeneous Treatment Effects // Interaction Terms: VL6, Yanazingawa, S.1, S.4 (first bullet) Coefficient Interpretation of Continuous-Dummy Interaction: VL6, Yanazingawa, S.4-7 Coefficient Interpretation of Dummy-Dummy Interaction: Vl6, Yanazingawa, S.8 Merge different Datasets together: Ü3, Yanazingawa, 5) c) Bad Controls: Ü3, Yanazingawa, 5) d), siehe Korrektur auf meinem gedruckten Problemset! Fixed Effects // FE: VL7, Yanazingawa, S.3 &amp; Zusammenfassung (ZF) auf letzter Seite des Handouts Difference-in-Differences // Diff-in-Diff // DiD: VL8, Yanazingawa, S.1 Parallel Trend Assumption: VL8, Yanazingawa, S.2 Interaction Terms between two FEs: Ü4, Yanazingawa, Aufgabe 3, Part I = a „country x time FE“ interaction controls for country-specific time-trends. However, to use it, you need 2 observations for each country and year (if we assume the time FE to be years…) Make Dummy Variables while having multiple Conditions: for example, the city needs to be between 25 km and 75 km away from the border = Ü4, Yanazingawa, Aufgabe 1, Part II Clustering the Standard Errors: Ü4, Aufgabe 1, Part II Endogeneous Regressor: VL9, Yanazingawa, S.1 Validity of an Instrument: VL9, Yanazingawa, S.1 Exclusion Restriction: VL9, Yanazingawa, S.2 Two Stage Least Squares // 2SLS // Second Stage: VL9, Yanazingawa, S.6 &amp; S.7 First Stage: VL9, Yanazingawa, S.6 Reduced Form: VL9, Yanazingawa, S.6 IV-Regression in R: Ü5, Yanazingawa, Aufgabe 1 a) Calculation of Mean while ignoring NAs: Ü5, Aufgabe 2 a) Standardize a Variable: Ü5, Audgabe 2 a) Interpretation with standardized Variables: Ü5, Audgabe 3 ATE, ATT, ATUT: Ü2, Biroli, Aufgabe 2 Selection Problem: Ü2, Biroli, Aufgabe 3 Histrogram: zeigt die an, wie häufig ein Wert auftaucht. Scatter-Plot: Plot, welcher die Korrelation zwischen zwei Variablen in einem Datensatz aufzeigen. Dichtefunktion: W’keit, einen Wert zwischen “Wart a” und “Wert b” zu erhalten (im Kontext von Zufallsvariablen) = verwendet man bei stetigen Massen, wie Gewicht oder Distanz etc… Cross-Sectional Data: Observation of an economic agent (for example individuals, firms, households etc.) collected at one point in time; you can have agent FEs, but not time FEs! Panel Data: Multiple observations on agents over time (—&gt; you have a time index, as well as an index for individuals!) —&gt; here you can add time FEs and also agent FEs Confounders // Mediator Variables: These are the unobservable factors in your regression // in the error term that correlate with your x-variable of interest, as well as your y-vaiable, thus inducing bias when you estimate your model Note: The term „Mediator“ is used for independent variables that cause a change in the y-variable. Note 2: The term „Moderator“ are used for interaction terms, e.g. that the effect of an x-variable is modified and depends on the second variable. Bivariate Regression: a regression with only one regressor // x-variable Multivariate Regression: a regression with multiple x-variables. 3 Common Reasons for Endogeneity: Simultaneous Causality: x causes a change in y but y also causes a change in x Correlated unobservable Variable: leads to OVB in your coefficient of interest Measurement Error in x: you will have attenuation bias, e.g. bias towards zero Note: There is also the possibility that you have measurement error in y. In this case, we do not have a bias in the coefficient of interest! —&gt; however, the standard error of the coefficient gets bigger if you have this kind of measurement error! Degrees of Freedom (d.o.f.): If we have „N“ observations and „K“ regressors in a regression, then we have: v = N - K „degrees of freedom“ R-Squared: measures the goodness of fit, e.g. how much of the variance of y can be explained by our model // it is the squared correlation between the population regression function and our predicted SRF. Problem with the measure R-Squared: if you rescale the y-axis in the regression, for example with a log-function, it will lead to a reduction of the variability in Y and increase the R squared without improving the regression. Overfitting the Data: This term is another problem that comes along with the measure „R squared“: the thing is, that you always increase the R squared when you add an additional regressors into your regression. With this characterstic, you could think of people adding regressors with almost no correlation to Y but still increasing the R squared without augmenting the precision of the estimated model! Unbalanced Groups (Kontext: RCTs pr Dummies): If the treatment and control group are - on average - different to begin with, then the groups are unbalanced! —&gt; your estimated treatment effect will likely to be biased BLUE = best (= efficient), linear, unbiased estimator Consistency: if you have an infinite number of observations for your sample, then the estimated coefficient will converge towards the true coefficient of the population with a smaller variance for the estimated coefficient distribution —&gt; we then say that the estimated coefficient is „consistent“ —&gt; Note: the statistical theorem that forms the basis for consistency is called the Law of Large Numbers. Attenuation Bias: This is the Bias, when we have measurement error, meaning that the coefficient of interest „beta“ will be biased towards zero —&gt; e.g. if beta &gt; 0, then beta has a negative bias (it is smaller than what it should be) or if beta &lt; 0, then beta has a positive bias (it is bigger than it should be) —&gt; note: If we have a measurement error in one particular „x“ of a multivariate regression, we will have a higher bias (in absolute value), which is worse than in a simple linear regression case —&gt; the bias towards zero gets bigger! (See the formula on page 90 of „sources of bias, Crawford) Partitioned Regression: you do a bivariate regression of the residuals of 1) a regression of x(i) on all other x(-k)] on 2) the residuals of a regression of y on all other x(-k)] Replication Samples // Monte Carlo Simulation: You (randomly!) draw different samples of size „N“ from the same population many times, for example say you draw them „R“ times. Then, you plot the distribution of the average of each sample —&gt; you will see that, by drawing larger sizes of samples „N“ holding fixed the number of draws „R“ —&gt; the average of the distributed average samples will converge to the true average of the population! Serial Correlation: correlation of a variable with a lagged version of itself —&gt; you need this because of the assumption 4 of the CLRM: residuals should not have a serial correlation with one another Selection Bias: when an individual selects himself into the treatment- / control-group, because he has an advantage of doing so. Compliers (in an Instrumental variable setting): You usually have compliers appearing, when an instrument is not assigning people randomly between treatment- and cotrol-groups. In this case, the compliers would be the people who choose to get treated (D=1), when they actually get assigned to treatment (Z=1) and decide not to get treated (D=0), when the instrument is not activated (Z=0). Never Takers (in an Instrumental variable setting): People who choose to never get treated, independently whether the instrument is switched on or off! —&gt; D = 0 (always!), if Z = 1 bzw. Z = 0 Always Takers (in an Instrumental variable setting): People who choose to always get treated, independently whether the instrument is switched on or off! —&gt; D = 1 (always!), if Z = 1 bzw. Z = 0 Synthetic Control: This method creates an appropriately weighted average of control units which best approximates the evolution of the outcome in the treated unit before treatment. —&gt; the key concept of using the synthetic control is that we construct an artificial control group to get a reasonable estimate for our missing counterfactual (= our treatment, which would not have been treated) RDD with a sharp Design: This is an RDD where the probability of treatment at the cutoff point changes from 0 to 1. RDD with a fuzzy Design: This is an RDD where the probability of getting treated changes (also) discontinuously, but less than 100%. Saturated Model: This is a model with a full set of dummies, e.g. in such a model, there is no constant! Type I-Error in statistical Testing: A type I error happens, when we wrongly conclude that the null hypothesis H(0) is false, when it‘s acutally true (Eselsbrücke —&gt; Jemand sagt die Wahrheit, aber alle meinen, er lügt!) The significance-level „alpha“ is representing the type I error. Ideally, we want to minimizes both errors: type I and type II Type II-Error in statistical Testing: A Type II error happens, when we wrongly conclude that the null hypothesis H(0) is true, when it is actually false (Eselsbrücke —&gt; Jemand erzählt eine Lüge, und alle glauben ihm diese!) Often, the type II error is referred to as „beta“: the probability of concluding that H(0) is true when it‘s actually false. Hence, (1-beta) is the probability of concluding that H(0) is false, when it is actually // truly false (probabilities sum up to 1, that‘s why we can do this!). This (1- beta) probability is also called the statistical power! Ideally, we want to minimizes both errors: type I and type II Statistical Power: read the first sub-point of type error II just above! Grundsätzlich gilt: a small sample size gives us little power to reject the null hypothesis (wahrscheinlich, because you have large standard errors // variance(beta-coefficient) is high), whereas a large sample size gives us more statistical power. —&gt; Usually, we want the power to be larger than 50%! —&gt; a power between 80 to 90%, e.g. the probability to reject H(0) when it is TRULY false. By the reverse probability, this would also imply that - with a statistical power of 80-90% - we would have a 10 to 20% probability to accept the H(0), even though it is false! Merk-Regel: A high statistical power is what you want, otherwise you would not conduct a study! Supplementary Analysis: supplementary analysis seeks to shed light on the credibility of the primary analysis (= this is for example a DiD method, or IV, or RDD) —&gt; an example of a supplementary analysis is placebo testing Objective Function: this is a general function that individuals seek to maximize —&gt; example: a lifetime utility function takes on many values. The individuals seek to choose the maximum value (sometimes the minimum value, if it’s a cost-funtion) out of all those values of this „objective function“. Resampling: If you have a classification problem, where you have a y-variable with 9000 cases of „obese“ and 1000 cases of „normal weight“, then you have a problem that „normal weight“ people are under-represented in your sample. Thus, you have imbalanced data and you need to use resampling techniques with - for example - the „imblearn“ library to have a dataset that is equally distributed, e.g. 1000 cases of „normal weight“ and 1000 cases of „obese“. A staggered Treatment (first time I heard of it was in the context of DiD): This means that - for example - an individual can choose to get treated, but once it gets treated, he cannot get out of the treatment (example: COVID vaccines are like that). 8.6 Different Tests // Vorgehen bei den Tests Check if random Assignment // Randomization into Treatment- &amp; Control-Group was successful: VL1, Yanazingawa, S. Placebo Testing: this test involves demonstrating that your effect does not exist when it really „should not“ exist —&gt; you pick a period where no treatment occured and try to see if your treatment group really did not react to „no treatment“ in this particular period! 8.7 Diverse Berechnungen Review of Hypothesis-Testing // Testing differences in Means: Vl1, Yanazingawa, S.9 &amp; S.11 Hypothesis testing “no effect” (= H0) VS. “there is an effect”: VL2, Yanazingawa, S.5 Bias = short Regression Coeff. - long Regression Coeff. = beta(2)*gamma [= corr(Y, omitted) * corr(X, omitted)] 8.8 Accept // Reject Null-Hypothesis t-statistic: beidseitiger Test: if t-stat (im Betrag) &gt; crit.-value –&gt; reject the null Note: we use Z(1- [alpha/2]) as the critical value, since we have a two-sided test. rechtsseitiger Test: if t-stat &gt; crit.-value –&gt; reject the null Note: use Z(1-alpha) as the crit value, since we have a one-sided test. linksseitiger Test: if t-stat &lt; critical value –&gt; reject the null Note: use Z(1-alpha) as the crit value, since we have a one-sided test. Important general fact about hypothesis: Just because you cannot reject a null-hypothesis does not mean that the null-hypothesis is true. It just means that you don’t have enough empirical evidence to prove that the alternative-hypothesis is true. p-value: Definition: Gives the probability that we would get our „sample-mean“ (= the mean that you just calculated from the data // sample you have drawn) IF the Null-hypothesis was true! Interpretation: This implies - when we have a very small p-value - that - GIVEN our sample - there is very small probability that we would get this sample mean if the nullhypothesis was true. —&gt; this is a good result for a researcher, since he / she seeks to reject the null-hypothesis! beidseitiger Test: if p-value &lt; alpha –&gt; reject the null rechtsseitiger Test: linksseitiger Test: Types of Errors in statistical Testing: 8.9 Formulierungen For “correlation”: “is correlated with”. “is associated with”. For “causation”: “lead to…” “has an effect on..” In Regressions: “hold constant” “certeris paribus” “holding fixed” “controlling for” “conditional on” “statistically account for” “Y is a function of X”: y(x) y = f(x) y depends on x y as a function of x 8.10 Coefficient Interpretation Lin-Lin: Lin-Log: a 1% increase in X will increase // decrase Y by (beta/100) units! Log-Lin: a unit change in X will increase // decrease Y by beta *100 percent! Log-Log: a 1% increase in X will increase // decrease Y by beta % Log-Dummy (dh logarithmierter dummy?): Here you have to be very careful (Fallunterscheidung!): If your coefficient on the dummy is very small, then you can do the normal „Lin-Log“ interpretation, it will be approximately correct. However, if your coefficient on the dummy is big, then you need to transform your coefficient before you do the interpretation! The formula would be: exp^(estimated beta) -1 —&gt; if you multiply this result by 100 then you get the correct %-units and you can procede to do the normal „Lin-Log“ interpretation! Coefficient Interpretation (multivariate Case): \"Beta 1 tells us the average change (, e.g. increase // decrease) in Y associated with a one-unit-change in X(1), holding constant X(2) and all other variables! Coefficient Interpretation (y-variable in Prozent): \"A unitary increase in X is associated with a change in Y of XYZ Percentage Points(!!!) Coefficient Interpretation (x-variable in percent): \"An 1 percentage point(!!) increase (= unitary increase when x-var. is in percent) is associated with a XY unitary change in y. Coefficient Interpretation of a Linear Probability Model: \"Being 1 year older (= x-variable is age [in years]) increases the probability of getting married. Pay attention: the increase is in percentage points (-&gt; y-variable is a dummy!!! [–&gt; dh the slope coefficient is the change in the probability in a LPM! –&gt; dh you need to multiply the number of the coefficient by 100 to get the result in the change in probability (in percentage points of course…)!]) Coefficient Interpretation of a Probit Model: you cannot interpret the magnitude (= Grösse des coeff.) directly, you need to transform it first (with a complicated formula for the s-shape function)! However, the sign AND the statistical significance can be directly interpreted from the coefficient of a probit model! Coefficient Interpretation of a dummy*continuous Interaction: for example the gender-wage gap –&gt; returns to education can vary depending if you are a man or a female. Hence, the coefficient represents the difference in slopes (for education, e.g. the continuous variable) for males and females. Coefficient Interpretation of a Dummy-Dummy Interaction Term (hier: female * black // gender * enthnicity): when you have 2 dummies, you have - in total - 4 different “states of the world” // 4 different possibilities. Hence, you have 2 different possibilities to do the coefficient interpretation, which is either the differences between black and white males // females, or the differences between white male and females // black male and females. –&gt; these two interpretations are equivalent, but often one is more interesting than the other! –&gt; very important: the dummy*dummy-coefficient = Diff-in-Diff estimator!! Coefficient Interpretation while “holding constant” FEs: Within(!!) districts, full RTML reception is associated with a XY unitary change in genocide cases, compared to zero reception. –&gt; Alternative: “holding constant all factors that vary across districts (= FEs)” Coefficient Interpretation with standardized Measurements in X and in Y: If corruption increases by one standard deviation, then child mortality will rise by 0.6259 standard deviations or 62.59% of a standard deviation of child mortality. 8.11 Nice to Know Why do we need statistics in econometrics? -&gt; Because it is the statistical theory that allows you to make statistical inference from your sample to your underlying population! Random VS. Non-Randomness of Mean &amp; Variance // Features: Features of the population are fixed, dh non-random (but unknown!), whereas features of the sample are random (but known!) —&gt; e.g. if you draw another sample from the population, it is very likely that you get different numbers than in your other sample! If you want to see, if an estimator is economically meaningful: This is done by looking at the magnitude of the coefficient. How to calculate the magnitude of a coefficient // whether to know if the effect is big or not? —&gt; formula: magnitude = coeff. of interest / average estimated Y —&gt; make a summary-statistic of your regression to find out this average estimated Y! Law of large numbers: The sample mean converges to the population mean as the sample size „N“ gets large. Bootstrapping: when you pick a random sample out of a population, you probably will get a different number than with another sample (of the exact same size &amp; exact same population). This exact method can be done with „Bootstrapping“ —&gt; the goal is to see, if your analysis gets completely different (—&gt; this would be bad…) or stays approximately the same (this is the outcome you want…). The 5 assumptions in a regression model that must be true (otherwise we get a biased estimator): The PRF is assumed to take a linear form (—&gt; that‘s why you need to sometimes take the logarithm of a non-linear relationship!) Key assumption for causal interpretation of the coefficient: Mean zero Error —&gt; the error term has an expected value of zero —&gt; E(error/X) = 0 implies that cov(error, X) = 0 —&gt; in other words: that there is no correlation between the coefficient of interest and „everthing else“ // all unobservables in a regression Homoskedasticity —&gt; dh the variance of the residuals should always stay the same —&gt; how to see if A3 is satisfied? —&gt; look at a scatterplot of the residuals (on y-axis) plotted against a regressor: you want to see „picasso on drugs“, e.g. a variation in the residuals on the y-axis that stays constant with increasing „x“ A4: No correlation between the lagged residuals —&gt; how to see if A4 is satisfied? —&gt; use the Autocorrelation-function in R to see if the residuals are correlated with each other (this is what you don‘t want!)- A5: Normality —&gt; residuals should be normally distributed —&gt; how to see if A5 is satisfied? —&gt; plot the density function of the residuals, it should look like a normal-distribution! If not, then the assumption is violated! What happens, if all the above 5 assumptions are satisfied? The coefficient of interest will be unbiased (= Unbiasedness) Efficiency —&gt; e.g. the variance of the estimated coefficient is the smallest (compared to non-OLS estimation), which is a good thing, because we will always have an estimated coefficient that will be the closest guess to the true beta-coefficient! Our estimated coefficient will be normally distributed (good for hypothesis testing etc…) What happens if A1-A5 are violated and how to fix it? Assumption 1 (= Linearity): This assumption is most certainly violated (the true PRF is very rarely linear) —&gt; fixing it is not such a big deal —&gt; either you can add non-linearity in x: log-transform your regressor Make a polynomial for x (often used in RDD) OR add interaction terms or add non-linearity in beta: Using nonlinear Least Squares (= these are non-linear models —&gt; look it up in a fat textbook!) Assumption 2 (= non-zero mean): This will lead to OVB // your regressor is endogeneous! —&gt; to fix it, you need to control for the unobservable factor OR to use some fancy methods like Fixed effects, IV, RDD or RCT that allows you to control for the unobserved factor! Assumption 3 (homoskedasticity): if your variance of the error term is heteroskedastic, your assumption 3 is violated —&gt; use a statistical test to see if your variance is homo- or heteroskedastic! —&gt; it‘s not a big deal if this assumption 3 is violated, because the estimated coefficient of interest will still be unbiased! But the standard errors will be wrong though —&gt; to fix it, you can tell your statistical software to account for heteroscedasticity! —&gt; when you account for heteroscedasticity, you will have to use so called „robust standard errors“! Assumption 4 (correlation between error terms is zero): if this assumption gets violated, it‘s also not a big deal, because (like A3) the estimated coefficient of interest will also still be unbiased! But the standard errors will again be wrong though! To fix this, you can tell your statistical software to cluster the standard errors. Assumption 5 (normally distributed errors): very uikely that this assumption is violated, because we have the very powerful „Central Limit theorem“ that backs it up! —&gt; prof crawford did not tell us how to fix it, because it is very rarely violated! Under which of the above assumptions is an estimated coefficient BLUE? —&gt; under the Assumptions 1-4 the estimated beta-coefficient will be BLUE! Why do we take correlations instead of covariances? —&gt; because correlations have no units (only a number between -1 and +1), while covariances can have strange units like „hours-gradepoints“ xD If you include an irrelevant variable in the regression, will there be a bias on your estimated coefficient of interest? —&gt; no, you coefficient stays unbiased, but the coefficient will be inefficient (not the lowest possible variance(beta_hat)) —&gt; if you are not sure whether to include a variable or not: better to include it, rather than ommit it (otherwise OVB in the worst case!) Dertermine the sign of bias (if you have OVB): where beta(2) is the correlation between the ommitted variable and the y-variable! Another formula to dertermine the sign of the bias: bias = short regression coeff. - long regression coeff. “Without loss of generality”? —&gt; it means that a statement // formula is always true! Check if an instrument is „strong“ or „weak“: —&gt; you can do an F-Test: if the instrument has an F-value that exceeds 10, you have approximately a strong instrument. If you only have one instrument, you can use the t-test. If the value exceeds 3.16, then you also have a „strong“ instrument (rule of thumb). When is the usage of the synthetic-control method optimal? —&gt; If you have a single unit that is treated (for example a country) and many other units that are not treated, all of which could be a control, but none of which is a perfect one. 8.12 Allgemeine Regeln “Larger sample sizes yield to smaller standard errors and thus less uncertainty // narrower confidence intervalls // preciser estimates.” “Larger residuals (= Differenz zwischen durchschnittliches Y und tatsächliches Y = noisier data) yield to larger standard errors and thus more uncertainty.” “Regress Y on X(1), X(2)…X(k)” “We regress Y on X(1), X(2), … X(k)” “The impact of X on Y…” 8.13 Ommitted Variable Bias = OVB Condition for ommitted variable bias: corr(Y, omm. var.) &gt; // &lt; 0 AND corr(X, omm. var.) &gt; // &lt; 0 –&gt; sign of the OVB Bias = short Regression Coeff. - long Regression Coeff. = beta(2)*gamma [= corr(Y, omitted) * corr(X, omitted)] Note: if beta(2) = 0 OR if gamma = 0, there is no bias! –&gt; the higher the magnitude of the bias, the less precise our sample regression funtion (SRF) relative to our population regression function (PRF), thus we have lower internal validity (dh likelyhood of estimating the true causal effect is low, since we have an unprecise SRF) 8.14 Randomization If you don’t randomize, there will be a selection problem, e.g. mean untreated outcomes will diﬀer from the mean of the treated outcomes. By randomizing, you will make it impossible for people to self-select in or out of a treatment! Implementation of “randomization” can be achieved by - for example - tossing a (fair) coin. –&gt; this is difficult to implement because not all people want // need a treatment to begin with (= there are often debates where such “randomized experiments” are seen as unethical!) 8.15 Dummy Variables You can also use other numbers than 0 or 1 to define a dummy. However, 0 and 1 is easier to interpret. If you estimate a model without(!!) a constant, then you can include all dummies. However, you cannot add all dummies from a category (for example you cannot add female &amp; male into one regression, when these two are the only gender-dummies), otherwise you will run into a multicolinearity problem. 8.16 Implication of statistically significant coefficients If a coefficient is statistically significant, it means that we have strong evidence for an association between X and Y! (very relevant for my Master-Thesis!) –&gt; thus, if you don’t find a statistically significant coefficient, you don’t have, you will never have enough evidence for a causal effect! 8.17 Linear probability model VS. Probit model: comparison A good rule-of-thumb is that the probit model’s prediction are similar to those of the linear probability model near the sample mean of X but can be very different far from the sample mean! 8.18 Heterogeneous treatment effects // interaction terms The three categories of different heterogeneous treatment effects are: Continuous * Dummy Dummy * Dummy Continuous * Continuous 8.19 Definitions of “bad controls” If the treatment can strongly influence some of your variables you control for, they are “bad controls”! 8.20 Fixed effects // FE Definition: By using fixed effects, one controlls for all the factors that differ across(!) groups, like districts (if you use for example “district fixed effects”) that are time-invariant!Therefore, any differences that vary across districts (and don’t change over time within a particular district) are controlled for. Instead, the regression exploits variation of the X-variable that vary within groups. Note: when you deal with cross-sectional data and use FEs, you controll for all the factors that differ across(!) groups. You cannot include time FE in a cross-section, since you only have observations for a particular point in time (not like panel data, where you have multiple observation for an agent over time, and thus there you can include time FE). Intuition, why you controll for all factors that vary across districts: in a regression, you want a counterfactual that - ideally - only differs from your population of interest by the treatment. FE is a powerful tool to construct a better counterfactual, because it allows you to group the data and thus make comparisons within those different groups! What does FEs not control for? –&gt; However, note that you need to control for all factors which vary within a district if you use FEs! Key requirement to use FEs: have multiple observations within a group! However, be aware that there will ALWAYS be other unobserved factors that could explain your results. The question is how important those other factors are, and whether they are correlated with the variable of interest (such as RTML reception in Rwanda example). Intuition for Time FEs: schlecht erklärt in Handouts, aber ich habe eine gute Visualisierung –&gt; stelle dir zwei benachbarte Regionen vor: ZH und Aargau –&gt; wenn ich jetzt Time-FEs habe in einer Regression, kontrolliere ich für folgendes: Alle Faktoren, die sich über die Zeit verändern ABER beide Regionen GLEICH STARK “hitten” (–&gt; zum Beispiel BIP-Growth Rates der SCHWEIZ oder global economic conditions, welche sich über die Zeit verändern, aber wahrscheinlich diese beiden Nachbar-Regionen gleich stark beeinflussen). Für was hier jedoch NICHT kontrolliert wird, sind zum Beispiel Faktoren wie REGIONALE BIP-Growthrates // REGIONALE crime rates // ein Meteoriteinschlag in Zürich, aber NICHT in Aargau. Deine Aufgabe als Forscher ist nun herauszufinden: welche dieser Faktoren für dein Modell jetzt relevant? –&gt; folglich weisst du auch, was die Faktoren sind, die durch Time FEs nicht kontrolliert werden und somit kannst du - hoffentlich - dafür kontrollieren! Intuition for Region FEs: schlecht erklärt in Handouts, aber ich habe eine gute Visualisierung: wie vorher: zwei Nachbar-Regionen, Zürich und Aargau –&gt; nun hat man region FEs in Regression. Diese kontrollieren für: Alle Faktoren, die sich zwischen Zürich und Aargau unterscheiden ABER über die Zeit sich nicht verändern // ZEITINVARIABEL sind (–&gt; zum Beispiel ein SCHWEIZER Gesetz, das nur auf den Aargau abzielt und welches sich über die (vom Forscher betrachtete // fixierte) Zeitperiode NICHT verändert hat oder die unterschiedliche Geographie der beiden Regionen (wird sich wohl kaum verändern über die betrachtete Zeit) oder meistens auch Bevölkerungszusammensetzung (unterscheiden sich zwischen ZH und AR, aber bleiben meist konstant über die Betrachtungsperiode). Indexing coefficients: when you use FEs, the index for the coefficients are the groups. —&gt; example: when you use time-FEs, the index will be time. Or, if you use region-FE, the index will be regions Usage of causal language: you can use causal language when interpreting the FEs coefficients if you are convinced that the Fixed effects have eliminated the main threats to internal validity! Controlling for Time FEs, how to make a coefficient interpretation? –&gt; when interpreting a coefficient while using Time FEs: On average and controlling for time trends(!!), a unitary increase in X is associated with an YZ-change in Y. 8.21 Cross-sectional Data Definition: Cross-sectional Data is data from one moment in time. Conditions to use FE for cross-sectional data: you can use FEs, if your data: contains groups AND if your groups have at least 2 observations 8.22 Panel Data Definition: Observing outcomes in the same units // of the same individuals // of the same states (etc.) in multiple moments in time (years). –&gt; note: you observe for example the beer-tax over multiple periods of times for many different states, e.g. we can do a cross-section analysis &amp; analysis over time! Fixed Effects with Panel Data: panel data allows us to: You can do Time Fixed effects, which control for all factors that differ over time but are constant across units, for example globalisation or price level –&gt; both vary over time but are the same across states // units You can do state fixed effects, e.g. you control for factors that differ across states but stay the same over time (= e.g. needs to be time invariant factors within the state // not change over time within the state). 8.23 Difference-in-Differences Key assumption that needs to be satisfied with DiD: parallel trend assumption –&gt; this means: in the absence of treatment(!) // policy change, the treatment group would have had the same mean change(!!) in outcomes (X) as the control group Wichtige Bemerkungen: The parallel assumtion cannot be tested, because we NEVER observe the counterfactual! BUT what you can do is to observe multiple data-points over time BEFORE treatment and look if the treatment- and control-group have similar trends –&gt; then you can look if they move in the same direction (this is what you want to have a good DiD). This assumption does not require the treatment group to have the same level(!!) of Y for the treatment and the control group, either before or after, the policy change! If the parallel trend assumption is satisfied, then the DiD technique allows you to control for variables that differ across the groups but are constant over time (within the group –&gt; for example district FEs) and also allows you to control for variables that differ over time but are contstant across the groups (within the determined time period –&gt; e.g. time FEs) Examples: other policies that don’t change over this period but vary across the districts (abgedeckt durch district FEs); global policies, that affect both regions the same (time FEs absorbs that); one example that you would absolutely need to control for in a Diff-in-Diff in the Card &amp; Kruger paper would be „distance to New Jersey\", because - after treatment - people near the border would start to head to New Jersey to gain more money because the policy changed rised it in New Jersey, but not in the neighboring state Pensylvania. Requirements to use Diff-in-Diff: you need to divide the world into 2 time periods: before &amp; after treatment. You need two groups: 1) treatment-group &amp; 2) control-group. Parallel trend assumption needs to hold, otherwise the results are biased. 8.24 Instrumental variables // IV Requirements to use IV: the instrument needs to be valid, which means: corr(Z, coefficient of interest X(1)) &gt; OR &lt; 0 &lt;–&gt; relevance assumption corr(Z, u) = 0 &lt;–&gt; exogeneity assumption // exclusion restriction (= Synonyme) Wichtige Bemerkung hier: it is NOT possible to test whether an instrument is valid. To be more precise: only the relevance assumption can be tested, but you cannot(!!) test the exogeneity assumption // exclusion restriction. —&gt; You can try and convince your readers and argue in favor of the exogeneity assumption by testing if the differences between two groups are not statistically significant from each other (and that the instrument is indeed a random process which is independent from every variable) Key (assumption): The instrument Z should affect the outcome Y ONLY through the variable X, and not through any other(!) channel (e.g. Z –&gt; u –&gt; Y darf nicht sein!) What is the First-Stage and why do you have to look at it? you want to show that the relevance assumption is satisfied, e.g. that there is indeed a correlation between your instrument and your coefficient of interest. you want a strong instrument, e.g. you want to see a statistical significance of at least 5% on your Z-variable. What is the Reduced-Form and why do you look at it? —&gt; You regress the outcome Y directly(!) on the instrument, without including your “X” in this regression. What is the “2 Stage-Least-Squares” regression? —&gt; this is the regression where you regress your Y on your estimated(!!) coefficient of interest (your statistical software skips the first stage). –&gt; by doing this you only use that part of the variation in your coefficient of interest “X”, that is due to the instrument “Z” which only correlates with “X” and NOT with other unobserved variables in the error-term “u”. —&gt; when you do an IV-Regression, you need to show this. It contains a First-Stage, where you regress your X (= coefficient of interest) on your instrument Z and a reduced form Usage of causal language? —&gt; at the 2SLS, you can use causal languange, if you assume the instrument to be valid (bzw. if you assume the exclusion restriction holds!) How to get the 2SLS-coefficient of interest if you only have the coefficients for the Reduced Form and the First Stage? –&gt; Berechnung: 2SLS = Reduced Form / First Stage LATE // Local average treatment effects –&gt; Estimates generated from instrumental variables are based on the individuals whose behavior is affected by the instrument. Thus, people refer to instrumental variable estimates as “local average treatment effects (= LATE)”, because these estimates are the average effects for a SUBSET (i.e. local part) of the population. Use of more than 1 instrument: You can use more than 1 instrument. However, all instrument need to be valid &amp; you need to include all instruments as explanatory variables in the estimation of the first stage. 8.25 Regression discontinuity design // RDD Requirement to RDD? –&gt; You need to have a “Cutoff Score” (= Assignment variable? –&gt; Synonyme?) which decides whether you get treated or not. This cutoff score needs to be a continuous variable. 3 Assumptions for the RDD that you need to check: Whether an individual gets treated should ONLY depend on the cutoff score! If - for example - an individual can still decide whether he participates in the treatment or not AFTER he knows his cutoff score, this assumotion is violated! The individuals cannot(!) perfectly control the cutoff score. The assignment variable // cutoff score cannot(!) be caused by the treatment or the outcome (of the treatment?) –&gt; reverse causality –&gt; Y-variable wird zur X-variable und X-variable zur Y-Variable… Which people do we compare in an RDD? —&gt; In a RDD, we only compare our individuals exactly above and below the treatment. That’s why we can say that - whether these indivuals receive the treatment or not - is basically random! The fact that those two individual being treated or not is random is nice, because the variable becomes basically independent (no correlation) from all other factors that we cannot conrtrol for. External validity with RDD? In an RDD, we compare the two individuals just above &amp; below the threshold value, e.g. we compare the very first person that does not get the treatment with the last person who gets treated. Generally, this generates precise and valid estimates (that are causal) –&gt; high internal validity. But because we look at two precise individuals when estimating our effect, the estimated effects are not generalizable, but rather: they estimate a local treatment effect, which can at most be applied to the individuals just around the threshold, but not for the rest of the sample! 8.26 Coded Algorithms &amp; R-Functions Monte Carlo Simulation: PS1, empirical methods, File: „ExpoR“ Construct a Scatterplot (= Streudiagramm) for the residuals, where you have the residuals on the Y-axis and a certain regressor on the X-Axis (—&gt; in order to check if A3 is satisfied): PS1, empirical methods, File: „Cigs“: Zeilen 20-33 (—&gt; Bemerkung: viel Zusatz, aber nützlicher Zusatz!) Construct a normal distribution: PS1, empirical methods, File: „Cigs“: Zeilen 29-33 Do an F-Test to check joint hypothesis: PS2, empirical methods, „PS2“: Zeilen 5-9 &amp; 45-49 Make a confidence intervall out of a regression: PS3, empirical methods, „PS3“: Zeilen 15-28 Only take the coefficient from a summary(regression_1) code: PS4, empirical methods, „PS4“: Zeilen 35; Zeilen 44-46 Reformulate a dataset, such that the unit of observation is not „the twin“, but rather the „family“: —&gt; PS4, empirical methods, „PS4“: Zeilen 52 Sort a dataset and only display some information &amp; drop some observations: PS4, empirical methods, „PS4“: Zeilen 83-84 Ommit NAs by looking at one particular column, from which you want to eliminate every NAs: Replication of Bonjour stud, lifestyle seminar, „Replication“: Zeilen 81-82 Make a beautiful table with two vectors (which are transformed to a dataframe): Replication of Bonjour stud, lifestyle seminar, „Replication“: Zeilen 163-182 Randomly add a person to a treatment group &amp; others to the control group: PS2, PeCi , „PS2“, Zeilen 119-121, Rmarkdown file Calculate means for treatment and control groups &amp; apply a t-test to see if they differ from each other in their characteristics: PS2, PeCi , „PS2“, Zeilen 129-133 Breusch-Pagan Test to test for heteroscedastic variances: PS3, PeCi, Aufgabe 2 a), Zeilen 66-68 Implementation of heterogeneous Treatment effects: PS3, PeCi, Aufgabe 4 a), Zeilen 112-118 &amp; 130-133 Calculate the average (total) treatment out of (multiple) heterogeneous treatment effects: PS3, PeCi, Aufgabe 4 b), Zeilen 137-139 Comparison of means in a DiD setting // summary statistics of a dataset using stargazer: PS5, Aufgabe 3 a), Zeilen 189-206 8.27 Nützliche Funktionen R: Count number of observations: NROW(dataset): counts the number of rows (= Zeilen = Anzahl Beobachtungen), OR dim(dataset): the first number would be the number of observations, the second one is the number of variables in the dataset Summary statistics of each variable in the dataset: summary(dataset) Simple linear bivariate regression model: lm(data$cigs ~ data$educ) Simple linear regression without a constant: lm(data$cigs ~ data$educ + 0), wobei hier zu beachten gilt, dass lm(y ~ x) Simple coefficient summary of a regression model: summary(regression_model) Autocorrelation-function // ACF: acf(residual, main=„title of the plot), need this to check A4 of the CLRM Create a dummy with one condition: ifelse(data$educ &gt;= 16, 1, 0) Append new vector (of same length as the dataset) to a dataset: data.frame(dataset, dummy_university, log-educ) Build a subset of a dataset: subset &lt;- subset(data, data$female ==0) Make a scatterplot (= Streuungsdiagram): plot(x, y, options) Make a histogram: hist(X, options) Adding new variable to the dataset: data[„new-variable“] &lt;- data$educ*data$noob Select a particular coefficient in your regression: regression$coefficients[5], Achtung: bei der selection wird der intercept mit in den code berücksichtigt!! Man hat also scho +1 gemacht, das wäre also der coefficient der 4. Variable!! Create a confidence intervall vector: predict(regression, intervall=„confidence“, level=.95) Delete a whole column (= variable) from a dataset: dataset$female &lt;- NULL Tell R in which format a date (= Datum) is: dataset$arrest_date &lt;- as.Date(datase$arrest_date, format =„%m/%d/%Y“) Umbenennung einer Spalte mit einem Spalten-Namen, welcher mehr Sinn macht: data$white_crime &lt;- data$PERP_RACE_WHITE 8.28 Useful Econometrics documents: Datascience: Datascience cheat-sheat:https://storage.ning.com/topology/rest/1.0/file/get/1211570060?profile=original r-graph-gallery.com python-graph-gallery.com Different Statistical Tests &amp; Models: Common statistical tests are linear models: https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf Statistical Power Calculator: http://powerandsamplesize.com Difference-in-Differences // DiD: Revisitting parallel trend assumption: https://blogs.worldbank.org/impactevaluations/revisiting-difference-differences-parallel-trends-assumption-part-i-pre-trend Machine Learning: Fussball Machine Learning code in python: https://mariamsulakian.com/2018/02/01/machine-learning-predicting-the-2018-epl-matches/ "],["machine-learning.html", "Chapter 9 Machine Learning 9.1 Typen von Machine Learning 9.2 Supervised Learning 9.3 Methods 9.4 Wichtige Funktionen 9.5 Uni-Kurs Neural Networks &amp; Deep Learning 9.6 Quellen", " Chapter 9 Machine Learning In diesem Kapitel, werde ich vertiefter in das Thema “Machine Learning” eingehen. 9.1 Typen von Machine Learning Es gibt grundsätzlich 2 Kategorien, in denen die verschiedenen Methoden in Machine Learning eingeordnet werden: Supervised Learning Goal 1: Classification Image Classification Goal 2: Predictions with Regressions Translation (zum Beispiel angewendet in Deepl.com) Image Captioning (siehe Bild): Unsupervised Learning Clustering: Matching // K-Means Clustering Dimension Reduction: Principal Component Analysis (PCA) Outlier Detection 9.2 Supervised Learning Bei Supervised Learning wird die Annahme gemacht, dass man die Funktion ‘f’ kennt, um mittels den Werten der Zufallsvariablen ‘X’ die Werte der Zufallsvariablen ‘Y’ herauszufinden. Mathematisch ausgedrückt: f : X -&gt; Y. 9.3 Methods Hier soll eine Auflistung aller Vorgehen bei Machine Learning aufgelistet werden: Algorithm for “best fit” to find the weights: Guess some random weights. “Go downhill”, e.g. apply a learning rate when using the method gradient descent. Is the fitted line good enough? If the answer is “no”, then go back to point 2). 9.4 Wichtige Funktionen Da Machine Learning mittels Aktivierungsfunktionen funktioniert, folgt eine Übersicht zu verschiedenen Funktionen: Identity Function: damit ist die Funktion f(x) = x bzw. y = x gemeint: Logitstic Function // Sigmoid Function // S-Shaped Function: damit ist die Funktion \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) gemeint: 9.5 Uni-Kurs Neural Networks &amp; Deep Learning 9.5.1 Notation Neural Networks Machine Learning verwendet unterschiedliche Begriffe für diverse gleiche Konzepte &amp; Definitionen aus den Wirtschaftswissenschaften. Nun geht es darum, die korrekte Übersetzungen für diese Wörter aufzulisten: weights = Beta-Coefficients = parameters = a, b = neurons Input units = independent variables “x” inputs = Anzahl Observations in Total Output unit = Dependent variable “y” = labels Activation function (AF) = you need this function to plug in your estimated regression model. Examples of AF: logit, probit, relu, simple “threshold” AF etc… Perceptron = Linear Binary Classifier = usually, the perceptron is a linear separator (= line that separates group in a regression) = Perceptron is a single layer neural network Multi-layer perceptron == Neural Networks. Example: learning rate = how far to go in a particular direction features = inputs = independent variables “x” = Xki labels = this is the “true Y” you observe in the real world = output = dependent variable “going downhill” = this is the learning process that you get by using the method “gradient decent” (look youtube video of user called “the coding train” at ca. 17:30) &amp; applying a “learning rate” to it yk = this is the estimated regression function zk = “logits”, e.g. this is the whole sum of the weights multiplied by the x-variables (= entire regression), but this time we put this entire regression as an input into the logistic function –&gt; in other words: the same as yk but we then apply the specific activation function “logit function” to the estimated zk // \\(\\hat{y}\\) Input Layer = Layer 0 = very first set of Neuron Output Layer = Last Layer = last set of Neurons Hidden Layers = All Layers between the input &amp; output Layer input node = nodes at the input layer output node = nodes at the output layer hidden nodes = nodes, which are in the hidden layers or at the output layer &amp; don’t give out outputs Feed forward neural networks = connections only between layer i and layer i+1 Convolutional neural networks = a type of feed-foward network Recurrent neural networks = connections flow backwards to previous layers as well supervised learning = function estimation There are 2 different types: regression, classification unsupervised learning = structure the data into groups (very subjective) // detecting patterns. Can also be used for: data reduction, outlier detection loss-function = cost-function = [TRUE y - ESTIMATED \\(\\hat{Y}\\)]2 = error –&gt; we define the loss-function be the “least squares” identity function = y = x bzw. f(x) = x sigmoid function = logit function bias term = error term Note: Oftmals wird der “input” für den bias term als Zahl “1” angegeben (siehe Bild oben “Example of Multi-Layer Network”, wo der bias term als Zahl “1” angegeben ist.) Epoch = means we go through the whole data set once –&gt; default is ten epochs Net Input Function = Regressionsmodell als Ganzes = sum of all weighted inputs // “x” kernel = (starting) values for the weights regularizers = penalties that are used to reduce overfitting (of the starting values for the weights?) backpropagation = back pass = when we have our error-term, we can calculate the gradient and - if the error was too big - we can backpass the error-term with the help of the learning rate to a previous layer and estimate new // better weights breaking symmetry = principle, which says that you need to have different initial weights for hidden units with the same activation function and same inputs batches = these are smaller samples that you take from the whole dataset, e.g. you take only a fraction of the dataset –&gt; you use batches because the computation gets faster rather than putting the whole dataset into the machine learning “Apparat” –&gt; Rule: the higher the batch size, the better estimates you get decay = In Machine Learning, it has become kind of standard to make learning rates dynamic, e.g. first have bigger learning rates, because you can be very wrong at the beginning with the random weights, but then - towards the end of estimation - you adapt the learning rate only very smoothly, since you slowly go towards the optimum –&gt; typically, this decay will make the learning rate smaller as the training continues. momentum = makes learning rates dynamic –&gt; If you see that - in the history of gradients - the gradients point generally in the same direction, momentum will adjust the learning rate by increasing the step size hyperparameters = examples are: Learning rate, Learning Rate Decay, Momentum, Batch Size, Weight / Bias initialization Confusion Matrix = C = shows - in the diagonal of the matrix - how many times your predicted outcome was the same as the actual outcome. All the other numbers are saying that your model’s prediction was not in line with the actual outcome Precision = if i look at a guess // prediction, how many % my algorithm guessed correctly? –&gt; E.g. Anteil der predicted outcome \\(\\hat{y}\\), welche korrekt mit den ture outcomes übereinstimmen. Mathematisch ausgedrückt: Im Zähler die Anzahl an übereinstimmenden predicted outcomes \\(\\hat{y}\\) &amp; im Nenner Totale Anzahl an predicted Outcomes \\(\\hat{y}\\). Recall = if i look at an actual true outcome, how many % where guessed correctly? –&gt; E.g. Anteil der true outcome Y, welche korrekt vorhergesagt wurden Mathematisch ausgedrückt: Im Zähler die Anzahl an korrekt vorhergesehenen true outcomes &amp; im Nenner Totale Anzahl an True Outcomes. Training data = Training set is the one on which we train and fit our model basically to fit the parameters. Testing data = Testing Data is used only to assess performance of the model. Variance = Overfit = you use too much X’s // features in your model –&gt; you get too much variance in your predictions Bagging = Train the same architecture on different subsets of data Boosting = Train different model architectures on the same data data augmentation = get more data by adding noise on the input layer weight tying = Make the weights similar 9.5.1.1 Alphabetisch sortiert: Um die Begriffe noch leichter zu finden, habe ich sie hier noch alphabetisch sortiert: Activation function = you need this function to plug in your estimated regression model. Examples of AF = logit, probit, relu, simple “threshold” AF etc… backpropagation = back pass = when we have our error-term, we can calculate the gradient and - if the error was too big - we can backpass the error-term with the helplearning rate to a previous layer and estimate new // better weights batches = these are samples from the whole dataset –&gt; you use batches because the computation gets faster rather than putting the whole dataset into the machine learning “Apparat” –&gt; Rule: the higher the batch size, the better estimates you get bias term = error term breaking symmetry = principle, which says that you need to have different initial weights for hidden units with the same activation function and same inputs Confusion Matrix = C = shows - in the diagonal of the matrix - how many times your predicted outcome was the same as the actual outcome. All the other numbers are saying that your model’s prediction was not in line with the actual outcome Convolutional neural networks = a type of feed fowardnetwork decay = make learning rates dynamic –&gt; typically, this decay will make the learning rate smaller as the training continues. Epoch = means we go through the whole data set once –&gt; default is ten epochs features = inputs = independent variables “x” = X(ki) Feed forward neural networks = connections only between layer i and layer i+1 “going downhill” = this is the learning process that you get by using the method “gradient decent” (look youtube video by “the coding train” at ca. 17:30) &amp; applying a “learning rate” to it Hidden Layers = All Layers between the input &amp; output Layer hidden nodes = nodes, which are in the hidden layers or at the output layer &amp; don’t give out outputs hyperparameters = examples are: Learning rate, Learning Rate Decay, Momentum, Batch Size, Weight / Bias initialization identity function = y = x bzw. f(x) = x inputs = Anzahl Observations in Total Input Layer = Layer 0 = very first set of Neuron input node = nodes at the input layer Input units = independent variables “x” kernel = (starting) values for the weights labels = this is the “true Y” you observe in the real world = output = dependent variable learning rate = how far to go in a particular direction loss-function = cost-function = [TRUE y - ESTIMATED Y(hat)]^2 = error –&gt; we define the loss-function be the “least squares” Net Input Function = Regressionsmodell als Ganzes = sum of all weighted inputs // “x” momentum = makes learning rates dynamic –&gt; If you see that - in the history of gradients - the gradients point generally in the same direction, momentum will adjust the learning rate by increasing the step size Output Layer = Last Layer = last set of Neurons output node = node at the output layer Perceptron = Linear Binary Classifier = linear seperator (= line that separates group in a regression) = Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Precision = if i look at a guess // prediction, how many % my algorithm guessed correctly? Recall = if i look at an actual true outcome, how many % where guessed correctly Recurrent neural networks = connections flow backwards to previous layers as well regularizers = penalties that are used to reduce overfitting (of the starting values for the weights?) sigmoid function = logit function saling = pre-processing data supervised learning = function estimation –&gt; 2 different types: 1) regression; 2) classification Training data = Training set is the one on which we train and fit our model basically to fit the parameters. Testing data = Testing Data is used only to assess performance of the model. unsupervised learning = structure the data into groups (very subjective) // detecting patterns // data reduction weights = Beta-Coefficients = parameters = a, b = neurons y(k) = this is the estimated(!!) regresion function z(k) = “logits” –&gt; the same as y(k) but we then apply the specific activation function “logit function” to the estimated z(k) // y(hat) 9.5.2 More ML-Jargon \"feed in\" == plug in (–&gt; häufig im Zusammenhang mit Einsetzen von konkreten Werten für die x-Variablen in das geschätzte Modell, um Predictions zu erhalten) instance == value within a cell == konkreter “x”-Wert, welcher angenommen wird und du - beispielsweise - für eine Prediction verwenden kannst. Forcasted data == These are the predictions that you do via the help of a model (–&gt; by plugging in concrete x-values), that you’ve built. Training Dataset // Training Set == This is the sample, that you use to estimate your model. Testing Dataset // Test Set == This is the hold out set, which you use at the end, to check whether your model is able to generalize // extrapolate well to new data. \"the model is learning == what is meant by “learning” is: given some Datenpunkt-Wolke, the computer will try and fit the “best line” [oftentimes by minimizing the sum of the squared residuals, if you use OLS (= Ordinary Least Squared)] to construct the “best model” possible. Training == You are estimating a model on the trainig-dataset, such that the model “learns” from the data. shuffle == englisches Wort für “mischen”. In the context of splitting the dataset into test- &amp; training-data, it is common practice to shuffle your observations within your dataset first. Reason why you shuffle?: Shuffling data serves the purpose of reducing variance AND making sure that models remain general AND overfit less. Merke: When dealing with time series, you should not use shuffle when splitting the data into training- &amp; testing-data. training score // Test Error == This error // score is the [oftentimes squared] difference between the true y-variable and estimated // predicted y-variable and measures // evaluates how well the model is “performing” // how good the model is on a test set with an increasingly bigger training dataset. cross-validation score // Validation Error == This is the (mean) error // score [because of cross-validation] that measures // evaluates how the model is “learning” over increasingly bigger training datasets. fold == subset of the training dataset Example: In a K-Fold Cross-Validation, you randomly split the training set into 10 distinct subsets, which are called folds. samples == number of rows // number of observations within a dataset. classifiers == These are simply regression-functions, where the Y-Variable is binary, e.g. die Y-Variable kann nur Y = 0 ODER Y = 1 als Werte annehmen. Example: Logit- &amp; Probit-Regression, or Naiv Bayes, etc… initialization // declaration // specification == This is the assignment of an initial value for a data object or variable. Array == A List of Data is an array. It is a data structure, which contains “n” objects within a list. Quelle: The Coding Train 3:10-3:22 Extrapolation == Voraussage von Y-Predicted Values, welche mittels X-Variablen ermittelt werden, welche zuvor nicht im Datensatz waren. Interpolation == Voraussage von Y-Predicted Values, welche mittels X-Variablen ermittelt werden, welche bereits im Datensatz waren, als die Schätzung getätigt wurde. sliding window == This is just a way to tell python how to do a cross-validation and have equal lengths of time series where we can learn on. Learning Task == Forecasting Task // Extrapolation [in Time Series] Forecasting Horizon == Time Points, you want to predict (= dein ) \\(\\hat{y}\\) Time Heterogenous Data == Different Time Series have different time stamps Quelle: Ab 27:05 (Link: http://www.youtube.com/watch?v=Wf2naBHRo8Q&amp;t=27m05s) Seasonal Periodicity == The number of months per year, in which the forecaster expects to see a seasonal pattern. Concrete Example: In Philipp’s Notebook, he had a seasonal periodicity of 2, e.g. he said that in winter &amp; sommer, he expects a seasonal pattern. Reduction is composable → Synonym wäre “addieren” → E.g. you can split a difficult task into a bunch of smaller tasks (= reduction) and “add” them together to solve the bigger task at the end (= “reduction is composable”). Pearson Korrelation == Lineare Korrelation Grid → Das ist nichts anderes als die Optimierung von diversen “Modell-Parametern”. - Beispiel: Bei Random-Forrest Modellen kann man zum Beispiel die Tiefe eines Modells bestimmen, dh die relevante Frage, welche ein Forscher sich stellt, ist, wie viele Baum-Zweigungen die geeignesten Predictions bringen? Mit Funktionen, wie zum Beispiel GridSearchCV() kann dieses Problem geregelt werden. Classifier == These are simply Regression-Functions, where the Y-variable is binary, e.g. die Y-Variable kann nur Y = 0 oder Y= = 1 als Werte annehmen. Beispiele: Logit-Regression Probit-Regression Naive Bayes etc… 9.6 Quellen https://blog.exxactcorp.com/lets-learn-the-difference-between-a-deep-learning-cnn-and-rnn/ https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/notes/Sonia_Hornik.pdf "],["mengenlehre.html", "Chapter 10 Mengenlehre 10.1 Wörterbuch 10.2 Mathematische “Schrift” lesen", " Chapter 10 Mengenlehre Du wirst ein bestimmtes Maß an Wissen in der Mengenlehre brauchen, denn dieser Teil der Mathematik ist besonders wichtig: für Statistik: weil in Statistik mit Mengen an possible outcomes gearbeitet wird. für Choice Theory: weil dort mit einer Menge an possible choices gearbeitet wird. Häufige Schwierigkeit, welche ich bei diesem Thema begegnet bin, ist das Nachvollziehen der mathematischen Schreibweise von Mengen, insbesondere in Kombination mit Funktionen! Wieso ist dies problematisch? Weil ich dann nicht einmal verstehe, was ich lese¨ Und das ist schlimm! Denn, ohne Verständnis, kann man gar nichts lernen. Man ist dann bloss jemand, der gut auswendig etwas nachplappert, was schlecht (&amp; gefährlich) ist! 10.1 Wörterbuch Mapping = Mapping is simply a (general) function between two methematical objects or structures. In many branches of mathematics, the term mapping (or map) is used to mean a function. Visualisierng: - Note: More than one x&lt;sub&gt;a&lt;/sub&gt; can point to a element of Y (= y&lt;sub&gt;a&lt;/sub&gt;), but every y&lt;sub&gt;a&lt;/sub&gt; is only associated with one unique x&lt;sub&gt;a&lt;/sub&gt;. 10.2 Mathematische “Schrift” lesen “Y is a function of X”: y(x) y = f(x) y depends on x y as a function of x Was heisst f: X &amp;#8594; Y? In Worten heisst dies: A function f from X to Y. Beispiel anhand einer “choice correspondance” (= C) definition: In Worten: A function C from the power set of X (= 2&lt;sup&gt;X&lt;/sup&gt; → das ist unser x in der Funktion C) (wihtout leere Menge) to the power set of X (= 2&lt;sup&gt;X&lt;/sup&gt; → das ist unser y in der Funktion C) (without leerer Menge). **Weitere Beispiele für f: X #&amp;8594; Y: ${delta}: Y #&amp;8594; [0, 1] In Worten: The function ${delta} - when applied to Y (= our X) - will assign a value in the intervall between 0 and 1 to Y, which will be the outcome. f: X x Y #&amp;8594; [0, 1], assume X is a vector, Y is a vector too In Worten: The function f - when applied to each combination of X and Y (= our inputs) - will assign a value in the intervall between 0 and 1 to each combination of X and Y, which will be the outcome… "],["python-mustercodes.html", "Chapter 11 Python Mustercodes 11.1 Inhaltsverzeichnis 11.2 ## Installation of Packages 11.3 ## Need help &amp; documentation 11.4 ## Path Handling 11.5 ## Save data 11.6 ## Read data // Load data 11.7 ## Vanilla Exploration of your Dataset 11.8 ## Selection &amp; Filtering Rows and Columns 11.9 ## Selection with “accessor operators” iloc &amp; loc 11.10 ## Sorting &amp; Filtering 11.11 ## Data-Cleaning: Missings 11.12 ## Missing Imputation 11.13 ## Data-Cleaning: Duplicates 11.14 ## Data-Cleaning: Outliers 11.15 ## Merging &amp; Splitting 11.16 ## Advanced Exploration of your data 11.17 ## Time-Series: Exploration &amp; Visualizations 11.18 ## Feature Engineering for Time-Series 11.19 ## Rechnen mit Date-Time Objects: Addition und Subtraktion von Zeiten 11.20 ## Dummy-Variables 11.21 ## Dummy-Variables: Create Columns for specific hours, seasons of the year etc… 11.22 ## Scaling Variables 11.23 ## Umwandlungen 11.24 ## Apply If-Conditions on a DataFrame 11.25 ## Visualization of the Data 11.26 ## Python Magic Commands 11.27 Show ALL Columns of a DataFrames 11.28 Working with HTML within Jupyter-Notebooks 11.29 Share Variables between Notebooks 11.30 ## Useful “Tricks” I stumbled upon 11.31 ## Math Tricks for Hacks 11.32 ## Fun", " Chapter 11 Python Mustercodes In diesem Dokument habe ich eine Liste von diversen Python-Codes, welche mir in meiner Arbeit als Data Scientist behilflich sein könnten. I hope you find what you are looking for =) 11.1 Inhaltsverzeichnis &lt;a id=&quot;gebe-hier-passenden-id-namen&quot; style=&quot;color:black; text-decoration: none;&quot;&gt;&lt;/a&gt; List Comprehensions String Manipulation Package Installation De-Install Packages with pip Help Path handling Save Data … as .CSV Read Data Load Excel Load .CSV … via a Website’s URL Load Pickle Vanilla Data Exploration Number of Rows View whole Dataset View 5 first observations in the Dataset View 5 last observations in the Dataset Number of Missings per Column within a dataset Find the Data Type of a Variable Drop 1 OR multiple Columns Re-Name 1 OR multiple Columns .reset_index() and .set_index() Summary of a Dataset Show only unique() Columns Median &amp; Mean for a particular Column Apply a Fustom-Function f(x), but only on certain rows within a Dataframe via numpy Transform a Series to a Dataframe Assign a unique ID Selection &amp; Filtering of Rows &amp; Columns Select 1 Column in a Dataframe Example with “dot-notation”, e.g. df.a_Column Select multiple Columns in a Dataframe Select AND Filter for Rows with 1 Condition within Dataframe Select AND Filter for Rows with multiple Conditions within a Dataframe Select only Rows with Missing-Values Selection with “Accessor Operators” loc &amp; iloc Example: Select 1st Row and 5th Column with iloc Example: Select 1st Row and 5th Column with loc Example: Select with a Threshold-Value via loc Example: Select entire Rows via iloc Example: Select entire Rows via loc Replace Values within Rows Slicing is different when using loc or iloc Sorting &amp; Filtering Sorting (Pearson) Correlation _from most-important to least-important Example: Select only highest Correlations, then Sort them and print() them Dealing with Missings Count total Number of Missings within a Dataframe Count Missings per Columns Count Non-Missings, but ONLY IF you don’t have the value 0 in it Show only Rows with missings Replace all Missings with 0’s Missing-Imputations Missing-Interpolation for Time-Series Duplicate Data Count all Duplicates in the Dataframe Drop Duplicates Outliers Draw a Boxplot Merging &amp; Splitting Split bigger Dataset by Categories into Subsets Merging 2 Dataframes by their (row) index Merging multiple Datasets _simultaneously Advanced Data Exploration Create a Correlation-Matrix from a Dataframe Time Series Exploration Plot Time-Series Autocorrelation &amp; Partial-Autocorrelation Cross-Correlation Feature Engineering Creating a Time-Index Copy Index as a Column Transform a “Pseudo”-Time-Column into a Date-Time-Column and set it as the index Create a range() of Dates Rechnen (Addition &amp; Substraktion) mit Date-Time Genauer Zeitpunkt “jetzt” (Heute // Today) herausfinden Zukunft relativ zu “jetzt” Subtraktion, um vergangenen Zeitpunkt zu berechnen Herausfinden, welcher Wochentag (Mo / Di / Mi etc…) ein bestimmtes Datum war Create Dummy-Variables for Time-Variables Create “normal” Dummies “Flagg”-Dummy by Merging Dummy for Hours Dummy for Quarters Dummy for Days Scaling Variables Normalization Series to Array Data Visualization Plot and Save Image Multiple Plots with For-Loops Umwandlungen von Data Types in einen anderen Data Type Convert Series to Array Useful “tricks” I stumbled upon Reshape(-1,1) to create a list within a list Math-Tricks for “Hacks” Round Downwards Calculate the Median Python Magic Commands View all Columns Working with HTML within Juypter-Notebooks Share Variables between Notebooks 11.1.1 List Comprehensions: List Comprehensions are a concept that is on of Python’s most beloved and unique features. It’s basically a loop where you apply a function to each of the elements during the loop. Its Output is a list. Examples: 11.1.2 List comprehension where we apply a function: x2 List comprehensions are really nice if you want to iterate over a column and apply a function on a particular column of your dataset. squares = [n**2 for n in range(10)] # apply f(x) = x^2 on a list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] squares [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] print(type(squares)) # Check output: should be a &#39;list&#39; &lt;class &#39;list&#39;&gt; 11.1.3 List comprehension with an if condition: planets = [&#39;Mercury&#39;, &#39;Venus&#39;, &#39;Earth&#39;, &#39;Mars&#39;, &#39;Jupiter&#39;, &#39;Saturn&#39;, &#39;Uranus&#39;, &#39;Neptune&#39;] # this is the list we will loop through short_planets = [planet for planet in planets if len(planet) &lt; 6] short_planets [&#39;Venus&#39;, &#39;Earth&#39;, &#39;Mars&#39;] 11.1.4 String Manipulation: 11.1.5 Transform a list that contains strings into a WHOLE string, only separated by commas: # step 0: create a list with many strings in it planets = [&#39;Mercury&#39;, &#39;Venus&#39;, &#39;Earth&#39;, &#39;Mars&#39;, &#39;Jupiter&#39;, &#39;Saturn&#39;, &#39;Uranus&#39;, &#39;Neptune&#39;] # Step 1: Transform the whole list into a single &quot;string&quot; str(planets) # should output: &quot;[&#39;Mercury&#39;, &#39;Venus&#39;, &#39;Earth&#39;, &#39;Mars&#39;, &#39;Jupiter&#39;, &#39;Saturn&#39;, &#39;Uranus&#39;, &#39;Neptune&#39;]&quot; # Step 2: Now, we replace all characters that are unnecessary - such as &#39; [ and ] -such that we return a whole string, # only separated by commas, with no whitespace in between them: n = str(planets).replace(&quot;&#39;&quot;, &quot;&quot;).replace(&#39;[&#39;, &#39;&#39;).replace(&#39;]&#39;, &#39;&#39;).replace(&#39; &#39;, &#39;&#39;) # replace everthing by empty-strings print(n) # Final output Mercury,Venus,Earth,Mars,Jupiter,Saturn,Uranus,Neptune 11.2 ## Installation of Packages 11.2.1 Installing with package-manager conda: Conda is a package manager, which will - before starting to install a package that you want - check which dependencies are needed for the package to be able to be used. Example: If you use sktime, then conda can detect that there ma be a conflict with the current version of - for example - the package Tornado. *Hence, conda will not only download sktime, but will also bring the package Tornado onto a newer version, such tht it will become compatible with sktime. Note: The installation via conda can take a while, since many dependencies will be checked! Improtant: Use the terminal for the following code. 11.2.1.1 To install a specific package - for example - sktime into an existing virtual-environment called \"myenv\", type the following into the terminal: conda install --name myenv sktime 11.2.1.2 Install the package sktime into the current (global) environment (= dh “normally”), type the following into the terminal: conda install sktime 11.2.2 Installing packages with pip: Der Nachteil von pip install [some package-name here] VS. conda install [some package-name here] liegt darin, dass pip keine Checks macht, ob die Versionen von verschiedenen Packages, die man verwendet, überhaupt kompatibel sind. 11.2.2.1 Alternative Installation, by using pip within the terminal: pip install sktime 11.2.3 Install some Packages // Modules, or Sub-Modules: import numpy as np import matplotlib.pyplot as plt from matplotlib.dates import date2num import pandas as pd from scipy import stats from datetime import datetime 11.2.4 Deinstall Packages mit Pip: pip uninstall fbprophet prophet cmdstanpy 11.3 ## Need help &amp; documentation help() Welcome to Python 3.8&#39;s help utility! If this is your first time using Python, you should definitely check out the tutorial on the Internet at https://docs.python.org/3.8/tutorial/. Enter the name of any module, keyword, or topic to get help on writing Python programs and using Python modules. To quit this help utility and return to the interpreter, just type &quot;quit&quot;. To get a list of available modules, keywords, symbols, or topics, type &quot;modules&quot;, &quot;keywords&quot;, &quot;symbols&quot;, or &quot;topics&quot;. Each module also comes with a one-line summary of what it does; to list the modules whose name or summary contain a given string such as &quot;spam&quot;, type &quot;modules spam&quot;. 11.4 ## Path Handling Sehr oft wirst du Modelle, die eventuell stundenlang gelernt haben oder Daten-Files etc… laden &amp; speichern müssen. Hierfür ist es von hoher Bedeutung, dass du gut mit Pfaden umgehen kannst, insbesondere mit der Library os! Zwei Haupt-Formate, die du verwenden wirst für das Abspeichern &amp; Laden von Files, sind: Das .csv-Format. Das .pkl-Format. etc… Es gibt noch viele andere Formate, aber die obigen beiden sind die wichtigsten, denen ich bisher begegnet bin. 11.4.1 Set the Path, where your Computer should Save Data / Models # Read data // Load data ---import os # this is the library that can handle paths save_folder = os.path.expanduser(os.path.join(&quot;~&quot;, # our User&#39;s &quot;home&quot;-directory --&gt; for my Mac it is: &quot;/Users/jomaye&quot; &quot;Dokumente&quot;, # Next, we jump 1 directory called &quot;Dokumente&quot; further below &quot;Programming&quot;)) # Allgemeint: each string after a Komma is a new directory you can set. This can go on infinitively ;) save_folder # check if it worked? --&gt; yes! Now you see your path =) &#39;/Users/jomaye/Dokumente/Programming&#39; 11.5 ## Save data 11.5.1 Save as CSV-File: dynamic_name = &quot;name_of_a_specific_characteristic_of_the_variable_that_you_want_to_save&quot; # this is needed for &quot;dynamisches speichern&quot; via F-String YOUR_VAR_NAME.to_csv(f&quot;Name-of-the-CSV-{dynamic_name}.csv&quot;) # I use f-strings since it allows me to adapt the CSV-filenames to a specific characteristic, that I used for a model / method etc... 11.6 ## Read data // Load data In order to load datasets, you will need the library pandas. For some cases, additional libraries may be needed. 11.6.1 Load data from Excel-Sheets: epex_df = pd.read_excel(&quot;./Data_V2/Preis_aktuell_Spot_EEX_CH-19-ver-mac.xlsx&quot;, # plug in the correct path header=[1], # The dataset Column-names beginnt ab 2. Zeile (--&gt; 1. Zeile ist der Titel des Excel-Files) sheet_name=&#39;Prices&#39;, # If you have more than 1 Excel-Sheet within the Excel-File, you need to specify # which sheet you want to load engine=&#39;openpyxl&#39;) # This input will (sometimes) be needed if you load data from an Excel-File via # a Windows-Computer, otherwise it can print an error! epex_df # output the ddset a 11.6.2 Load Dataset via a CSV-File: test = pd.read_csv( &quot;C:/Users/u235051/Downloads/ETS_Database_v38/ETS_Database_v38.csv&quot;, sep=&#39;\\t&#39; # Im CSV-File waren die Spalten via &quot;Tab&quot;-Taste separiert, deshalb diese option zwingend anzugeben ist (ansonsten Error!) ) test .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country country_code ETS information main activity sector name unit value year 0 Belgium BE Verified emissions 35 Production of pulp tonne of CO2 equ. 102581.0 2009 1 Belgium BE Verified emissions 35 Production of pulp tonne of CO2 equ. 106671.0 2011 2 Belgium BE Verified emissions 35 Production of pulp tonne of CO2 equ. 126702.0 2012 3 Belgium BE Total surrendered units 35 Production of pulp tonne of CO2 equ. 98349.0 2007 4 Belgium BE Total surrendered units 35 Production of pulp tonne of CO2 equ. 96708.0 2008 … … … … … … … … 57389 Hungary HU Verified emissions 10 Aviation tonne of CO2 equ. 12599748.0 Total 3rd trading period (13-20) 57390 Hungary HU 1.1.3 Free allocation for modernisation of ele… 23 Metal ore roasting or sintering tonne of CO2 equ. 0.0 Total 3rd trading period (13-20) 57391 Hungary HU 1.1.3 Free allocation for modernisation of ele… 30 Production of lime, or calcination of dolom… tonne of CO2 equ. 0.0 Total 3rd trading period (13-20) 57392 Hungary HU Total surrendered units 43 Production of hydrogen and synthesis gas tonne of CO2 equ. 883972.0 Total 3rd trading period (13-20) 57393 Ireland IE 1.1.3 Free allocation for modernisation of ele… 10 Aviation tonne of CO2 equ. 0.0 Total 3rd trading period (13-20) 57394 rows × 7 columns 11.6.3 Load Dataset via a URL from a Website: In order to be able to access a dataset stored as a csv-file on a website, we will need to use - besides pandas - an additional library called requests. 11.6.3.1 Step 1: Make the file ready to be downloaded be sending a query to the remote-server // website that hosts the csv-file. import requests # load the library needed download_url = &quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/nba-elo/nbaallelo.csv&quot; # absolute URL target_csv_path = &quot;nba_all_elo.csv&quot; # name of the .csv-file that contains the data response = requests.get(download_url) # using an API that &quot;gets&quot; (= http-protocol language) the data from the server response.raise_for_status() # Check that the request was successful with open(target_csv_path, &quot;wb&quot;) as f: f.write(response.content) print(&quot;Download ready.&quot;) Download ready. 11.6.3.2 Step 2: Load the actual data. import pandas as pd # load the library needed nba = pd.read_csv(&quot;nba_all_elo.csv&quot;) # load the data --&gt; ddset is called &#39;nba&#39; type(nba) # check if it worked? --&gt; should output: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; pandas.core.frame.DataFrame nba.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gameorder game_id lg_id _iscopy year_id date_game seasongame is_playoffs team_id fran_id … win_equiv opp_id opp_fran opp_pts opp_elo_i opp_elo_n game_location game_result forecast notes 0 1 194611010TRH NBA 0 1947 11/1/1946 1 0 TRH Huskies … 40.294830 NYK Knicks 68 1300.0000 1306.7233 H L 0.640065 NaN 1 1 194611010TRH NBA 1 1947 11/1/1946 1 0 NYK Knicks … 41.705170 TRH Huskies 66 1300.0000 1293.2767 A W 0.359935 NaN 2 2 194611020CHS NBA 0 1947 11/2/1946 1 0 CHS Stags … 42.012257 NYK Knicks 47 1306.7233 1297.0712 H W 0.631101 NaN 3 2 194611020CHS NBA 1 1947 11/2/1946 2 0 NYK Knicks … 40.692783 CHS Stags 63 1300.0000 1309.6521 A L 0.368899 NaN 4 3 194611020DTF NBA 0 1947 11/2/1946 1 0 DTF Falcons … 38.864048 WSC Capitols 50 1300.0000 1320.3811 H L 0.640065 NaN 5 rows × 23 columns 11.6.4 Load Pickle-Files: filename = &quot;Put_Your_Pickle-File_Name_Here&quot; # Alternativ: os.path.join(data_folder_variable_where_pkl_is_saved, filename) test_loaded_pkl = pickle.load(open(filename, &#39;rb&#39;)) 11.7 ## Vanilla Exploration of your Dataset After having told Python to read in your dataset, the first thing you will want to do is to get (very) familiar with your dataset. This is key, otherwise you will not be able to perform a good data analysis! 11.7.1 Find out the number of observations // rows in your dataset: len(nba) # to get the number of observations // rows # note: &#39;nba&#39; is the name of the ddset 11.7.2 Find out the number of rows &amp; columns within your dataset: nba.shape # to get number of rows AND columns # note: &#39;nba&#39; is the name of the ddset 11.7.3 View the whole Dataset: nba # just type in the name of the variable in which your dataframe is stored in --&gt; &#39;nba&#39; is the name of your dataframe here 11.7.4 View the first 5-rows of your dataset: nba.head() # head() is often used to check whether your dataset really contains data you care about # here we check: does the ddset really contains data about the NBA? Achtung: If you have alot of columns, Python will not display them all when you use head(). However you can change the settings via: pd.set_option(&quot;display.max.columns&quot;, None) # this will tell Python: &quot;show me ALL the columns!&quot; nba.head() # execute &#39;head()&#39; again to check if the setting changed correclty? --&gt; yes! 11.7.4.1 You can also View the 5 last rows of your dataset by using tails(): nba.tail() # View last 5 rows # Viewing very specific rows is also possible: nba.tail(3) # Here, we view the last 3 rows 11.7.5 Find out the Data Type of each column within your dataset &amp; number of non-missing values: With the following simple code, we can find out, whether the columns are from the type of an integer, a string, a boolean etc… nba.info() # this will output all the types of each column in your dataset &amp; how many NON-missings you have per column # note: Pay attention to any columns that are from the type &#39;object&#39;! --&gt; lese bemerkung unten... To get a beautiful overview over all types that exist in Python, I recommend you to visit the website of W3Schools.com Bemerkung hier: It can be, that columns are from the type object. In practice, it often means that all of the values in an object-column are strings. If you encounter any object-columns, it is strongly recommended that you convert them into a more apropriate data-type, otherwise some of the functions won’t work on these object-columns… 11.7.5.1 Find the type of any object in Python: x = 5 print(type(x)) 11.7.6 Drop specific Columns: Note that axis = 1 denotes the columns that will be droped, while axis = 0 (default), will denote the rows that should be dropped. df_energy = df_energy.drop([&#39;tInfoDaySin&#39;, &#39;tInfoDayCos&#39;, &#39;tInfoYearSin&#39;, &#39;tInfoYearCos&#39;], axis=1) 11.7.7 Re-Name the Columns of a DataFrame: df2 = df.rename({&#39;oldName1&#39;: &#39;newName1&#39;, &#39;oldName2&#39;: &#39;newName2&#39;}, axis=&#39;columns&#39;) # not all columns have to be renamed, only those with a new name 11.7.8 Set &amp; Reset the Index / Row-Label: df_energy = df_energy.set_index(&#39;Date-Time&#39;) # to set the index df_energy = df_energy.reset_index() # to reset the index --&gt; df.reset_index(drop= True) will drop the index, which would # otherwise become a new column instead of just dropping it! 11.7.9 Make a Summary out of all the variables in your dataset: Note that, in order to be able to do some summary-statistics, you will need the additional library numpy. import numpy as np # load numpy for summary-statistics nba.describe().round(2) # results will be rounded onto 2 digits Achtung: If you have columns from the type object, you will need a slightly different version of the describe() function to display some summary-statistics from such columns. nba.describe(include=object) # if you have some weird columns being of the type &#39;object&#39; 11.7.10 Find the unique values from a column: This can be useful, when you want to filter all unique categories within a column. You can then put all those categories wihtin a new variable in order to loop through them to apply some function to those. gapminder[&#39;continent&#39;].unique() 11.7.11 Calculating the mean or median from a particular column: For the mean, you will need the following code. mean_points = reviews.points.mean() # calculate the mean of the column &#39;points&#39; within the ddset &#39;reviews&#39; For the median: median_points = reviews.points.median() # calculate the median of the column &#39;points&#39; within the ddset &#39;reviews&#39; 11.7.12 Transform a column based on conditions with numpy: import numpy as np # In order to be able to perform a transformation, we will need the `numpy`-package # set-up: x = np.arange(10) # this is our column &#39;x&#39; condlist = [x&lt;3, x&gt;5] # set of conditions, that need to be fullfilled --&gt; which are the value-ranges, on which you will apply # a custom-function [which will be defined next]? --&gt; all numbers below 3 AND all numbers above 5 choicelist = [x, x**2] # the custom-function you will apply here: x^2 # output: np.select(condlist, choicelist, default=np.nan) # apply x^2 on: x &lt; 3 AND x &gt; 5 array([ 0., 1., 2., nan, nan, nan, 36., 49., 64., 81.]) 11.7.13 Transform a Pandas Series into a DataFrame: import pandas as pd s = pd.Series([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], name=&quot;vals&quot;) s.to_frame() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } vals 0 a 1 b 2 c 11.7.14 Assign a unique ID: df[&#39;id&#39;] = df.groupby([&#39;LastName&#39;,&#39;FirstName&#39;]).ngroup() # here, we use the column &#39;LastName&#39; &amp; &#39;FirstName&#39; together, to create # a unique ID. # Quelle: https://stackoverflow.com/questions/45685254/q-pandas-how-to-efficiently-assign-unique-id-to-individuals-with-multiple-ent df[&#39;id&#39;] = df.groupby([&#39;date&#39;]).ngroup() # if Course, you could also simply use 1 column fo the assignment of an unique ID. 11.8 ## Selection &amp; Filtering Rows and Columns When you will work wit dataframes in pandas, one of the most important things you will need to master is how to select some columns, as well as print out subsets // particular columns from the dataframe. To get started and become acquainted with common techniques, I recommend you to watch this beginner tutorial for filtering &amp; selecting columns. Load the next cell to be able to run the examples that follow: import pandas as pd # load package needed file_path = &#39;./data/filter-and-selection/sample_orders.csv&#39; # type in the path-location of your data dd = pd.read_csv(file_path) # load the data dd.head() # check if it worked? --&gt; yes! --&gt; should print the first 5 rows of your ddset .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order_id order_date customer_id items_ordered order_total 0 1000 12/1/17 1 3 65.00 1 1001 12/1/17 3 2 25.75 2 1002 12/1/17 6 1 20.75 3 1003 12/1/17 8 6 86.00 4 1004 12/1/17 8 1 30.00 dd.info() # check also all data-types &lt;class ‘pandas.core.frame.DataFrame’&gt; RangeIndex: 16 entries, 0 to 15 Data columns (total 5 columns): # Column Non-Null Count Dtype — —— ————– —– 0 order_id 16 non-null int64 1 order_date 16 non-null object 2 customer_id 16 non-null int64 3 items_ordered 16 non-null int64 4 order_total 16 non-null float64 dtypes: float64(1), int64(3), object(1) memory usage: 768.0+ bytes 11.8.1 Select only 1 single column of your Dataframe: In this example, I will select the column order_id from my ddset dd: print( dd[&#39;order_id&#39;] # take the dataframe &#39;dd&#39; and print me only the column called &#39;order_id&#39; --&gt; this is a subset ) 0 1000 1 1001 2 1002 3 1003 4 1004 5 1005 6 1006 7 1007 8 1008 9 1009 10 1010 11 1011 12 1012 13 1013 14 1014 15 1015 Name: order_id, dtype: int64 11.8.1.1 Möglichkeit 2: Selection of columns with dot-notation Nachteil dieser Methode ist, dass sie nicht funktioniert, wenn es in den Columns Leerschläge gibt! test = dd.order_id # ACHTUNG: funktioniert nicht, wenn es Leerschläge gibt!! print(test) 0 1000 1 1001 2 1002 3 1003 4 1004 5 1005 6 1006 7 1007 8 1008 9 1009 10 1010 11 1011 12 1012 13 1013 14 1014 15 1015 Name: order_id, dtype: int64 11.8.2 Select multiple Columns from your Dataframe: Similarly to R, we need to pass a “set” into the selector of columns. print( dd[[&#39;order_id&#39;, &#39;order_total&#39;]] # take the dataframe &#39;dd&#39; and print me only the columns called &#39;order_id&#39; &amp;&amp; &#39;order_total&#39; ) order_id order_total 0 1000 65.00 1 1001 25.75 2 1002 20.75 3 1003 86.00 4 1004 30.00 5 1005 20.00 6 1006 21.00 7 1007 26.25 8 1008 20.75 9 1009 15.75 10 1010 45.00 11 1011 30.00 12 1012 15.00 13 1013 85.00 14 1014 25.00 15 1015 15.00 11.8.3 Select AND filter for a particular row, with one condition: Let’s say, we want to select the row // observation with the \"order_id\" == 1004. print( dd[ dd[&#39;order_id&#39;] == 1004 # note that this INNER bracket will run a function that searches through and would only print # a Boolean-List of &quot;True&quot; or &quot;False&quot; for all rows --&gt; example-video: ab 2:36-3:34 --&gt; https://www.youtube.com/watch?v=htyWDxKVttE ] # This outer selector will tell Python: &quot;Select&quot; only the row where the column &#39;order_id&#39; == 1004 ) # short-version: print(dd[dd[&#39;order_id&#39;] == 1004]) order_id order_date customer_id items_ordered order_total 4 1004 12/1/17 8 1 30.0 order_id order_date customer_id items_ordered order_total 4 1004 12/1/17 8 1 30.0 Quelle: Basics of selecting and filtering rows and columns in Python, ab 2:36-3:34 11.8.4 Select AND filter for rows with more than one condition: Now, we want to select the rows // observations, where \"order_total\" &gt;= 50.00 AND \"order_date\" == 12/1/17 are fulfilled // filtered. Basically, it is the same as with the selection &amp; filtering with one condition, you just need to wrap up the INNER brackets with an additional (...) brackets (for each condition!). print( dd[ (dd[&#39;order_total&#39;] &gt;= 50) &amp; (dd[&#39;order_date&#39;] == &#39;12/1/17&#39;) # in contrast to **one condition**, we just wrap up a (...) for each condition ] # This outer selector will tell Python: &quot;Select&quot; only the row where the column &#39;order_id&#39; == 1004 ) order_id order_date customer_id items_ordered order_total 0 1000 12/1/17 1 3 65.0 3 1003 12/1/17 8 6 86.0 Alternative mit “OR”: Statt AND kann man auch - zum Beispiel - die OR-Condition verwenden print(dd[(dd[&#39;order_total&#39;] &gt;= 50) | (dd[&#39;order_date&#39;] == &#39;12/1/17&#39;)]) order_id order_date customer_id items_ordered order_total 0 1000 12/1/17 1 3 65.00 1 1001 12/1/17 3 2 25.75 2 1002 12/1/17 6 1 20.75 3 1003 12/1/17 8 6 86.00 4 1004 12/1/17 8 1 30.00 13 1013 12/3/17 7 5 85.00 11.8.5 Select only rows with Missing-Values null_data = df[df.isnull().any(axis = 1)] # this will only select the rows that contain at least one missing-value null_data # check, if it worked? 11.9 ## Selection with “accessor operators” iloc &amp; loc Das Package Pandas erlaubt die Verwendung von sogenannten accessor operators, um Dataframes einfacher zu filtrieren. Dabei unterscheidet man zwischen: iloc (= based on the Postition (= Nummer) des Index innerhalb der Spalten &amp; Reihen im ddset), Beachte: Reihenfolge der inputs within iloc == 1) ‘rows’, then 2) ‘columns’ loc (= based on the Index-Namen of the Spalten &amp; Zeilen im ddset). Beachte: Reihenfolge der inputs within loc == 1) ‘rows’, then 2) ‘columns’ 11.9.1 Selection of the entry situated in the 1st row and 5th column only with iloc To be able to use iloc, the key is to know the position of the row- &amp; column-labels (= index of columns &amp; rows). Note: iloc &amp; loc are often used to print a particular entry WITHIN a dataframe. However, loc and iloc also are able to print entire rows // observations and not just one specific value within a row, as we will see. test = dd.iloc[0,4] # Reihenfolge der inputs == 1) &#39;rows&#39; (--&gt; &quot;0&quot; == 1st row), then 2) &#39;columns&#39; (--&gt; &quot;4&quot; == 5th column) print(test) # check if it worked? --&gt; should print the value &#39;65&#39; 65.0 11.9.2 Selection of the entry situated in the 1st row and 5th column only with loc The key to use loc is that you know the names of the columns &amp; rows. Wichtige Bemerkung: Der Default-Name (= Standard-Name) der rows innerhalb eines Dataframes ist einfach die Zahlenabfolge von 0,1,2...,10,11,12,.... Lustigerweise ist der row-label // index von rows auch standardmässig die Zahlenabfolge 0,1,2,...,10,11,12,.... Deshalb kommt es sehr oft vor, dass loc und iloc denselben ersten Input haben! xD test = dd.loc[0,&#39;order_total&#39;] # Reihenfolge der inputs == 1) &#39;rows&#39; (--&gt; &quot;0&quot; == NAME der 1st row), then 2) &#39;columns&#39; (--&gt; &quot;order_total&quot; == NAME der gewünschten 5th column) print(test) # check if it worked? --&gt; should print the value &#39;65&#39; 65.0 11.9.3 Apply a Threshold-Filter using loc on each row: This can be useful, when you want to replace some values in certain columns → see ‘Outliers’-chapter # Only display all rows, where the &#39;pressure&#39;-column is &gt; than the threshold of 1051 bar df_weather.loc[df_weather.pressure &gt; 1051, &#39;pressure&#39;] 11.9.4 Selection of an ENTIRE row // record // observation from a ddset with iloc: ##### ## Möglichkeit 1: selection of only 1 row test_row1 = dd.iloc[0,] # Important: 1) rows, then 2) columns --&gt; we want the entire 1st row, which includes ALL columns # Note: das Komma NACH dem &quot;0&quot; zeigt an, dass wir ALLE columns selektieren wollen! ##### ## Möglichkeit 2: selection of &gt; 1 row --&gt; notice the additional wrap with [...] WITHIN iloc[]! test_multiRow = dd.iloc[[0,1,2,3,5,8],] # &#39;[0,1,2,3,5,8]&#39; will select the &#39;1st, 2nd, 3rd, 4th, 6th and 9th&#39; row # while also selecting ALL columns simultaneously ##### ## check if it worked? --&gt; yes! print(test_row1) # should print only 1 row BUT with ALL the column --&gt; weird output, because the columns sind abgebildet als rows xD print(test_multiRow) order_id 1000 order_date 12/1/17 customer_id 1 items_ordered 3 order_total 65 Name: 0, dtype: object order_id order_date customer_id items_ordered order_total 0 1000 12/1/17 1 3 65.00 1 1001 12/1/17 3 2 25.75 2 1002 12/1/17 6 1 20.75 3 1003 12/1/17 8 6 86.00 5 1005 12/2/17 1 1 20.00 8 1008 12/2/17 9 1 20.75 11.9.5 Selection of an ENTIRE row // record // observation from a ddset with loc: Tipp: Von der Eleganz &amp; Effizienz her, empfehle ich dir undbedingt Möglichkeit 3 im unteren Code-Beispiel!! ##### ## Möglichkeit 1: selection of only 1 row test_row1 = dd.loc[0,] # das Komma NACH dem &quot;0&quot; zeigt an, dass wir ALLE columns selektieren wollen! ##### ## Möglichkeit 2: selection of &gt; 1 row --&gt; notice the additional wrap [...] WITHIN loc[]! test_multiRow = dd.loc[[0,1,2,3,5,8],] # Weil - per default - die &#39;row-labels&#39; (= name des Indexes # der Zeilen) dieselben sind, wie die Position, ist der Code # für &#39;loc&#39; derselbe, wie für &#39;iloc&#39; hier... ##### ## Möglichkeit 3: Beste &amp; schönste Solution (meiner Meinung nach!) rows = list(range(0,16)) # will create a list that goes from 0 to 99 --&gt; this will be for the row-labels columns = [&#39;order_id&#39;, &#39;order_date&#39;, &#39;order_total&#39;] # this will be for the column-labels # Pro-Tipp: columns = list(data.columns) df = dd.loc[rows, columns] ##### ## check if it worked? --&gt; yes! print(test_row1) # should print only 1 row BUT with ALL the column --&gt; weird output, because the columns sind abgebildet als rows xD print(test_multiRow) print(df) order_id 1000 order_date 12/1/17 customer_id 1 items_ordered 3 order_total 65 Name: 0, dtype: object order_id order_date customer_id items_ordered order_total 0 1000 12/1/17 1 3 65.00 1 1001 12/1/17 3 2 25.75 2 1002 12/1/17 6 1 20.75 3 1003 12/1/17 8 6 86.00 5 1005 12/2/17 1 1 20.00 8 1008 12/2/17 9 1 20.75 order_id order_date order_total 0 1000 12/1/17 65.00 1 1001 12/1/17 25.75 2 1002 12/1/17 20.75 3 1003 12/1/17 86.00 4 1004 12/1/17 30.00 5 1005 12/2/17 20.00 6 1006 12/2/17 21.00 7 1007 12/2/17 26.25 8 1008 12/2/17 20.75 9 1009 12/3/17 15.75 10 1010 12/3/17 45.00 11 1011 12/3/17 30.00 12 1012 12/3/17 15.00 13 1013 12/3/17 85.00 14 1014 12/3/17 25.00 15 1015 12/3/17 15.00 11.9.6 Replace values within a Column with some new Values: df.loc[df.ungewichtet &gt; 1, &#39;ungewichtet&#39;] = 1 # hier werde ich alle Werte der Spalte &quot;ungewichtet&quot; &gt; 1 mit dem Wert &quot;1&quot; ersetzen! 11.9.7 Different Slicing when using iloc VS. loc: When using iloc, the range 0:5 will select entries 0,...,4 that is: it indexes EXCLUSIVELY. On the other hand, loc, meanwhile, indexes INCLUSIVELY. So the same range 0:5 will select entries 0,...,5!!! Hence, if you want the SAME output with loc and iloc, you simply need to slightly change the range()-function. Example: ## Möglichkeit 1: with &#39;iloc&#39; iloc_test = dd.iloc[0:5,0] # row-position == 0:5 --&gt; first 5 rows; EXCLUDES &#39;5&#39; from the range &quot;0,1,2,3,4,5&quot; # --&gt; hence range(0:5) results in --&gt; &quot;0,1,2,3,4&quot; # column-position == 0 --&gt; 1st row --&gt; remember: indexing in # Python starts at &#39;0&#39;! # IMPORTANT: &#39;iloc&#39; uses the &#39;Python stdlib&#39; indexing scheme, where the first element of the range is # included and the last one excluded. So 0:5 will select entries 0,...,4 (= these are the first *5* # entries!!). ## Möglichkeit 2: to get the SAME output with &#39;loc&#39;, we need a slightly DIFFERENT range! loc_test = dd.loc[0:4,&#39;order_id&#39;] # row-position == 0:4 --&gt; first 5 rows; INCLUDES &#39;4&#39; # --&gt; hence range(0:4) results in --&gt; &quot;0,1,2,3,4&quot; ## check if the output are the same, even though &quot;range()&quot; has slightly different inputs? --&gt; yes! print(iloc_test) print(loc_test) 0 1000 1 1001 2 1002 3 1003 4 1004 Name: order_id, dtype: int64 0 1000 1 1001 2 1002 3 1003 4 1004 Name: order_id, dtype: int64 11.10 ## Sorting &amp; Filtering 11.10.1 How to sort a Data-Frame Column (hier: Pearson-Correlation Matrix) from ‘most important’ to ‘least important’?: ### Find the correlations&#39; ranking for the day-ahead electricity price and the rest of the features: # Step 1: Create a Pearson-Korrelation Matrix out of your dataframe: correlations = df.corr(method=&#39;pearson&#39;) # the variable &#39;correlations&#39; is a dataframe! # Step 2: use &#39;sort_values&#39; to sort the column from &quot;most important&quot; (highest value) to &quot;least important&quot;: print(correlations[&#39;pricesCH&#39;].sort_values(ascending=False).to_string()) 11.10.2 Filter the WHOLE Dataframe after a condition (hier: Correlations &gt; 0.75), select ONLY all observations that fullfill the condition (= ‘stack them’) and sort them left in your Data Frame: Dieser Code beruht auf den Abschnitt: How to sort a Data-Frame Column (hier: Pearson-Correlation Matrix) from ‘most imporant’ to ‘least important’. highly_correlated = correlations[correlations &gt; 0.75] print(highly_correlated[highly_correlated &lt; 1.0].stack().to_string()) 11.11 ## Data-Cleaning: Missings 11.11.1 Number of Missing Values across the WHOLE Data-Frame: print(&#39;There are {} missing values or NaNs in df_energy.&#39; .format(df_energy.isnull().values.sum())) 11.11.2 Count the TOTAL number of Missings in each column: df_energy.isnull().sum(axis=0) # outputs the number of NaNs for each column 11.11.3 Count the non-missings (and non-zero values) in each column: Achtung: the “Code-Trick” below only works, if your columns don’t contain values, that are ‘0’! This is because the number ‘0’ - as a boolean - will be printed out to False, and hence, we will get the “wrong” number of missing values. Verwende den unteren Code also nur, wenn du zuerst abgecheckt hast, dass du keine 0 in den einzlenen Spalten hast! # Display the number of non-missing values in each column print(&#39;Non-zero values in each column:\\n&#39;, df_energy.astype(bool).sum(axis=0), sep=&#39;\\n&#39;) Since the above cell only gives out the “correct” number of non-missing values, if you have no 0 in your columns, here is code to count how many 0 you have in each column: (df_energy == 0).astype(int).sum(axis=0) # count the numbers of &#39;0s&#39; in each column [axis = 0, for columns...] 11.11.4 Display each row with a Missing-Value: # Display the rows with null // missing values: df_energy[df_energy.isnull().any(axis=1)].tail() 11.11.5 Replace all Missings in a Column with 0s: df[&#39;ColumnWithMissings&#39;] = df_tot[&#39;ColumnWithMissings&#39;].fillna(0) # replaces all missing-values within the column with 0s. 11.11.6 Replace all Values in a Column with Missings: Assume that we have a time-series that has a row-index with time-stamps! ### Step 1: define the range in which you want to replace values start = &#39;2020-01-01&#39; # 01. Januar 2020 --&gt; ab hier wollen wir die values der Time-Series mit Missings ersetzen stop = &#39;2020-08-01&#39; # 01. August 2020 --&gt; bis zu diesem Datum sollen die Missings eingefügt werden ### Step 2: replace the values with missings via the &quot;.loc[row-indexer, column-indexer]&quot; df.loc[start:stop, &#39;y_hat&#39;] = None # This will replace all the values within the &#39;y_hat&#39;-column - in the range from # 01.01.2020-01.08.2020 with Missing-values (instead of &quot;normal&quot;-values) 11.12 ## Missing Imputation This list will grow with time, the more I stumble onto various codes: 11.12.1 Missing Interpolation: 11.12.1.1 For a Time Series: # Fill null values using interpolation: df_energy.interpolate(method=&#39;linear&#39;, limit_direction=&#39;forward&#39;, inplace=True, axis=0) # since we have 11.13 ## Data-Cleaning: Duplicates 11.13.1 Number of Duplicates across the WHOLE Data-Frame: temp_energy = df_energy.duplicated(keep=&#39;first&#39;).sum() print(&#39;There are {} duplicate rows in df_energy based on all columns.&#39; .format(temp_energy)) 11.13.2 Drop Duplicate values: # Variante 1: mit reset_index &amp; neuer set_index df_weather_2 = df_weather.reset_index().drop_duplicates(subset=[&#39;time&#39;, &#39;city_name&#39;], # Drop the duplicate, if all the rows are the same // have the same values (Achtung: we only look at the duplicates in the &#39;time&#39; &amp; &#39;city_name&#39;-column from this analysis!). keep=&#39;last&#39;).set_index(&#39;time&#39;) # if you have duplicate, keep only the last of the duplicated-rows. # Variante 2: man dropt den &quot;alten&quot; Index (VOR merge) und neuem set_index df_unique_dates = df_ferien.drop_duplicates(subset=&#39;datum&#39;, # only consider the column &quot;datum&quot; [= column that has duplicates] when dropping the duplicates keep=&#39;first&#39;).reset_index(drop=True) # reset the index, otherwise you get weird indizes (mit 10&#39;000 für manche) # &#39;drop = True&#39; means that we do not keep the &quot;old&quot; index as a separate &#39;column&#39; df_unique_dates 11.14 ## Data-Cleaning: Outliers 11.14.1 Draw a Boxplot for a specific column: import seaborn as sns sns.boxplot(x=df_weather[&#39;pressure&#39;]) plt.show() Key-Question: Are there Outliers? → Yes / No “Trick” to answer the question: If you deal with temperature for example, google for “highest Temperature on earth” and look it up in Wikipedia to dertermine whether your value is an outlier. Example: Even a pressure of approximately 100,000 HPa or 10 MPa, which is clearly visible in the above figure, corresponds to a quantity greater than the atmospheric pressure of Venus. In order to be sure, we will set as NaNs every value in the pressure-column which is higher than 1051 hPa, which is just above the highest air pressure ever recorded in the Iberian peninsula. While outliers on the low side are not visible in the boxplot above, it is a good idea to also replace the values which are lower than 931 hPa, i.e. the lowest air pressure ever recorded in the Iberian peninsula. Step 2: If the answer to the above question is ‘yes’, then set the value above values to NaNs, which are above a certain “unprobable” threshold. # Replace outliers in the `Set_Name_of_Column_with_the_Outlier_hier`-column with `NaN`s df_weather.loc[df_weather.pressure &gt; 1051, &#39;pressure&#39;] = np.nan df_weather.loc[df_weather.pressure &lt; 931, &#39;pressure&#39;] = np.nan 11.15 ## Merging &amp; Splitting Important note before you start: If you want to merge 2 dataframes, where one is smaller than the other, then you CANNOT make the bigger dataset become smaller with merge(). I did spend alot of time, but without success. However, there is another solution: you just need to drop the duplicates of the bigger dataframe (see chapter Duplicates in order to be able to make the dataset smaller! =) 11.15.1 Splitting a Dataset into smaller parts, by using categories to split it up: # Split the df_weather into 5 dataframes (one for each of the 5 cities): df_1, df_2, df_3, df_4, df_5 = [x for _, x in df_weather.groupby(&#39;city_name&#39;)] dfs = [df_1, df_2, df_3, df_4, df_5] str(planets).replace(&quot;&#39;&quot;, &quot;&quot;) ‘[Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune]’ 11.15.2 Merge 2 separate Data Sets together via their row-index (= row-label): Damit du dieses Merging erfolgreich durchführen kannst, müssen beide Datensätze denselben Zeilen-Index besitzen. Ziel: Dadurch kannst du zum Beispiel neue Spalten von einem anderen Datensatz hinzufügen (passiert sehr oft in der Praxis). # Let&#39;s merge all the y- &amp; X-Variables from the training-set together: test = pd.merge( y_train, # der Trainingsdatensatz für die y-Variable x_train, # der Trainingsdatensatz für alle X-Variablen how=&quot;outer&quot;, left_on=y_train.index, # merging via index // row-label des DataFrames der y-Variable right_on=x_train.index, # merging via index // row-label des DataFrames der x-Variablen ).set_index(&#39;key_0&#39;) # optional: da wir hier eine Zeitreihe haben, dessen Row-Index den Column-Name &#39;key_0&#39; animmt beim Merging, wird hier als neuer Row-Index für den gemerged Dataframe gesetzt test # check if it worked 11.15.3 Merge different datasets simultaneously together: Ausgangssituation: Assume that - initially - we have 2 datasets, 1 is for the weather and 1 is for the Energy-Prices. Furthermore, they those two datasets are time-series. Hence, they have the same time-index (= row-label), formatted in UTC. Step 1: Split up the weather-dataset, sorted by cities # Split the df_weather into 5 dataframes (one for each of the 5 cities): df_1, df_2, df_3, df_4, df_5 = [x for _, x in df_weather.groupby(&#39;city_name&#39;)] dfs = [df_1, df_2, df_3, df_4, df_5] Step 2: Merge the 5 sub-datasets with the Energy-Price Dataset # Step 1: save a copy, in case you do a wrong merging! df_final = df_energy # Step 2: make a for-loop, to merge all the 6 datasets simultaneously for df in dfs: # here, we loop through every city-group of our list of data frames (see step 1) city = df[&#39;city_name&#39;].unique() # we store the names of the 5 cities - as a list - in a variable city_str = str(city).replace(&quot;&#39;&quot;, &quot;&quot;).replace(&#39;[&#39;, &#39;&#39;).replace(&#39;]&#39;, &#39;&#39;).replace(&#39; &#39;, &#39;&#39;) # we perform some string-manipulation to eliminate all the characters that are not necessary df = df.add_suffix(&#39;_{}&#39;.format(city_str)) # we re-name the columns, by adding the name of the city to each column df_final = df_final.merge(df, # this is the merging-part! on=[&#39;time&#39;], # we want to merge via the index // row-label of both datasets --&gt; since they are both in UTC-time, this will work! how=&#39;outer&#39;) # &#39;outer&#39; means: we want the &#39;union&#39; --&gt; see this youtube-video for a good explanation: https://www.youtube.com/watch?v=h4hOPGo4UVU df_final = df_final.drop(&#39;city_name_{}&#39;.format(city_str), axis=1) # let&#39;s drop some columns that we don&#39;t need anymore # Step 3: &quot;final results&quot;-check df_final.columns # show the merging-results, by displaying all the column-names --&gt; DONE! =) Step 3: Make some final-checks Key-Question: Did the merging really worked? → Yes / No “Trick” to answer the question: Look at Missings &amp; Duplicates. # Display the number of NaNs and duplicates in the final dataframe print(&#39;There are {} missing values or NaNs in df_final.&#39; .format(df_final.isnull().values.sum())) temp_final = df_final.duplicated(keep=&#39;first&#39;).sum() print(&#39;\\nThere are {} duplicate rows in df_energy based on all columns.&#39; .format(temp_final)) If the answer is ‘yes’, then merging should have worked, if you have no missings and no duplicates. If the answer to the above question is ‘no’, then you need to go back to step 2 and try to figure out what you did wrong when merging the datasets together. 11.16 ## Advanced Exploration of your data 11.16.1 How to create &amp; plot a Pearson Correlation Matrix out of a Dataframe?: # Step 1: construct a &#39;Pearson Correlation Matrix&#39; as a Dataframe: correlations = df.corr(method=&#39;pearson&#39;) # Step 2: Load Libraries &amp; plot Pearson correlation matrix: import matplotlib.pyplot as plt import seaborn as sns fig = plt.figure(figsize=(24, 24)) # sns.heatmap(correlations, annot=True, fmt=&#39;.2f&#39;) # import seaborn as sns plt.title(&#39;Pearson Correlation Matrix&#39;) plt.show() 11.17 ## Time-Series: Exploration &amp; Visualizations 11.17.1 Plotting a Time Series: Step 1: Transform the date-column into a date-time-column. from datetime import datetime as dt # this is the package we need to convert a column into a &#39;date-time&#39; # Step 1: Define a function that is reading the date-column correctly def parse_date(date): data=str(date) if date==&quot;&quot;: # this condition will never be true, since the column &#39;Date-Time&#39; NEVER has an empty string; ### raise exception return None else: return pd.to_datetime(date, format=&#39;%Y-%m-%d %H:%M:%S&#39;, yearfirst = True, utc = True) # Step 2: apply the above function on the Column &#39;Date-Time&#39; to transform the column into a &#39;date-time&#39;-type raw_dd[&quot;Date-Time&quot;] = raw_dd[&quot;Date-Time&quot;].apply(parse_date) Step 2: Set the ‘time’-column as the index (= row-label), otherwise the plotting won’t work # set the date-column as our row-label: raw_dd = raw_dd.set_index(&quot;Date-Time&quot;) Step 3: Visualize the Time-Series from sktime.utils.plotting import plot_series # use sktime and the &#39;plot_series&#39;-module for this task # Define our y-variable (= DA-Prices in CH): y = raw_dd[&quot;pricesCH&quot;] plot_series(y) # visual check if it worked? --&gt; yes! 11.17.2 Plot Auto-Correlation &amp; Partial-Auto-Correlation Functions: The partial autocorrelation plot of the eletricity price time series shows that the direct relationship between an observation at a given hour (t) is strongest with the observations at t-1, t-2, t-24 and t-25 time-steps and diminishes afterwards. Thus, we are going to use the 25 previous values of each time series which will constitute a feature for our models. # Step 1: load library from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # to plot # Step 2: Plot autocorrelation and partial autocorrelation plots fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 6)) plot_acf(df_energy[&#39;pricesCH&#39;], lags=50, ax=ax1) plot_pacf(df_energy[&#39;pricesCH&#39;], lags=50, ax=ax2) plt.tight_layout() plt.show() 11.17.3 Plot the Cross-Correlation(Yt, Xt - k): It would quite definitely be more beneficial if we only chose to use specific past values (observations at certain time-lags) of a given feature, based on the cross-correlation between the electricity price and each one of the features in the dataset. For example, below we can see the cross-correlation between the electricity price and the Renewable Generation from Switzerland. We see that there are many time-lags with a correlation which is close to zero and could be ommited. from statsmodels.tsa.stattools import ccf # to plot the cross-correlation function # Step 2: plot the cross-correlation between the day-ahead price &quot;today&quot; and with each of the first 50-lags of the column &#39;RenGen&#39; cross_corr = ccf(df_energy[&#39;RenGenCH&#39;], df_energy[&#39;pricesCH&#39;]) plt.plot(cross_corr[0:50]) plt.show() 11.18 ## Feature Engineering for Time-Series Wenn du mit Variablen arbeitest, welche mit “Zeit” zu tun haben, brauchst du einige “Tricks”, um Data Cleaning betreiben zu können mit sogenannten Date-Time Objects in Python. 11.18.1 Creating an index for Time-Series: start = &#39;2020-01-01&#39; stop = &#39;2021-01-01&#39; # ACHTUNG: wird am 31.12.2020 enden ts_index = pd.date_range(start, stop, freq = &#39;h&#39;, closed = &#39;left&#39;, tz = &#39;UTC&#39;) ts_index DatetimeIndex([‘2020-01-01 00:00:00+00:00’, ‘2020-01-01 01:00:00+00:00’, ‘2020-01-01 02:00:00+00:00’, ‘2020-01-01 03:00:00+00:00’, ‘2020-01-01 04:00:00+00:00’, ‘2020-01-01 05:00:00+00:00’, ‘2020-01-01 06:00:00+00:00’, ‘2020-01-01 07:00:00+00:00’, ‘2020-01-01 08:00:00+00:00’, ‘2020-01-01 09:00:00+00:00’, … ‘2020-12-31 14:00:00+00:00’, ‘2020-12-31 15:00:00+00:00’, ‘2020-12-31 16:00:00+00:00’, ‘2020-12-31 17:00:00+00:00’, ‘2020-12-31 18:00:00+00:00’, ‘2020-12-31 19:00:00+00:00’, ‘2020-12-31 20:00:00+00:00’, ‘2020-12-31 21:00:00+00:00’, ‘2020-12-31 22:00:00+00:00’, ‘2020-12-31 23:00:00+00:00’], dtype=‘datetime64[ns, UTC]’, length=8784, freq=‘H’) 11.18.1.1 (Optionale) Erweiterung: Index-Slicing Der Nachteil am obigen Index ist, dass er immer erst ab 0:00 Uhr beginnt. Das Problem dabei ist, dass du nicht immer einen Index benötigen wirst, welcher punktgenau um 0:00 Uhr beginnt, sondern vielleicht einmal um 23:00 Uhr. Lösung: Verwende Slicing, um ab der ersten Beobachtung vom Jahr 2020, um 00:00 zu starten, beispielsweise so: DF2 = DF1.loc[:'2019-05-26 13:00:00+00:00'] für einen DataFrame, beziehungsweise so: ts_index[4:] für eine Pandas-Series. 11.18.2 Make a copy of the index as a Column of a DataFrame: p_train = p_train.reset_index() p_train[&#39;date&#39;] = p_train[&#39;Date-Time&#39;] p_train.set_index(&#39;Date-Time&#39;) 11.18.3 Converting a Time-Column into a Datetime-Column and set it as an Index (= row label): If you work with time-series, it will oftentimes be the case that you will need to convert your date-column - which oftentimes are strings - into an actual date-type column! df_energy[&#39;Date-Time&#39;] = pd.to_datetime(df_energy[&#39;Date-Time&#39;], utc=True, infer_datetime_format=True) df_energy = df_energy.set_index(&#39;Date-Time&#39;) # set the &#39;Date-Time&#39;-Column as the index // row-label of our data-frame 11.18.3.0.1 If you want to set the format by yourself: #convert Delivery day to a date time column epex_df[&#39;Delivery day&#39;] = pd.to_datetime(epex_df[&#39;Delivery day&#39;], format = &#39;%Y%m%d&#39;) 11.18.4 Creating a range of dates in Python: Specify start and end, with the default daily frequency. import pandas as pd pd.date_range(start=&#39;1/1/2018&#39;, end=&#39;1/08/2018&#39;) DatetimeIndex([‘2018-01-01’, ‘2018-01-02’, ‘2018-01-03’, ‘2018-01-04’, ‘2018-01-05’, ‘2018-01-06’, ‘2018-01-07’, ‘2018-01-08’], dtype=‘datetime64[ns]’, freq=‘D’) 11.19 ## Rechnen mit Date-Time Objects: Addition und Subtraktion von Zeiten Oftmals wirst du Zeitpunkte in der Zukunft oder Vergangenheit definieren müssen. Deshalb ist es essentiell, dass du lernst, wie man mit Date-Time Objekten rechnet: 11.19.1 Finde den genauen Zeitpunkt “jetzt” // “heute” heraus: import datetime # Da wir mit &quot;Zeit&quot; arbeiten, brauche ich das Package &quot;datetime&quot; jetzt = datetime.datetime.now() # current time print(jetzt) # check: should output the current time 2021-07-01 13:54:53.425398 11.19.2 Zukunftszeitpunkt mit dem “jetzt” als Referenzpunkt // Startpunkt: import datetime # Da wir mit &quot;Zeit&quot; arbeiten, brauche ich das Package &quot;datetime&quot; jetzt = datetime.datetime.now() # current time added_time = datetime.timedelta(hours= 1, minutes= 1) # anzahl an Stunden &amp; Minuten, die du dazu addieren willst ende = jetzt + added_time print(ende) # check: should output &quot;current time + 5h 2 min&quot; 2021-07-01 15:00:59.045610 11.19.3 Rechnen mit Datum: Subtraktion mit fixed time-point als Referenzpunkt // Startpunkt: import datetime # Da wir mit &quot;Zeit&quot; arbeiten, brauche ich das Package &quot;datetime&quot; fixe_zeit = datetime.datetime(2021, 7, 1, 8, 12) # datetime(year, month, day, hour, minute, second, microsecond), # wobei die ersten 3 Argumente (Jahr, Monat, Tag) OBLIGATORISCH sind! --&gt; hier interessiert uns die letzten 2 Inputs, # nämlich die &quot;8&quot; und die &quot;12&quot; --&gt; diese wiederspiegeln meine fixe 8h 12 min Arbeitszeit bei der SBB added_time = datetime.timedelta(hours= 1, minutes= 1) # anzahl an Stunden, die du noch arbeiten musst wie_lange_noch = fixe_arbeitszeit - added_time print(wie_lange_noch.strftime(&quot;%H%M&quot;)) # check: should output 7h 11min --&gt; yes! 0711 11.19.4 Day of the Week given a Date: If you want to know - for example - which “Wochentag” z.B. the 21.03.2020 was, simply use this code: import datetime fixe_zeit = datetime.datetime(2020, 12, 12) # datetime(year, month, day, hour, minute, second, microsecond) fixe_zeit.weekday() # --- Gemäss Documentation # 0 = Monday # 1 = Tuesday # 2 = Wednesday # 3 = Thursday # 4 = Friday # 5 = Saturday # 6 = Sunday 5 addierte_anzahl_tage = datetime.timedelta(days = 28) next_day = fixe_zeit + addierte_anzahl_tage next_next_day = next_day + addierte_anzahl_tage next_next_next_day = next_next_day + addierte_anzahl_tage next_next_next_next_day = next_next_next_day + addierte_anzahl_tage next_next_next_next_day datetime.datetime(2021, 3, 22, 0, 0) We see that the 21.03.2020 was a Saturday. I checked it with the calendar: it’s correct =) 11.20 ## Dummy-Variables 11.20.1 Create a “normal” Dummy: In Python, you need the where-method to create a dummy on a single condition. This is equivalent to the ifelse()-Function in R: import numpy as np np.where(maupayment[&#39;log_month&#39;] == maupayment[&#39;install_month&#39;], &#39;install&#39;, &#39;existing&#39;) 11.20.2 Create a Dummy by merging &amp; flagging: This can be useful, when you need to track which of the variable was part of the “left” and which one was part of the “right” dataset. You literally create a dummy-variable when you do this and it is called the _merge-column. Andwendungsfall 1: als ich bei der SBB versucht habe, einen “Ferien-Dummy” zu kreieren. df2 = df.merge(df_year_2019, on=&#39;date&#39;, # in this case, we merge over the &quot;date&quot;-column; note that &#39;df&#39; has 365 rows, while &#39;df_year_2019&#39; only 270 rows how=&#39;outer&#39;, # we need to set indicator=True) # the flag is &quot;indicator&quot; df2 # we get a ddset with 360 rows. On the outside, the &#39;testo&#39;-ddset seems NOT to be different form the &#39;df&#39;, however, the purpose of the merge was to create a new dummy-variable to see from which dataset it came from # step 2: we want some more meaningful values within the &#39;merge&#39;-column --&gt; apply an if-statement onto the (string-) column df2.loc[df2[&#39;_merge&#39;] == &#39;both&#39;, &#39;ferien&#39;] = 1 # Here, we create a column &#39;Ferien&#39;, which will be equal to &quot;1&quot;, if the value in the &#39;_merge&#39;-column is equal to the string &quot;both&quot; df2.loc[df2[&#39;_merge&#39;] != &#39;both&#39;, &#39;ferien&#39;] = &#39;False&#39; # Ferien-Dummy equals zero otherwise 11.21 ## Dummy-Variables: Create Columns for specific hours, seasons of the year etc… In the following, I will use a column called Date-Time to create new columns to extract some ‘time-information’ (hours, quarters etc…) needed. 11.21.1 Extract hours and put it into a separate column: raw_dd[&#39;Hour&#39;] = raw_dd[&#39;Date-Time&#39;].apply(lambda x: x.hour) # Werte von 0 [= Stunde 0:00-1:00] bis 23 [= Stunde 23:00-24:00] 11.21.2 Extract Seasons (= “qarters”) and put it into a separate column: raw_dd[&#39;Quarter&#39;] = raw_dd[&#39;Date-Time&#39;].apply(lambda x: x.quarter) # Werte von 1 [Januar bis März] bis 4 [Oktober bis Dezember] 11.21.3 Extract Days and put it into a separate column: raw_dd[&#39;day_of_week&#39;] = raw_dd[&#39;Date-Time&#39;].apply(lambda x: x.weekday()) # Werte von 0 [= Montag] bis 6 [= Sonntag] 11.22 ## Scaling Variables When applying machine learning, one condition to use those fancy models will be to scale our variables first. The main methods for scaling can be seen in this Stack-Exchange Post. 11.22.1 Normalization: If you normalize variables, then it means that the RANGE of possible values (= Definitionsbereich), is set between 0 and 1. 11.22.1.1 Scale all X-Variables and your Y-variable: from sklearn.preprocessing import MinMaxScaler # Der Name der Klasse für Normalisierung = &#39;MinMaxScaler&#39; (--&gt; mega weird xDD) # step 1: initialize the Class scaler_X = MinMaxScaler(feature_range=(0, 1)) scaler_y = MinMaxScaler(feature_range=(0, 1)) # step 2: wende den Scaler uf die X- &amp; Y-Variablen (dabei nur auf das Training-Set) an (Achtung: funktioniert nur auf numy-arrays, # aber nicht auf Series // Data-Frame Columns!) scaler_X.fit(X_compl_df[:train_end_idx]) # alternativ: scaler_y.fit(X_train.to_numpy().reshape(-1, 1)) scaler_y.fit(y_compl_df[:train_end_idx]) # alternativ: scaler_y.fit(y_train.to_numpy().reshape(-1, 1)) # step 3: nach dem &quot;fit&quot; haben wir die Werte noch nicht als &quot;DataFrame&quot;-Typ gespeichert, weshalb wir nun noch &#39;transform&#39; anwenden X_norm = scaler_X.transform(X_compl_df) y_norm = scaler_y.transform(y_compl_df) 11.23 ## Umwandlungen 11.23.1 Convert a Series into an array: Series.to_numpy() # this also works on a column of a dataframe =) 11.24 ## Apply If-Conditions on a DataFrame Here is a really good reference: https://datatofish.com/if-condition-in-pandas-dataframe/ 11.25 ## Visualization of the Data 11.25.1 Plot &amp; Save an Image: # Step 1: import the library matplotlib for visualization import matplotlib.pyplot as plt # for the settings of the graph, such as title, windows-size etc... # Step 2: make the plot plt.figure(figsize=(14,6)) fig, ax = plot_series(y_train[10290:10320], # only take the last 30 observations from the trainings-set y_test, y_pred, labels=[&quot;y_train&quot;, &quot;y_test&quot;, &quot;y_pred&quot;]) plt.xticks(rotation=90) # rotates Beschriftung der X-Achse um 90-Grad, damit man überhaupt die X-Achse lesen kann # Step 3: save the plot fig.savefig(&quot;monochrome-2-test.png&quot;) # save the graph 11.25.2 For-Loops to iterate over lots of variables and save them: # step 1: create a list of all the columns, you want to iterate through cov_list = X_train.columns # I select all columns in my dataframe &#39;X_train&#39; # step 2: for i in cov_list: print(i) # just as a check, to see how the loop progresses covariate_name = i # everytime I iterate over a new variable, I store it into this variable --&gt; I will use this variable later to save each covariate = X_train[i] # I need each variable to be of type &quot;Series&quot; when creating a plot x = list(range(0, 720, 24)) # this will put a tick-mark after each day x.append(719) # I also add &#39;719&#39;, since the range()-function does not include this number fig, ax = plot_series(covariate[-720:]) # this plots the last month plt.xticks(x); # to only display some dates plt.xticks(rotation=90); # we need to rotate the x-axis, otherwise you cannot read it fig.savefig(&quot;visualisation_30_days-{}.png&quot;.format(covariate_name)) # save the graph for each column. The &quot;trick&quot; here is, that we can use # f-strings to create a unique name for each file =) 11.26 ## Python Magic Commands 11.27 Show ALL Columns of a DataFrames Wenn grosse Datensätze geprinted werden, ist beim Output oftmals ein Teil der Spalten maskiert, da schlichtweg der Platz fehlt, um alle Spalten anzuzeigen. Allerdings gibt es einen Weg, die Einstellungen via Pandas zu ändern: import pandas as pd pd.set_option(&#39;display.max_rows&#39;, 500) pd.set_option(&quot;display.max.columns&quot;, None) # this will tell Python: &quot;show me ALL the columns!&quot; pd.set_option(&#39;display.max_columns&#39;, 500) pd.set_option(&#39;display.width&#39;, 1000) 11.28 Working with HTML within Jupyter-Notebooks 11.28.1 How to use anchor-tags to make references within your Jupyter-Notebook? Klick here to move to the very top of this Jupyter-Notebook. This is achieved by using the &lt;a&gt;-HTML-Tag, and some unique ID-name &amp; CSS-styling on the &lt;a&gt;-Tag. &lt;a id=&quot;gebe-hier-passenden-id-namen&quot; style=&quot;color:black; text-decoration: none;&quot;&gt;Hier kommt der Text, auf welchem du zeigen willst...&lt;/a&gt; [Und das ist der Text, welcher dich weiterleitet](#top) 11.29 Share Variables between Notebooks This Magic Commands allows you to share any variable between different Jupyter Notebooks. You need to pass the original variable with the magic command. To retrieve the variable, you need to pass the same command with the -r parameter. myData = &quot;The World Makes Sense&quot; %store myData Stored ‘myData’ (str) Now that you ‘saved’ the variable myData with the Magic Commands, go and open another of your Jupyter Notebooks and type in the following: %store -r myData # step one: run this line in the first cell myData # step 2: run this line in the secons cell 11.30 ## Useful “Tricks” I stumbled upon 11.30.1 Reshape(-1, 1): What does reshape(-1,1) do? If you have an array or list, like this: [1,2,3,4,5]… Using reshape(-1,1) on it, will create a new list of lists, with only 1 element in each sub-list, e.g. the output will be [[1], [2], [3], [4], [5]]. See also this Stack-Overflow answer. 11.31 ## Math Tricks for Hacks 11.31.1 How to always round downwards?: Mit der Funktion round, kann man auf- oder abwärts runden, wie wir es gewohnt sind: round(47/24, ndigits = 2) # runde auf 2 Nachkommastellen 1.96 Mit der floor-Funktion, kann allerdings stets abwärts gerundet werden, wie das Beispiel zeigt: import math # um &#39;floo&#39; math.floor(47/24) 1 11.31.2 Calculate the Median? Since there is no built-in function for the median, you can write a function which will be able to calculate the median: # Step 1: make a function for the case &quot;median calculation for an ODD (= ungerade) list of numbers&quot; def _median_odd(xs) -&gt; float: return sorted(xs)[len(xs) // 2] # Step 2: make a function for the case &quot;median calculation for an EVEN (= gerade) list of numbers&quot; def _median_even(xs) -&gt; float: sorted_xs = sorted(xs) hi_midpoint = len(xs) // 2 return (sorted_xs[hi_midpoint - 1] + sorted_xs[hi_midpoint]) / 2 # Step 3: Use the 2 above functions to finally be able to build the median-function def median(v) -&gt; float: return _median_even(v) if len(v) % 2 == 0 else _median_odd(v) assert median([1, 10, 2, 9, 5]) == 5 # check if the above function was written correctly (you can change the number &#39;5&#39; and see # what happens, if this expression becomes incorrect) 11.32 ## Fun In this section, I will just put some important concepts &amp; some of my “inventions” that I found useful. import random import numpy as np df = pd.DataFrame(np.random.randn(10, 2), columns=[&#39;Col1&#39;, &#39;Col2&#39;]) df[&#39;X&#39;] = pd.Series([&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col1 Col2 X 0 1.616909 -0.998962 A 1 0.036902 -0.991193 A 2 -1.418524 -0.070984 A 3 1.207328 1.896855 A 4 1.376757 -0.333235 A 5 0.295216 1.488426 B 6 1.226809 -1.351006 B 7 0.476242 -0.431204 B 8 -1.565872 1.174200 B 9 -0.469251 -0.353409 B for i,el in enumerate(list(df.columns.values)[:-1]): a = df.boxplot(el, by =&#39;type&#39;) png png png 11.32.1 Arbeitszeit (left to work…) Berechnung: Um zu sehen, wie viele Stunden &amp; Minuten ich heute noch arbeiten muss, habe ich - als Übung - eine Funktion hours_to_go selbst erfasst, welche mir die Anzahl an noch zu verbleibenden Arbeitsstunden am Tag berechnet: 4*30*24 2880 import datetime # Da wir mit &quot;Zeit&quot; arbeiten, brauche ich das Package &quot;datetime&quot; def hours_to_go(Stunden, Minuten): # Input: wie viele Stunden &amp; Minuten hast du heute bereits gearbeitet? &quot;&quot;&quot;gibt an, wie lange ich noch heute &quot;Schaffen&quot; muss --&gt; gebe dafür bloss die Anzahl stunden &amp; minuten ein, die du &quot;heute&quot; bereits gearbeitet hast &gt; hours_to_go(3,5) --&gt; 0507 interpretation: ich habe fix 8h 12min zu arbeiten. Davon ziehe ich nun 3h 5 min ab --&gt; 0507 heisst, ich habe noch 5h 7min zu arbeiten, was stimmt!&quot;&quot;&quot; fixe_arbeitszeit = datetime.datetime(2021, 7, 1, 8, 12) # datetime(year, month, day, hour, minute, second, microsecond), # wobei die ersten 3 Argumente (Jahr, Monat, Tag) OBLIGATORISCH sind! --&gt; hier interessiert uns die letzten 2 Inputs, # nämlich die &quot;8&quot; und die &quot;12&quot; --&gt; diese wiederspiegeln meine fixe 8h 12 min Arbeitszeit bei der SBB gearbeitet = datetime.timedelta(hours= Stunden, minutes= Minuten) # anzahl an Stunden, die du noch arbeiten musst wie_lange_noch = fixe_arbeitszeit - gearbeitet print(wie_lange_noch.strftime(&quot;%H%M&quot;)) hours_to_go(0,5) # call the function to output, how many hours are left to work 0807 Um zu sehen, um welche Uhrzeit ich heute Feierabend machen darf, habe ich - als Übung - eine Funktion arbeit_ende selbst erfasst: import datetime # Da wir mit &quot;Zeit&quot; arbeiten, brauche ich das Package &quot;datetime&quot; def arbeit_ende(Stunden, Minuten): &quot;&quot;&quot;gibt an, wann ich heute mit &quot;Schaffen&quot; fertig bin --&gt; gebe dafür bloss die Anzahl stunden &amp; minuten ein, die du &quot;heute&quot; noch arbeiten musst, zum Beispiel arbeite ich heute noch &#39;4h 44 min&#39;: &gt; arbeit_ende(4, 44) --&gt; 2021-07-01 17:53:02.907698 &quot;&quot;&quot; jetzt = datetime.datetime.now() # current time added_time = datetime.timedelta(hours= Stunden, minutes= Minuten) # anzahl an Stunden, die du noch arbeiten musst ende = jetzt + added_time print(ende) arbeit_ende(8,7) # call the function to output, until when I need to work today 2021-07-29 15:53:13.919695 "],["openai.html", "Chapter 12 OpenAI 12.1 What can OpenAI do? | The Output 12.2 How does the Model work? | The Blueprint 12.3 Terminology of OpenAI-API | Wörterbuch 12.4 Technology behind it | Quellen", " Chapter 12 OpenAI Here is the link in order for you to play around: https://beta.openai.com/playground Achtung Kostenpflichtig (ab sofort): Even if you play around on the “playground”-area, you will get charged! OpenAI treats “Playground” the same, as the usage from the regular API. You can see how much you are billed by navigating to this link: https://beta.openai.com/account/usage (you get 18$ and 3 months to spend them “for free”). Open AI is a non-profit organisation that uses NLP-Models (with Deep-Learning) to be able to understand the context of - for example - questions and generate content on its own! But it goes even further than that.: in his youtube-video, Fireship.io talks about OpenAI creating art, e.g. the computer is able to draw for you! It is awesome! The guy / team who created this, truly is / are admirable and has / have surely put an incredible amount of effort in it! :O 12.1 What can OpenAI do? | The Output Content generation Summarization Classification, categorization, and sentiment analysis Data extraction Translation Many more! 12.2 How does the Model work? | The Blueprint The model predicts which text is most likely to follow the text preceding it. 12.3 Terminology of OpenAI-API | Wörterbuch Prompt = Input-Text you give the model. For example, this can be a question like What is Economics? And what is Structural Economics?. Key: Adding a simple adjective to our “prompt” is able to change the resulting output completely! Designing your prompt is essentially how you “program” the model. Completions = This is the output of the model, for example, it would be the “prediction” // answer that the model provides for the question What is Economics? And what is Structural Economics? “Temperature” = This is a way to bring in variation in the prediction // suggested results of the model. Example: If you submit the same prompt multiple times when the “temperature” is set to 0, the model would always return identical or very similar completions // results. If you re-submit the same prompt a few times with temperature set to 1, you will notice that the proposed results will be different. Possible Range for “temperature”: You can set the parameter “temperature” with values between (and including) 0 and 1. Key to note: “Temperature” is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature towards 0 means it will take fewer risks when making a prediction. Faustregel: It’s usually best to set a low “temperature” for tasks where the desired output is well-defined. Higher “temperature” may be useful for tasks where variety OR creativity are desired. token = You can think of tokens as pieces of words used for natural language processing. For English text, 1 token is approximately 4 characters or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900’000 words or 1.2M tokens. “Wechselkurs” Token zu Wörter: 1 token = 0.75 words, also ein 1:0.75 Verhältnis, dh für 1 Token erhalte ich 0.75 Wörter bzw. für 1 Wort erhalte ich 1.333 Token. “Wechselkurs” Token zu Zeichen: 1 token = 4 Zeichen, also ein 1:4 Verhältnis, dh für 1 Token erhalte ich 4 Zeichen bzw. für 1 Zeichen erhalte ich 1/4-tel Token. 12.4 Technology behind it | Quellen Hierarchical Text-Conditional Image-Generation with CLIP-Latents | Ramesh, Dhariwal "]]
