# Econometrics

## Synonyme:

- Dependent variable // y-variable // regressand // Y-intercept // outcome variable // response variable // label // ground truth
- X-variable // variable of interest // explanatory variable // regressor // covariates // independent variable // predictor // attribute
- regression of y on x // regress y on x
- slope coefficient // beta
- Controlling for // conditional on // holding fixed // holding constant // "certeris paribus" // statistically account
- Classical linear regression model // CLRM 
- Linear probability model // LPM
- Probit model // probit regressions
- Logit model // logit regressions
- (Population) data generating process // Population regression function // PRF // „true“ regression  (—> very often unknown)
- Sample regression function // SRF // fitted model // estimated model
- Range // distribution // histogram
- Dummy variable // categorical variable
- Endogeneity // Omitted variable Bias // OVB // violation of assumption 2 (= the non-zero mean assumption)
- R squared // coefficient of determination 
- RSS // SSR // residual sum of squares // sum of squared residuals
- Fitted value // predicted value // Y(hat)
- Validity // internal Validity // external Validity
- Randomized experiment // Random Assignment study // social experiment // randomized control trial // randomized trial
- Expected value // predicted value 
- Heterogeneous treatment effects // interaction terms 
- Fixed Effects // FE
- Difference-in-Differences // Diff-in-Diff // DiD 
- Two stage least squares // 2SLS // Second Stage // IV-Regression
- NA // Censored Data // Latent Variable —> can be estimated with a Tobit Model or Heckman Two stage Model (where the dependent variable is censored // has NAs, for example wages of people who don’t work cannot be observed // are censored) 
- Observation // Row in R // samples // records (in computer science)
- Column in R // field (in computer science)
- Variable // Column in R 
- Finite // Definite
- Infinite // indefinite
- Relativ zu // im Verhältnis zu

## English Words Synonyme

- Among = across
- Fixed = not random —> an example would be the true parameters of the population would be fixed // not random, but you don’t know them —> you can only approximate the true parameters by taking a random sample and estimating it’s coefficient to approach the true / fixed coefficient with a random (but estimated) coefficient, which resulted from a sample that the researcher took.
- With respect to … = in Bezug auf… = Auf der (z.B. klanglichen) Ebene…

## Allgemeines

- **Numerator** = Zähler
- **Denominator** = Nenner
- **Ditto** = ebenfalls
- **A priori** = When dealing with ommitted variable bias, you need to ask yourself the question: what are the most likely sources of important omitted variable bias in this regression? Answering such a question requires applying economic theory & expert knowledge, and should occur before you actually runy any regressions; because this step is done before analyzing the data, it is referred to as „a priori“ („before the fact“) reasoning.
- **Non-Linearity in X examples**: A regression is not linear in one regressor „x“, if - for example - the „x“ was logarithmic or is an interaction term.
- **Non-linearity in „beta“ // parameters // coefficients**: An example of a regression function being non-linear in the parameters // coefficients are: 1) logistic regressions, where the dependent variable is between 0 and 1 (you use an s-shape function that can match all x-values between -infinity to +infinity to a y-value between 0 and 1); or 2) negative exponential growth:
![Non-Linearity in Beta illustriert. **Importantly**, non-linearity in „beta“ cannot be estimated using OLS, but rather, with an extention of OLS called nonlinear least squareds.](./bilder/econ/nonlinear-in-their-parameters.jpeg)
- **Independence assumption when dealing with probabilities & variances**: Independence has implications on how you calculate in statistics. For example, it affects the calculation of joint probabilities or variances with 2 random variables:
![Example of the calculation when two random variables are assumed to be independent from each other.](./bilder/econ/independence-assumption-on-joint-prob.jpeg)

## Statistics Formulas

- **Formula for covariance**:
![The Formula for Covariance](./bilder/econ/formula-covariance.jpeg)
- **Formula for (population) variance**:
![The Formula for the population Variance](./bilder/econ/formula-pop-variance.jpeg)

## Definitionen

- **Beliefs**: VL1, Yanazingawa, S.2
- **Evidence**: VL1, Yanazingawa, S.2
- **Counterfactual**: VL 1, Yanazingawa, S.5
- **Validity // internal Validity // external Validity**: VL 1, Yanazingawa, S.7, S.12
- **Randomized experiment // Random Assignment study // social experiment // randomized control trial // randomized trial**: VL1, Yanazingawa, S.7
- **Control Group**: it mimics the counterfactual: VL1, Yanazingawa, S.7
- **Treatment Group**: VL1, Yanazingawa, S.7
- **Random Assignment (= part of randomization)**: VL1, Yanazingawa, S.7
- **Random Sampling (= part of randomization)**: VL1, Yanazingawa, S.7
- **Why is randomization important?**: VL1, Yanazingawa, S.7-8
- **Observational Study**: VL1, Yanazingawa, S.10 (wichtig für MA, weil es genau in mein Kontext fällt!)
- **Bivariate Regression**: Vl2, Yanazingawa, S.2
- **Population Regression Funtion // Sample Regression Function**: VL2, Yanazingawa, S.3
- **Dependent variable // y-variable // Regressand // Y-intercept**: VL2, Y-intercept, S.3
- **Independent variable // x-variable // Regressor // Covariate // Explanatory Variable(s)**: VL2, Y-intercept, S.3
- **Slope Coefficient // Beta**: VL2, Y-intercept, S.3
- **Interpretation of Slope Coefficient Bivariate Regression**: VL2, Yanazingawa, S.4
- **Interpretation of Slope Coefficient Multivariate(!) Regression**: VL2, Yanazingawa, S.10
- **Interpretation of Intercept Bivariate Regression**: VL2, Yanazingawa, S.4
- **Interpretation of Intercept Multivariate(!) Regression**: VL2, Yanazingawa, S.10
- **Expected Value // Predicted Value**: VL2, Yanazingawa, S.4
- **Interpretation p-value (= it's a bedingte(!!) W'keit)**: VL2, Yanazingawa, S.5
- **What does it mean to have a small Standard-Error?**:  VL2, Yanazingawa, S.6
- **Statistical Significance & its Implication**: VL2, Yanazingawa, S.7
- **Understanding "Holding Constant"**: VL2, Yanazingawa, S.12
- **Positive // Negative Bias Table**: VL3, Yanazingawa, S.5
- **Overstated // Understated Concept** (Prof does not like the concept...): VL3, Yanazingawa, S.5
- **Dummy Variable // Categorical Variable**: VL4, Yanazingawa, S.1
- **Dummy Variable Trap**: VL4, Yanazingawa, S.3 & S.6
- **Multicolinearity**: VL4, Yanazingawa, S.3
- **Range // Distribution // Histogram**: Ü1, Yanazingawa
- **Scatterplot**: Ü1, Yanazingawa
- **Linear probability Model // LPM**: VL5, Yanazingawa, S.2
- **Interpretation of Slope Coefficient in Linear Probability Model // Regression**: VL4, Yanazingawa, S.2
- **Interpretation of Intercept Linear Probability Model // Regression**: VL4, Yanazingawa, S.2
- **Advantages // Disadvantages of LPM**: VL5, Yanazingawa, S.4
- **Advantages // Disadvantages of Logit Model**: VL5, Yanazingawa, S.5
- **Probit Model // Probit Regressions**: VL5, Yanazingawa, S.5
- **Logit Model // Logit Regressions**: VL5, Yanazingawa, S.13
- **Implement a Probit Model with Code**: Ü2, Yanazingawa, Question 6
- **Heterogeneous Treatment Effects // Interaction Terms**: VL6, Yanazingawa, S.1, S.4 (first bullet)
- **Coefficient Interpretation of Continuous-Dummy Interaction**: VL6, Yanazingawa, S.4-7
- **Coefficient Interpretation of Dummy-Dummy Interaction**: Vl6, Yanazingawa, S.8
- **Merge different Datasets together**: Ü3, Yanazingawa, 5) c)
- **Bad Controls**: Ü3, Yanazingawa, 5) d), siehe Korrektur auf meinem gedruckten Problemset!
- **Fixed Effects // FE**: VL7, Yanazingawa, S.3 & Zusammenfassung (ZF) auf letzter Seite des Handouts
- **Difference-in-Differences // Diff-in-Diff // DiD**: VL8, Yanazingawa, S.1
- **Parallel Trend Assumption**: VL8, Yanazingawa, S.2
- **Interaction Terms between two FEs**: Ü4, Yanazingawa, Aufgabe 3, Part I = a „country x time FE“ interaction controls for country-specific time-trends. However, to use it, you need 2 observations for each country and year (if we assume the time FE to be years...)
- **Make Dummy Variables while having multiple Conditions**: for example, the city needs to be between 25 km and 75 km away from the border = Ü4, Yanazingawa, Aufgabe 1, Part II
- **Clustering the Standard Errors**:  Ü4, Aufgabe 1, Part II
- **Endogeneous Regressor**: VL9, Yanazingawa, S.1
- **Validity of an Instrument**: VL9, Yanazingawa, S.1
- **Exclusion Restriction**: VL9, Yanazingawa, S.2
- **Two Stage Least Squares // 2SLS // Second Stage**: VL9, Yanazingawa, S.6 & S.7
- **First Stage**: VL9, Yanazingawa, S.6
- **Reduced Form**: VL9, Yanazingawa, S.6
- **IV-Regression in R**: Ü5, Yanazingawa, Aufgabe 1 a)
- **Calculation of Mean while ignoring NAs**: Ü5, Aufgabe 2 a)
- **Standardize a Variable**: Ü5, Audgabe 2 a)
- **Interpretation with standardized Variables**: Ü5, Audgabe 3
- **ATE, ATT, ATUT**: Ü2, Biroli, Aufgabe 2
- **Selection Problem**: Ü2, Biroli, Aufgabe 3
- **Histrogram**: zeigt die an, wie häufig ein Wert auftaucht.
- **Scatter-Plot**: Plot, welcher die Korrelation zwischen zwei Variablen in einem Datensatz aufzeigen.
- **Dichtefunktion**: W'keit, einen Wert zwischen "Wart a" und "Wert b" zu erhalten (im Kontext von Zufallsvariablen) = verwendet man bei stetigen Massen, wie Gewicht oder Distanz etc...
- **Cross-Sectional Data**: Observation of an economic agent (for example individuals, firms, households etc.) collected at one point in time; you can have agent FEs, but not time FEs!
- **Panel Data**: Multiple observations on agents over time (—> you have a time index, as well as an index for individuals!) —> here you can add time FEs and also agent FEs
- **Confounders // Mediator Variables**: These are the unobservable factors in your regression // in the error term that correlate with your x-variable of interest, as well as your y-vaiable, thus inducing bias when you estimate your model
![Confounders illustrated.](./bilder/econ/mediator-variables-illustration.jpeg)
  - <u>Note</u>: The term „Mediator“ is used for independent variables that cause a change in the y-variable.
  - <u>Note 2</u>: The term „Moderator“ are used for interaction terms, e.g. that the effect of an x-variable is modified and depends on the second variable.
- **Bivariate Regression**: a regression with only one regressor // x-variable
- **Multivariate Regression**: a regression with multiple x-variables.
- **3 Common Reasons for Endogeneity**:
    - <u>**Simultaneous Causality**</u>: x causes a change in y but y also causes a change in x
    - <u>**Correlated unobservable Variable**</u>: leads to OVB in your coefficient of interest
    - <u>**Measurement Error in x**</u>: you will have attenuation bias, e.g. bias towards zero 
        - <u>Note</u>: There is also the possibility that you have measurement error in y: in this case, we do not have a bias in the coefficient of interest! —> however, the standard error of the coefficient gets bigger if you have this kind of measurement error!
- **Degrees of Freedom (d.o.f.)**: If we have „N“ observations and „K“ regressors in a regression, then we have: v = N - K „degrees of freedom“
- **R-Squared**: measures the goodness of fit, e.g. how much of the variance of y can be explained by our model // it is the squared correlation between the population regression function and our predicted SRF.  
    - **Problem with the measure R-Squared**: if you rescale the y-axis in the regression, for example with a log-function, it will lead to a reduction of the variability in Y and increase the R squared without improving the regression.
- **Overfitting the Data**: This term is another problem that comes along with the measure „R squared“: the thing is, that you always increase the R squared when you add an additional regressors into your regression. With this characterstic, you could think of people adding regressors with almost no correlation to Y but still increasing the R squared without augmenting the precision of the estimated model! 
- **Unbalanced Groups (Kontext: RCTs pr Dummies)**: If the treatment and control group are - on average - different to begin with, then the groups are unbalanced! —> your estimated treatment effect will likely to be biased
-  **BLUE** = best (= efficient), linear, unbiased estimator
- **Consistency**: if you have an infinite number of observations for your sample, then the estimated coefficient will converge towards the true coefficient of the population with a smaller variance for the estimated coefficient distribution —> we then say that the estimated coefficient is „consistent“ —> Note: the statistical theorem that forms the basis for consistency is called the Law of Large Numbers 
- **Attenuation Bias**: This is the Bias, when we have measurement error, meaning that the coefficient of interest „beta“ will be biased towards zero —> e.g. if beta > 0, then beta has a negative bias (it is smaller than what it should be) or if beta < 0, then beta has a positive bias (it is bigger than it should be) —> note: If we have a measurement error in one particular „x“ of a multivariate regression, we will have a higher bias (in absolute value), which is worse than in a simple linear regression case —> the bias towards zero gets bigger! (See the formula on page 90 of „sources of bias, Crawford)
- **Partitioned Regression**: you do a bivariate regression of the residuals of 1) a regression of x(i) on all other x(-k)] on 2) the residuals of a regression of y on all other x(-k)]
- **Replication Samples // Monte Carlo Simulation**: You (randomly!) draw different samples of size „N“ from the same population many times, for example say you draw them „R“ times. Then, you plot the distribution of the average of each sample —> you will see that, by drawing larger sizes of samples „N“ holding fixed the number of draws „R“ —> the average of the distributed average samples will converge to the true average of the population! 
- **Serial Correlation**: correlation of a variable with a lagged version of itself —> you need this because of the assumption 4 of the CLRM: residuals should not have a serial correlation with one another
- **Selection Bias**: when an individual selects himself into the treatment- / control-group, because he has an advantage of doing so.
- **Compliers (in an Instrumental variable setting)**: You usually have compliers appearing, when an instrument is not assigning people randomly between treatment- and cotrol-groups. In this case, the compliers would be the people who choose to get treated (D=1), when they actually get assigned to treatment (Z=1) and decide not to get treated (D=0), when the instrument is not activated (Z=0).
- **Never Takers (in an Instrumental variable setting)**: People who choose to never get treated, independently whether the instrument is switched on or off! —> D = 0 (always!), if Z = 1 bzw. Z = 0
- **Always Takers (in an Instrumental variable setting)**: People who choose to always get treated, independently whether the instrument is switched on or off! —> D = 1 (always!), if Z = 1 bzw. Z = 0
- **Synthetic Control**: This method creates an appropriately weighted average of control units which best approximates the evolution of the outcome in the treated unit before treatment. —> the key concept of using the synthetic control is that we construct an artificial control group to get a reasonable estimate for our missing counterfactual (= our treatment, which would not have been treated)
- **RDD with a `sharp` Design**: This is an RDD where the probability of treatment at the cutoff point changes  from 0 to 1.
- **RDD with a `fuzzy` Design**: This is an RDD where the probability of getting treated changes (also) discontinuously, but less than 100%. 
- **Saturated Model**: This is a model with a full set of dummies, e.g. in such a model, there is no constant!
- **`Type I`-Error in statistical Testing**: A type I error happens, when we wrongly conclude that the null hypothesis H(0) is false, when it‘s acutally true (Eselsbrücke —> Jemand sagt die Wahrheit, aber alle meinen, er lügt!)
    - The significance-level „alpha“ is representing the type I error.
    - Ideally, we want to minimizes both errors: type I and type II
- **`Type II`-Error in statistical Testing**: A Type II error happens, when we wrongly conclude that the null hypothesis H(0) is true, when it is actually false (Eselsbrücke —> Jemand erzählt eine Lüge, und alle glauben ihm diese!)
    - Often, the type II error is referred to as „beta“: the probability of concluding that H(0) is true when it‘s actually false. Hence, (1-beta) is the probability of concluding that H(0) is false(!!), when it is actually // truly false (probabilities sum up to 1, that‘s why we can do this!). This (1- beta) probability is also called the statistical power! 
    - Ideally, we want to minimizes both errors: type I and type II
- **Statistical Power**: read the first sub-point of type error II just above!
    - <u>Grundsätzlich gilt</u>: a small sample size gives us little power to reject the null hypothesis (wahrscheinlich, because you have large standard errors // variance(beta-coefficient) is high), whereas a large sample size gives us more statistical power. —> Usually, we want the power to be larger than 50%! —> a power between 80 to 90% (e.g. the probability to accept H(0) even though it is false, would be 10 to 20% in this case) is what you want, otherwise you would not conduct a study
- **Supplementary Analysis**: supplementary analysis seeks to shed light on the credibility of the primary analysis (= this is for example a DiD method, or IV, or RDD) —> an example of a supplementary analysis is placebo testing
- **Objective Function**: this is a general function that individuals seek to maximize —> example: a lifetime utility function takes on many values. The individuals seek to choose the maximum value (somtimes the minimum value, if it’s a cost-funtion) out of all those values of this „objective function“.
- **Resampling**: If you have a classification problem, where you have a y-variable with 9000 cases of „obese“ and 1000 cases of „normal weight“, then you have a problem that „normal weight“ people are under-represented in your sample. Thus, you have imbalanced data and you need to use resampling techniques with - for example - the „imblearn“ library to have a dataset that is equally distributed, e.g. 1000 cases of „normal weight“ and 1000 cases of „obese“.
- **A staggered Treatment (first time I heard of it was in the context of DiD)**: This means that - for example - an individual can choose to get treated, but once it gets treated, he cannot get out of the treatment (example: COVID vaccines are like that).

## Different Tests // Vorgehen bei den Tests

- **Check if random Assignment // Randomization into Treatment- & Control-Group was successful**: VL1, Yanazingawa, S.
- **Placebo Testing**: this test involves demonstrating that your effect does not exist when it really „should not“ exist —> you pick a period where no treatment occured and try to see if your treatment group really did not react to „no treatment“ in this particular period!

## Diverse Berechnungen

- **Review of Hypothesis-Testing // Testing differences in Means**: Vl1, Yanazingawa, S.9 & S.11
- **Hypothesis testing "no effect" (= H<sub>0</sub>) VS. "there is an effect"**: VL2, Yanazingawa, S.5
- **Bias** = short Regression Coeff. - long Regression Coeff. = `beta(2)*gamma [= corr(Y, omitted) * corr(X, omitted)]`

## Accept // Reject Null-Hypothesis

- **t-statistic**:
  - *beidseitiger Test*: if t-stat (im Betrag) > crit.-value --> reject the null
    - <u>Note</u>: we use Z<sub>(1- [alpha/2])</sub> as the critical value, since we have a two-sided test.
  - *rechtsseitiger Test*: if t-stat > crit.-value --> reject the null
    - <u>Note</u>: use Z<sub>(1-alpha)</sub> as the crit value, since we have a one-sided test.
  - *linksseitiger Test*: if t-stat < critical value --> reject the null
    - <u>Note</u>: use Z<sub>(1-alpha)</sub> as the crit value, since we have a one-sided test.
- **Important general fact about hypothesis**: Just because you cannot reject a null-hypothesis does not mean that the null-hypothesis is true. It just means that you don’t have enough empirical evidence to prove that the alternative-hypothesis is true.



