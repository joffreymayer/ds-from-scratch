---
title: "Foundations of Data Science"
author: "Joffrey Mayer"
date: "10/17/2021"
output: html_document
---

# Foundations of Data Science

In meinem Data Scientist Job werde ich häufig auf ähnliche Probleme stossen mit der Zeit. Hier habe ich eine Reihe an Fragen aufgelistet, welche ich fähig sein muss, zu beantworten, wenn ich effizient in meinem Beruf sein will!

## Find Data

In order to work with data, you also need to have the supply. 

Here is a list of **websites, that offer free to use datasets**:

- Kaggle
- UCI Machine Learning Repository
- [awesome-public-datasets](https://project-awesome.org/awesomedata/awesome-public-datasets)
- Google Dataset Search
- https://datasetsearch.research.google.com/

## Questions you need to answer when starting a new Project

BEFORE you start writing your R-Scripts or Jupyter-Notebooks, you **first need to think about several key-things** &amp; -questions:

```
- Welche Datenbasis haben wir?
- Was ist das Prognoseobjekt?
- Welche Metrik hast du in deinen Notebooks verwendet?
- Wie ist das Trainings- & Validierungs-Dataset aufgebaut?
- Welche Daten sind im Test-Set, welche für den Benchmark verwendet werden?
- Welche Prognoseansätze wurden angewendet? 
- Welche Daten sind effektiv genutzt & welche sind verfügbar?
- Etc…
```

- <u>Datenbasis // Woher kommen die Daten?</u>: 

ENTSOE exklusiv. Das ist der Dachverband der TSO (kennen Nachfrage und Angebot)

- <u>Prognoseobjekt (= y-dach)</u>: 

Stündliche EUR/kWh für Folgetag.

- <u>Metrik</u>: 

Zeigt, wie gut das Modell "lernt" (= training error) & "generalisiert" // exrapoliert (= generalization error). Im Notebook von Philipp werden 2 verschiedene 
Metriken verwendet, nämlich: 

	- Mean Absolute Error, sowie 
	- [Root] Mean Squared Error.

- <u>Training Dataset</u>: Daten von 2019.

- <u>Validation Dataset</u>: Random choice of 20% of the hours within this year.

- <u>Mit welchem *true y-value* werden die Predictions verglichen? // Was ist der Benchmark? // Test-Set</u>: EFFEX Spotpreise benutzt für Benchmark // true [y-]values.

- <u>Struktur [der Analyse]</u>:

	- <u>Output</u>: Day ahead Strompreis CH
	- <u>Input</u>: jeweils 24h für ca. 20 Prädiktoren. Reshape: K dimensionen = #Prediktors x 24h
	
- <u>Was ist die Daten-Imputation Methode?</u>: Missing Values mit Mittelwerten


## Effective Research: How to find answers quickly?

> How to find Answers quickly, especially when a Concept is complicated

Aus Erfahrung weiss ich jetzt, dass **Youtube bisher *immer*, die beste Quelle war, um mir etwas schnell &amp; effizient beizubringen**. 

Twitter-Community von Wissenschaftlern sind ebenfalls sehr wertvoll. Als **Beispiel wäre [dieser Twitter-Post](https://twitter.com/seanjtaylor/status/1123278380369973248?s=11) vom Prophet-Gründer**.

## How to read Notebooks from other People?

*Es geht am Anfang um die Gesamtübersicht und noch nicht um die Details // Eigenheiten im Code oder im Datensatz! Am wichtigsten ist es, dass du [diese Fragen](#questions-you-need-to-answer-when-starting-a-new-project) zunächst beantworten kannst, wenn du das Notebook liest.*

## Machine-Learning Theory

### Difference between Training-, Validation- &amp; Test-dataset

Since I started with Machine Learning, I was always confused about the concept of **data splitting**, e.g. which sub-set of the *entire* dataset is now considered to be the *training dataset*, which one is the *validation dataset* and which one is the *testing dataset*. 

<u>In the graph below, you see how it is defined correctly</u>:

![Training Set VS. Validation Set VS. Test Set](./bilder/training-VS-validation-VS-test-ddset.jpg)

- <u>Important to note</u>: There is oftentimes confusion between the definition of *validation dataset* VS. *testing dataset*, because there is no consens about it. **Therefore - and if we take the above picture as the "true" definition - some people will call the *validation dataset* the "test dataset" and vice versa, e.g. the *test data* as the "validation data"! xD** 

#### Validation-Set VS. Test-Set

> What is the difference between a `Validation Dataset` and a `Testing Dataset`?

Es gibt keinen Konsens dafür, was nun das `validation dataset` und welches das `test set` genau ist. **Nima** (= mein Mentor bei der SBB) verwendet den Begriff `test set` für dasjenige Dataset, welches `hold out` // separat für die (spätere) `Prediction` verwendet wird.

- <u>Regel zu Unterscheidung der beiden Terme</u>: Wenn steht: `we hold out [name of the set]`, dann ist es das `testing dataset`.

#### Reason for a Validation-Set

> Why do we need `validation set`?

At the end, you want the best model possible. If you want to tune your hyperparameters, you will need your `test set`. **The big problem here, however, is that - *if we use the test-set more than once when wanting to find out the best hyperparameters* - our model will know the data by heart and the predictions will be too good, <u>but only</u> on this dataset that you are currently using**! The model will not generalize well on new data. 

*That's why we need this additional subsetting of the training-dataset!*

### Concept of Stratification

Angenommen du hast eine *kategorische* Y-Variable, welche entweder Nullen oder Einsen als Werte annimmt. Nehme nun an, dass - *im Training-Dataset* - der Anteil der Nullen 60% beträgt. Wenn du nun ein `stratified Sample` willst, dann wird das *Test-Dataset* ebenfalls einen Anteil von 60% an Nullen enthalten!

- <u>Quelle</u>: [Train-Test-Split in Sklearn and Cross-Validation](https://www.youtube.com/watch?v=KEBS7Kyc0Po)

### Cross-Validation

The concept of *cross-validation* can be splitted into 2 parts:

- <u>Step 1</u>: Split your dataset into 1 training- &amp; 1 test-set.
	- <u>Rule of Thumb</u>: Usually, the split is **70-90% training set** and **10-30% for the test-set**.
	- <u>Code Example</u>: 

![Example of Code for Train-Test Split](./bilder/code-example-cross-validation.png)

- <u>Step 2</u>: Now, we divide the *training set* further, such that it will contain - *if we assume a 3-fold cross-validation as an example* -  **3 <u>different</u> validation sets** and **3 training sets**.

![Allgemeine Cross-Validation](./bilder/example-of-a-general-3-fold-cross-validation.png)

#### <u>Special Case</u>: Cross-Validation for Time-Series Data

Because time-series are ordered, since *the flow of time* is only going forward, the graph shown above for *step 2* is <u>not</u> valid and we need another approach: 

#### Why do we use Cross-Validation?

There are 2 reasons:

1) It allows us **to compare the score (MAPE, MAE or RMSE) of different model set-ups**, for example: 
  - *CV-Scheme changes*, e.g. a _Time-Series_ Prophet-Model with a `Sliding-Window` of 4 Months VS. a Prophet-Model with an `Expanding-Window` (full past).
  - OR *changes in the covariates*, e.g. a model that includes an important covariate, while the other model does not.
  **This will allow you to draw conclusions regarding the selection of "the best" model**.
2) It allows you **to assess the performance** of different machine-learning methods. In a _classification-setting_ for example, you can use the _confusion-matrix_. 

![Cross-Validation for Time Series](./bilder/example-3-fold-cross-validation-for-time-series.png)

This picture shows **an example of a 3-fold cross-validation for a time-series**. 

### Normalisierung der Daten

> Was versteht man unter `Normalisierung`? Was sollte man dabei beachten? Warum wird in einem Pre-Processing Schritt eine Normalisierung durchgeführt?

`Normalisierung` == **Subtrahiere den Mittelwert und dividiere durch die Standardabweichung jedes Merkmals**. Es sollte beachtet werden, dass der Mittelwert und die Standardabweichung **nur anhand der Trainings-Daten berechnet werden sollte**, damit die Modelle keinen Zugriff auf die Werte in den Validierungs- und Testsätzen haben.

**Es ist wichtig, Features zu skalieren, bevor ein neuronales Netzwerk trainiert wird**. Normalisierung ist eine gängige Methode für diese Skalierung.

### RNN

> Was macht ein `Reccurrent Neural Network (RNN)`?

Ein Beispiel für ein RNN wäre ein `Long Short Term Memory` Modell (LSTM Modell). Dabei nimmt das RNN zunächst ein ganz kleines Vergangenheits-Intervall, macht eine Modell-Estimation und dann - im nächsten Schritt - wird ein grösseres Vergangenheits-Intervall verwendet (**inkl. predicted Y-Variable aus dem vorherigen Modell**), um eine neue Modell-Estimation zu machen etc.

![Beispiel eines RNN: hier ein LSTM](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/lstm_many_window.png?raw=1)

### Data-Pipelines

**Data Pipelines use an input to produce an output and then - on a second step - use the produced output as an input to produce another output etc...**

- <u>Visualisierung</u>: Du kannst dir unter _Data Pipelines_ nichts anderes als eine "Guetzli Fabrik" vorstellen, welche diverse Produktionsmaschinen verwendet - zum Beispiel einen Teig-Cutter, dann einen Butter-Schmierer, sowie einen fetten Ofen und einen Sortierer von "guten VS. schlechte Guetzli" - um die Inputs immer weiter zu verarbeiten, sodass schlussendlich ein Endprodukt (= die fertigen "Guetzli") ensteht, dass man verkaufen kann.

> [How to create good Data-Pipelines in Scikit-Learn?](https://www.youtube.com/watch?v=w9IGkBfOoic)

 - <u>Ziel von Data-Pipeline</u>: Write more clean // readable code, especially when you do <em>data cleaning</em>. **A datapipeline is basically a way of standardizing your code**.
 - <u>Warum sind Data-Pipelines so geil?</u>: Because you can **compare many different regression-models (Linear-Regression Vs. Logistic-Regression Vs. RandomForrest ...), applying different "scaling-techniques" (= normalize a variable with mean 0 and standard-deviation of 1), as well as using "data cleaning techniques" (= reduce dimensions via PCA, reduce missing-values etc...)**. Another cool thing to note is, that **you can choose the order, in which the cleaning, scaling and fitting occures!**
   - See also the summary of the guy [on Youtube ab 10:00-11:00](https://www.youtube.com/watch?v=w9IGkBfOoic).
- <u>Link to Github for an example</u>: <a href="https://github.com/krishnaik06/Pipelines-Using-Sklearn/blob/master/SklearnPipeline.ipynb" target="_blank">Jupyter-Notebook code-example on how to create a little Data-Pipeline by yourself</a>

## Programming Theory

### Data-Types

In `R` oder `Python` ist es wichtig zu verstehen, dass gewisse Funktionen nur dann funktionieren, wenn die Inputs, die wir in die Funktion eingeben wollen, einen bestimmten Data-Type aufweisen müssen.

On the website *W3-Schools*, I found this [extremely good overview of all data-types](https://www.w3schools.com/python/python_datatypes.asp), which is crucial concept to understand when doing data cleaning. 

![Overview of different Data-Types](./bilder/data-types-overview.jpg)
*Data-Types are a key-thing to understand. Otherwise, you won't be able to apply some algorithms on your dataset!*

### Global Variables VS. Local Variables

Variables that are created **outside of a function** are known as *global variables*.

Global variables can be used by everyone, both **inside of functions <u>and</u> outside**.

- <u>Example of a global variable</u>:

```
x = "awesome"

def myfunc():
  print("Python is " + x)

myfunc()

>>> Python is awesome
```

In contrast, if you create a variable *with the same name* <u>inside</u> a function, this variable will be **local**, and **can only be used <u>inside</u> the function**. The global variable with the *same* name will remain as it was, global and with the original value.

- <u>Example of a local variable</u>:

```
x = "awesome"

def myfunc():
  x = "fantastic"
  print("Python is " + x)

myfunc()

print("Python is " + x)
```
- Output of this: [click here](https://www.w3schools.com/python/trypython.asp?filename=demo_variables_global2)

### Array

> What is an `array`?

An `Array` is a List of Data. In a `DataFrame`-Object, you can think of a `column` *or* a `row` to be `arrays`. It is a data structure, which contains "n" objects within a list. 
	- <u>Quelle</u>: [The Coding Train 3:10-3:22](https://www.youtube.com/watch?v=NptnmWvkbTw)


## Statistics-Theory

### P-Hacking

> Was ist <u>p-hacking</u>?

In der Statistik gibt es den **p-Wert** ein: *Man nimmt an die Hypothese sei wahr und berechnet dann die Wahrscheinlichkeit, dass die beobachtete Statistik mindestens so extrem ausfallen würde (für die Gegner von Wischi-Waschi [hier die Wikipedia-Definition](https://de.wikipedia.org/wiki/P-Wert#Mathematische_Formulierung))*. Falls diese Wahrscheinlichkeit unter 5% liegt, dann sei das Resultat "statistisch signifikant" (yay!) und die Nullhypothese kann verworfen werden, was oftmals die Absicht ist. 

<u>Das **Problem** ist nur</u>: *Hypothesen gibt es viele und z.T. auch recht ähnliche*. Wenn man genug Hypothesen aufstellt - **vor allem, <u>nachdem</u> man sich die Daten angeschaut hat** - dann ist es durchaus möglich, dass man ein statistisch signifikantes Resultat erhält, unabhängig davon, ob das Resultat tatsächlich auch stimmt. **Das nennt man p-Hacking**. Es kommt häufig in der Forschung vor, aber es kommt sicher auch in der SBB vor (dennoch hier eine +1 für Hypothesen-basiertes arbeiten!). Wie einfach man in die "falsche Signifikanz Falle" tappen kann, wird hübsch in dieser Gallerie falscher Korrelationen illustriert.

## Appendix for the Future

> Welche Zeitperiode sind am geeignetsten für Zeitreihenanalysen mit Machine Learning?




