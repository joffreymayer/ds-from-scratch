# Machine Learning

In diesem Kapitel, werde ich vertiefter in das Thema "Machine Learning" eingehen. 

## Typen von Machine Learning

Es gibt grundsätzlich **2 Kategorien**, in denen die verschiedenen *Methoden in Machine Learning* eingeordnet werden:

1) **Supervised Learning**
    - <u>Goal 1</u>: **Classification**
      - Image Classification
    - <u>Goal 2</u>: **Predictions** with Regressions
      - Translation (zum Beispiel angewendet in Deepl.com)
      - Image Captioning (siehe Bild): ![Beispiel zu Image Captioning](./bilder/machine-learning/example-image-captioning.png)
      
  
2) **Unsupervised Learning**
    - Clustering:
      - Matching // K-Means Clustering
    - Dimension Reduction:
      - Principal Component Analysis (PCA)
    - Outlier Detection
  

## Supervised Learning

**Bei Supervised Learning** wird die **Annahme** gemacht, *dass man die Funktion 'f' **kennt**, um mittels den Werten der Zufallsvariablen 'X' die Werte der Zufallsvariablen 'Y' herauszufinden*. 

<u>Mathematisch ausgedrückt</u>: `f : X -> Y`.

## Methods

Hier soll eine Auflistung aller Vorgehen bei Machine Learning aufgelistet werden:

- **<u>Algorithm for "best fit" to find the weights</u>**:
  
  1) Guess some random weights.
  
  2) "Go downhill", e.g. **apply a _learning rate_** when using the method **gradient descent**.
  
  3) Is the **fitted line good enough?** 
      - If the *answer is "no"*, then go **back to point 2)**.

## Wichtige Funktionen

Da Machine Learning mittels Aktivierungsfunktionen funktioniert, folgt eine Übersicht zu verschiedenen Funktionen:

- **<u>Identity Function</u>**: damit ist die Funktion `f(x) = x` bzw. `y = x` gemeint:

```{r identity-function, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
myfun <- function(xvar) {
  xvar
} # first, I specify the function
ggplot(data.frame(x = c(-10, 10)), aes(x = x)) +
  stat_function(fun = myfun, geom = "line", colour="#FF9999") + # then, pass the above "user-defined function" called 'myfun' as an input into the function stat_function(). Note that I plot the function with a red line, to differentiate it from the x- & y-axis.
  geom_vline(xintercept = 0) + # zeichnet eine vertikale Linie bei 0, um die y-Achse zu verdeutlichen
  geom_hline(yintercept = 0) # zeichnet eine horizontale Linie bei 0, um die y-Achse zu verdeutlichen
```

- **<u>Logitstic Function // Sigmoid Function // S-Shaped Function</u>**: damit ist die Funktion $\sigma(x) = \frac{1}{1 + e^{-x}}$ gemeint:
  
```{r input-only-2, echo=FALSE, message=FALSE, warning=FALSE}
# Here I will plot a Graph with ggplot2:


### 1) Load packages & datasets needed:

library(ggplot2)
library(dplyr)
library(gcookbook)

### 2) Plot a function:

myfun <- function(xvar) {
1 / (1 + exp(-xvar))
} # first, I specify the function

# First, create a dataframe with only 1 variable - e.g. our range of x-values -
# that goes - for example - from 0 to 20 [note that - technically - the command
# x = c(0,20) will only create two values, namely 0 and 20, but NOT the whole
# range of values from 0 to 20!!! However, the code below works anyway!]. Next,
# tell R that you want those numbers 0 & 20 to be plottet on your x-axis -
# -> this is all the code within the 'ggplot()' function:
ggplot(data.frame(x = c(-7, 7)), aes(x = x)) +
  stat_function(fun = myfun, geom = "line", colour="#FF9999") + # then, pass the above "user-defined function" called 'myfun' as an input into the function stat_function()
  geom_vline(xintercept = 0) + # zeichnet eine vertikale Linie bei 0, um die y-Achse zu verdeutlichen
  geom_hline(yintercept = 0) # zeichnet eine horizontale Linie bei 0, um die y-Achse zu verdeutlichen
```

## Uni-Kurs `Neural Networks & Deep Learning`

### Notation Neural Networks

**Machine Learning verwendet unterschiedliche Begriffe für diverse gleiche Konzepte & Definitionen aus den Wirtschaftswissenschaften**. Nun geht es darum, die korrekte Übersetzungen für diese Wörter aufzulisten: 

- weights = Beta-Coefficients = parameters = a, b = neurons
- Input units = independent variables "x" 
- inputs = Anzahl Observations in Total
- Output unit = Dependent variable "y" = labels
- Activation function (AF) = you need this function to plug in your <u>estimated</u> regression model. 
  - <u>**Examples of AF**</u>: 
    - logit, 
    - probit, 
    - relu, 
    - simple "threshold" AF etc...
- Perceptron = Linear Binary Classifier = usually, the perceptron is a <u>linear</u> separator (= line that separates group in a regression) = *Perceptron* is a **single layer neural network**
- Multi-layer perceptron == *Neural Networks*.
  - <u>Example</u>: 
  ![Neural Network with 3 Layers](./bilder/machine-learning/example-of-a-multi-layer-network.png)
- learning rate = how far to go in a particular direction
- features = inputs = independent variables "x" = X<sub>ki</sub>
- labels = this is the "true Y" you observe in the real world = output = dependent variable
- "going downhill" = this is the learning process that you get by using the method "gradient decent" ([look youtube video of user called "the coding train" at ca. 17:30](https://www.youtube.com/watch?v=L-Lsfu4ab74)) & applying a "learning rate" to it
- y<sub>k</sub> = this is the <u>estimated</u> regression function
- z<sub>k</sub> = "logits", e.g. this is the whole *sum of the weights multiplied by the x-variables (= entire regression)*, but this time we **put this entire regression as an _input_ into the logistic function** --> <u>in other words</u>: the same as y<sub>k</sub> but we then apply the specific activation function "logit function" to the estimated z<sub>k</sub> // $\hat{y}$
- Input Layer = Layer 0 = very first set of Neuron
- Output Layer = Last Layer = last set of Neurons
- Hidden Layers = All Layers between the input & output Layer
- input node = nodes at the input layer
- output node = nodes at the output layer
- hidden nodes = nodes, which are in the hidden layers or at the output layer & don't give out outputs
- Feed forward neural networks = connections only between layer i and layer i+1
- Convolutional neural networks = a type of feed-foward network
- Recurrent neural networks = connections flow backwards to previous layers as well
- supervised learning = function estimation
  - <u>There are 2 different types</u>: 
    1) regression,
    2) classification
- unsupervised learning = structure the data into groups (very subjective) // detecting patterns. 
  - Can also be used for:
    1) data reduction,
    2) outlier detection
- loss-function = cost-function = [TRUE y - ESTIMATED $\hat{Y}$]<sup>2</sup> = error --> we define the loss-function be the "least squares"
- identity function = y = x bzw. f(x) = x
- sigmoid function = logit function
- bias term = error term 
  - <u>Note</u>: Oftmals wird der **"input" für den bias term als Zahl "1"** angegeben (siehe Bild oben "Example of Multi-Layer Network", wo der bias term als Zahl "1" angegeben ist.)
- Epoch = means we go through the whole data set once --> default is ten epochs
- Net Input Function = Regressionsmodell als Ganzes = sum of all weighted inputs // "x"
- kernel = (starting) values for the weights
- regularizers = penalties that are used to reduce overfitting (of the starting values for the weights?)
- backpropagation = back pass = when we have our error-term, we can calculate the gradient and - if the error was too big - we can backpass the error-term with the help of the *learning rate* to a previous layer and estimate new // better weights
- breaking symmetry = principle, which says that you need to have different initial weights for hidden units with the same activation function and same inputs
- batches = these are smaller samples that you take from the whole dataset, e.g. you take only a fraction of the dataset  --> you use batches because the computation gets faster rather than putting the whole dataset into the machine
learning "Apparat" --> Rule: the higher the batch size, the better estimates you get
- decay = In Machine Learning, it has become kind of standard to make learning rates dynamic, e.g. first have bigger learning rates, because you can be very wrong at the beginning with the random weights, but then - towards the end of estimation - you adapt the learning rate only very smoothly, since you slowly go towards the optimum --> typically, this decay will make the learning rate smaller as the training continues.
- momentum = makes learning rates dynamic --> If you see that - in the history of gradients - the gradients point generally in the same direction, momentum will adjust the learning rate by increasing the step size
- hyperparameters = <u>examples are</u>: 
  - Learning rate, 
  - Learning Rate Decay, 
  - Momentum, 
  - Batch Size, 
  - Weight / Bias initialization
- Confusion Matrix = C = shows - in the diagonal of the matrix - how many times your predicted outcome was the same as the actual outcome. All the other numbers are saying that your model's prediction was not in line with the actual outcome
- Precision = if i look at a guess // prediction, how many % my algorithm guessed correctly? --> E.g. Anteil der predicted outcome $\hat{y}$, welche korrekt mit den ture outcomes übereinstimmen.
  - <u>Mathematisch ausgedrückt</u>: Im Zähler die Anzahl an übereinstimmenden predicted outcomes $\hat{y}$ & im Nenner Totale Anzahl an predicted Outcomes $\hat{y}$.
- Recall = if i look at an actual true outcome, how many % where guessed correctly? --> E.g. Anteil der true outcome Y, welche korrekt vorhergesagt wurden 
  - <u>Mathematisch ausgedrückt</u>: Im Zähler die Anzahl an korrekt vorhergesehenen true outcomes & im Nenner Totale Anzahl an True Outcomes.
- Training data = Training set is the one on which we train and fit our model basically to fit the parameters.
- Testing data = Testing Data is used only to assess performance of the model.
- Variance = Overfit = you use too much X's // features in your model --> you get too much variance in your predictions
- Bagging = Train the same architecture on different subsets of data
- Boosting = Train different model architectures on the same data
- data augmentation = get more data by adding noise on the input layer
- weight tying = Make the weights similar

#### Alphabetisch sortiert:

Um die Begriffe noch leichter zu finden, habe ich sie hier noch alphabetisch sortiert:

- Activation function = you need this function to plug in your estimated regression model. Examples of AF = logit, probit, relu, simple "threshold" AF etc...
- backpropagation = back pass = when we have our error-term, we can calculate the gradient and - if the error was too big - we can backpass the error-term with 
the helplearning rate to a previous layer and estimate new // better weights
- batches = these are samples from the whole dataset --> you use batches because the computation gets faster rather than putting the whole dataset into the machine
learning "Apparat" --> Rule: the higher the batch size, the better estimates you get
- bias term = error term 
- breaking symmetry = principle, which says that you need to have different initial weights for hidden units with the same activation function and same inputs
- Confusion Matrix = C = shows - in the diagonal of the matrix - how many times your predicted outcome was the same as the actual outcome. All the other numbers are saying that your model's prediction was not in line with the actual outcome
- Convolutional neural networks = a type of feed fowardnetwork
- decay = make learning rates dynamic --> typically, this decay will make the learning rate smaller as the training continues.
- Epoch = means we go through the whole data set once --> default is ten epochs
- features = inputs = independent variables "x" = X(ki)
- Feed forward neural networks = connections only between layer i and layer i+1
- "going downhill" = this is the learning process that you get by using the method "gradient decent" (look youtube video by "the coding train" at ca. 17:30) & applying a "learning rate" to it
- Hidden Layers = All Layers between the input & output Layer
- hidden nodes = nodes, which are in the hidden layers or at the output layer & don't give out outputs
- hyperparameters = examples are: Learning rate, Learning Rate Decay, Momentum, Batch Size, Weight / Bias initialization
- identity function = y = x bzw. f(x) = x
- inputs = Anzahl Observations in Total
- Input Layer = Layer 0 = very first set of Neuron
- input node = nodes at the input layer
- Input units = independent variables "x" 
- kernel = (starting) values for the weights
- labels = this is the "true Y" you observe in the real world = output = dependent variable
- learning rate = how far to go in a particular direction
- loss-function = cost-function = [TRUE y - ESTIMATED Y(hat)]^2 = error --> we define the loss-function be the "least squares"
- Net Input Function = Regressionsmodell als Ganzes = sum of all weighted inputs // "x"
- momentum = makes learning rates dynamic --> If you see that - in the history of gradients - the gradients point generally in the same direction, momentum will adjust the learning rate by increasing the step size
- Output Layer = Last Layer = last set of Neurons
- output node = node at the output layer
- Perceptron = Linear Binary Classifier = linear seperator (= line that separates group in a regression) = Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.
- Precision = if i look at a guess // prediction, how many % my algorithm guessed correctly?
- Recall = if i look at an actual true outcome, how many % where guessed correctly
- Recurrent neural networks = connections flow backwards to previous layers as well 
- regularizers = penalties that are used to reduce overfitting (of the starting values for the weights?)
- sigmoid function = logit function
- saling = pre-processing data 
- supervised learning = function estimation --> 2 different types: 1) regression; 2) classification
- Training data = Training set is the one on which we train and fit our model basically to fit the parameters.
- Testing data = Testing Data is used only to assess performance of the model.
unsupervised learning = structure the data into groups (very subjective) // detecting patterns // data reduction
- weights = Beta-Coefficients = parameters = a, b = neurons
- y(k) = this is the estimated(!!) regresion function
- z(k) = "logits" --> the same as y(k) but we then apply the specific activation function "logit function" to the estimated z(k) // y(hat)

### More ML-Jargon

- `"feed in"` == plug in (--> häufig im Zusammenhang mit Einsetzen von konkreten Werten für die x-Variablen in das geschätzte Modell, um Predictions zu erhalten)
- `instance` == value within a cell == konkreter "x"-Wert, welcher angenommen wird und du - beispielsweise - für eine Prediction verwenden kannst.
- `Forcasted data` == These are the predictions that you do via the help of a model (--> by plugging in concrete x-values), that you've built.
- `Training Dataset` // `Training Set` == This is the sample, that you use to estimate your model.
- `Testing Dataset` // `Test Set` == This is the `hold out` set, which you use at the end, to check whether your model is able to `generalize` // `extrapolate` well to new data.
- `"the model is learning` == what is meant by "learning" is: given some Datenpunkt-Wolke, the computer will try and fit the "best line" [oftentimes by minimizing the sum of the squared residuals, if you use `OLS` (= Ordinary Least Squared)] to construct the "best model" possible.
- `Training` == You are estimating a model on the trainig-dataset, such that the model "learns" from the data.
- `shuffle` == englisches Wort für "mischen". In the context of `splitting the dataset into test- & training-data`, it is common practice to `shuffle` your observations within your dataset first.
	- <u>Reason why you `shuffle`</u>?: Shuffling data serves the purpose of
		- reducing variance AND 
		- making sure that models remain general AND 
		- overfit less.
	- <u>Merke</u>: When dealing with `time series`, you should not use `shuffle` when `splitting the data` into training- &amp; testing-data.
- `training score` // `Test Error` == This error // score is the [oftentimes squared] difference between the `true y-variable` and `estimated // predicted y-variable` and **measures // evaluates how well the model is "performing" // how good the model is on a `test set` with an increasingly bigger `training dataset`**.
- `cross-validation score` // `Validation Error` == This is the (mean) error // score [because of `cross-validation`] that **measures // evaluates how the model is "learning" over increasingly bigger `training datasets`**.
- `fold` == **subset** of the `training dataset`
	- <u>Example</u>: In a `K-Fold Cross-Validation`, you *randomly* split the <u>training set</u> into **10 distinct subsets**, which are called `folds`.
- `samples` == number of rows // number of observations within a dataset.
- `classifiers` == These are simply regression-functions, where the **Y-Variable is binary**, e.g. die Y-Variable kann nur `Y = 0` ODER `Y = 1` als Werte annehmen.
	- <u>Example</u>: Logit- &amp; Probit-Regression, or Naiv Bayes, etc...
- `initialization` // `declaration` // `specification` ==  This is the assignment of an initial value for a data object or variable.
- `Array` == A List of Data is an array. It is a data structure, which contains "n" objects within a list.
	- <u>Quelle</u>: [The Coding Train 3:10-3:22](https://www.youtube.com/watch?v=NptnmWvkbTw&t=3m10s)
- `Extrapolation` == Voraussage von Y-Predict<u>ed</u> Values, welche mittels X-Variablen ermittelt werden, welche <u>zuvor nicht im Datensatz</u> waren.
- `Interpolation` == Voraussage von Y-Predict<u>ed</u> Values, welche mittels X-Variablen ermittelt werden, welche <u>bereits im Datensatz</u> waren, als die Schätzung  getätigt wurde.
- `sliding window` == This is just a way to tell python how to do a cross-validation and have equal lengths of time series where we can learn on.
- `Learning Task` == Forecasting Task // Extrapolation [in Time Series]
- `Forecasting Horizon` == Time Points, you want to predict (= dein ) $\hat{y}$
- `Time Heterogenous Data` == Different Time Series have different time stamps
	- <u>Quelle</u>: Ab 27:05 (Link: http://www.youtube.com/watch?v=Wf2naBHRo8Q&t=27m05s)
- `Seasonal Periodicity` == The number of months per year, in which the forecaster expects to see a seasonal pattern. 
	- <u>Concrete Example</u>: In Philipp's Notebook, he had a seasonal periodicity of 2, e.g. he said that in winter & sommer, he expects a seasonal pattern.
- `Reduction is composable` &#8594; Synonym wäre "addieren" &#8594; E.g. you can split a difficult task into a bunch of smaller tasks (= reduction) and "add" them together to solve the bigger task at the end (= "reduction is composable").
- `Pearson Korrelation` == <u>Lineare</u> Korrelation
- `Grid` &#8594; Das ist nichts anderes als die **Optimierung von diversen "Modell-Parametern"**. 
		- <u>Beispiel</u>: Bei Random-Forrest Modellen kann man zum Beispiel die Tiefe eines Modells bestimmen, dh die relevante Frage, welche ein Forscher sich stellt, ist, *wie viele Baum-Zweigungen die geeignesten Predictions bringen?* Mit Funktionen, wie zum Beispiel `GridSearchCV()` kann dieses Problem geregelt werden.
- `Classifier` == These are simply Regression-Functions, where **the Y-variable is binary**, e.g. die Y-Variable kann nur `Y = 0` <u>oder</u> `Y= = 1` als Werte annehmen. 
  - <u>Beispiele</u>: 
    - Logit-Regression
    - Probit-Regression
    - Naive Bayes
    - etc...
    
## Wörterbuch 

### Data Science

- **bias**:
![This is the definition, Google-Researchers gave to `bias`.](./bilder/home/def-bias-from-google.png)
- **batch**: sample-size when you train your model with a dataset
- **Granularität** (Beispiel) = Angenommen, man möchte wissen, wie viele Tage, Stunden, Minuten und Sekunden innerhalb von 20'044 Sekunden enthalten sind &#8594; hier haben wir also *Granularität von 4* &#8594; <u>Lösung</u>: 4 Tage, 18 Stunden, 37 Minuten, 44 Sekunden
- **Kalibrierung des Modells** == Wie wurde das Modell programmiert im Allgemeinen? &#8594; Dazu gehört - bei uns - die Anpassung der Prognose.
- label == true y-value
- `Pop` == "Pop" is simply a word that means "to drop something". For example, `df.pop('date')` means that I am dropping the column called "data" from a dataframe called "df". Fancily, if you write `df.pop('date')` and *save* it into another variable, then you can only **select** this dropped column from the ddset.
	- <u>Example</u>: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pop.html
- relativ zu == im Verhältnis zu
- shape == dimension
- target == predicted y-value
- Treiber == x-Variablen
- Volume == Speicherplatz
- weights == estimated coefficients

## Quellen 

- https://blog.exxactcorp.com/lets-learn-the-difference-between-a-deep-learning-cnn-and-rnn/
https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/notes/Sonia_Hornik.pdf


