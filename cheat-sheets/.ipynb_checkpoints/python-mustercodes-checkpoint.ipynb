{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"top\" style=\"color:black; text-decoration: none;\">Python Mustercodes</a>\n",
    "---\n",
    "\n",
    "In diesem Dokument habe ich eine **Liste von diversen Python-Codes**, welche mir in meiner Arbeit als Data Scientist behilflich sein könnten. I hope you find what you are looking for =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhaltsverzeichnis\n",
    "\n",
    "```\n",
    "<a id=\"gebe-hier-passenden-id-namen\" style=\"color:black; text-decoration: none;\"></a>\n",
    "```\n",
    "\n",
    "- [List Comprehensions](#list-compr)\n",
    "- [String Manipulation](#str-manip)\n",
    "- [Package Installation](#package-install)\n",
    "    - [De-Install Packages with `pip`](#deinstall-pip)\n",
    "- [Help](#help)\n",
    "- [Path handling](#path)\n",
    "- [Save Data](#saving)\n",
    "    - [... as `.CSV`](#save-csv)\n",
    "- [Read Data](#read-data)\n",
    "    - [Load `Excel`](#load-excel)\n",
    "    - [Load `.CSV`](#load-csv)\n",
    "    - [... via a Website's URL](#load-url)\n",
    "    - [Load Pickle](#load-pickle)\n",
    "- [Vanilla Data Exploration](#vanilla-exploration)\n",
    "    - [Number of Rows](#number-of-rows)\n",
    "    - [View whole Dataset](#view-all-ddset)\n",
    "    - [View 5 first observations in the Dataset](#view-first-five)\n",
    "    - [View 5 last observations in the Dataset](#view-last-five)\n",
    "    - [Number of Missings per Column within a dataset](#missings-per-column)\n",
    "    - [Find the `Data Type` of a Variable](#find-data-type)\n",
    "    - [Drop 1 OR multiple Columns](#drop-columns)\n",
    "    - [Re-Name 1 OR multiple Columns](#rename-columns)\n",
    "    - [`.reset_index()` and `.set_index()`](#reset-and-set-idx)\n",
    "    - [Summary of a Dataset](#summary-ddset)\n",
    "    - [Show only `unique()` Columns](#unique-values)\n",
    "    - [Median &amp; Mean for a particular Column](#median-and-mean)\n",
    "    - [Apply a Fustom-Function `f(x)`, but only on certain rows within a Dataframe via `numpy`](#apply-a-function-based-on-cond)\n",
    "    - [Transform a Series to a Dataframe](#trans-series-to-frame)\n",
    "    - [Assign a `unique ID`](#unique-id)\n",
    "- [Selection &amp; Filtering of Rows &amp; Columns](#sel-and-filt)\n",
    "    - [Select 1 Column in a Dataframe](#select-1-col-ddset)\n",
    "        - [Example with \"dot-notation\", e.g. `df.a_Column`](#dot-not)\n",
    "    - [Select multiple Columns in a Dataframe](#select-multi-col-ddset)\n",
    "    - [Select AND Filter for Rows with _1 Condition_ within Dataframe](#filter-1-row-ddset)\n",
    "    - [Select AND Filter for Rows with _multiple Conditions_ within a Dataframe](#filter-multi-cond-ddset)\n",
    "    - [Select only Rows with Missing-Values](#select-only-missings)\n",
    "    - [Selection with \"Accessor Operators\" `loc` &amp; `iloc`](#sel-loc-iloc)\n",
    "        - [Example: Select 1st Row and 5th Column with `iloc`](#example-iloc-sel)\n",
    "        - [Example: Select 1st Row and 5th Column with `loc`](#example-loc-sel)\n",
    "        - [Example: Select with a _Threshold-Value_ via `loc`](#example-sel-with-threshold-loc)\n",
    "        - [Example: Select _entire Rows_ via `iloc`](#example-sel-entire-row-with-iloc)\n",
    "        - [Example: Select _entire Rows_ via `loc`](#example-sel-entire-row-with-loc)\n",
    "    - [Replace Values within Rows](#replace-values-within-rows)\n",
    "    - [Slicing is different when using `loc` or `iloc`](#different-slicing-loc-and-iloc)\n",
    "- [Sorting &amp; Filtering](#sort-and-filt)\n",
    "    - [Sorting (Pearson) Correlation _from most-important to least-important](#sort-pearson-corr-from-most-imp-to-least-imp)\n",
    "        - [Example: Select only highest Correlations, then Sort them and `print()` them](#example-of-sort-corr)\n",
    "- [Dealing with Missings](#dealing-with-missings)\n",
    "    - [Count total Number of Missings within a Dataframe](#count-total-missings-ddset)\n",
    "    - [Count Missings per Columns](#count-missings-per-columns)\n",
    "    - [Count _Non_-Missings, but _ONLY IF_ you don't have the value `0` in it](#count-non-missings-only-if-no-0)\n",
    "    - [Show only Rows with missings](#show-only-rows-with-missings)\n",
    "    - [Replace all Missings with 0's](#replace-missings-with-0)\n",
    "    - [Missing-Imputations](#missing-imputations)\n",
    "        - [Missing-Interpolation for Time-Series](#interpolation-for-time-series)\n",
    "- [Duplicate Data](#data-duplicates)\n",
    "    - [Count _all_ Duplicates in the Dataframe](#count-all-duplicates)\n",
    "    - [Drop Duplicates](#drop-duplicates)\n",
    "- [Outliers](#outliers)\n",
    "    - [Draw a Boxplot](#draw-boxplot)\n",
    "- [Merging &amp; Splitting](#merging-and-splitting)\n",
    "    - [Split bigger Dataset by Categories into Subsets](#split-big-ddset-by-categories)\n",
    "    - [Merging 2 Dataframes by their (row) index](#merging-by-idx)\n",
    "    - [Merging _multiple_ Datasets _simultaneously](#merge-multiple-ddset)\n",
    "- [Advanced Data Exploration](#advanced-exploration)\n",
    "    - [Create a Correlation-Matrix from a Dataframe](#create-correlation-matrix)\n",
    "- [Time Series Exploration](#time-series-exploration)\n",
    "    - [Plot Time-Series](#plot-time-series)\n",
    "    - [Autocorrelation &amp; Partial-Autocorrelation](#auto-and-partial-auto-corr)\n",
    "    - [Cross-Correlation](#cross-corr)\n",
    "    - [Feature Engineering](#feature-engineering-time-series)\n",
    "        - [Creating a Time-Index](#time-stamps-as-index)\n",
    "        - [Copy Index as a Column](#copy-index-as-columns)\n",
    "        - [Transform a \"Pseudo\"-Time-Column into a `Date-Time`-Column and set it as the index](#make-datetime-and-set-index)\n",
    "        - [Create a `range()` of Dates](#create-range-of-dates)\n",
    "        - [Rechnen (Addition &amp; Substraktion) mit `Date-Time`](#rechnen-mit-date-time-objects)\n",
    "            - [Genauer Zeitpunkt \"jetzt\" (Heute // `Today`) herausfinden](#today)\n",
    "            - [Zukunft relativ zu \"jetzt\"](#zukunft-relativ-zu-jetzt)\n",
    "            - [Subtraktion, um vergangenen Zeitpunkt zu berechnen](#subtraktion-von-zeitpunkt)\n",
    "            - [Herausfinden, welcher Wochentag (Mo / Di / Mi etc...) ein bestimmtes Datum war](#wochentag-herausfinden)\n",
    "        - [Create Dummy-Variables for Time-Variables](#dummy-variables-for-time-variables)\n",
    "            - [Create \"normal\" Dummies](#create-normal-dummies)\n",
    "            - [\"Flagg\"-Dummy by Merging](#flagg-dummy-by-merging)\n",
    "            - [Dummy for Hours](#dummy-for-hours)\n",
    "            - [Dummy for Quarters](#dummy-for-quarters)\n",
    "            - [Dummy for Days](#dummy-for-days)\n",
    "- [Scaling Variables](#scaling-variables)\n",
    "    - [Normalization](#normalization-scaling)\n",
    "    - [Series to Array](#series-to-array)\n",
    "- [Data Visualization](#data-viz)\n",
    "    - [Plot and Save Image](#plot-and-save-image)\n",
    "    - [_Multiple_ Plots with For-Loops](#multi-plots-for-loop)\n",
    "- [Umwandlungen von `Data Types` in einen anderen `Data Type`](#umwandlungen)\n",
    "    - [Convert Series to Array](#series-to-array)\n",
    "- [Useful \"tricks\" I stumbled upon](#useful-tricks)\n",
    "    - [`Reshape(-1,1)` to create a list _within_ a list](#reshape-1-1)\n",
    "- [Math-Tricks for \"Hacks\"](#math-tricks)\n",
    "    - [Round Downwards](#round-downwards)\n",
    "    - [Calculate the Median](#calc-median)\n",
    "- [Python Magic Commands](#magic-commands)\n",
    "    - [View all Columns](#view-all-columns)\n",
    "    - [Working with HTML within Juypter-Notebooks](#html-with-jupyter-notebooks)\n",
    "    - [Share Variables between Notebooks](#share-variables-between-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u><a id=\"list-compr\" style=\"color:black; text-decoration: none;\">List Comprehensions</a></u>:\n",
    "\n",
    "List Comprehensions are a concept that is on of `Python`'s most beloved and unique features. It's basically a loop where you apply a function to each of the elements during the loop. Its Output is a `list`. \n",
    "\n",
    "- <u>**Examples**</u>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>List comprehension where we apply a `function`</u>: x<sup>2</sup>\n",
    "\n",
    "List comprehensions are really nice if you want to iterate over a column and apply a function on a particular column of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares = [n**2 for n in range(10)] # apply f(x) = x^2 on a list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(squares)) # Check output: should be a 'list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>List comprehension with an `if` condition</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Venus', 'Earth', 'Mars']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planets = ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune'] # this is the list we will loop through\n",
    "\n",
    "short_planets = [planet for planet in planets if len(planet) < 6]\n",
    "short_planets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u><a id=\"str-manip\" style=\"color:black; text-decoration: none;\">String Manipulation</a></u>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Transform a `list` that contains strings into a WHOLE string, only separated by commas</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercury,Venus,Earth,Mars,Jupiter,Saturn,Uranus,Neptune\n"
     ]
    }
   ],
   "source": [
    "# step 0: create a list with many strings in it\n",
    "planets = ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune']\n",
    "\n",
    "# Step 1: Transform the whole list into a single \"string\"\n",
    "str(planets) # should output: \"['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune']\"\n",
    "\n",
    "# Step 2: Now, we replace all characters that are unnecessary - such as ' [ and ] -such that we return a whole string,\n",
    "        # only separated by commas, with no whitespace in between them:\n",
    "n = str(planets).replace(\"'\", \"\").replace('[', '').replace(']', '').replace(' ', '') # replace everthing by empty-strings\n",
    "print(n) # Final output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"package-install\" style=\"color:black; text-decoration: none;\">Installation of Packages</a>\n",
    "---\n",
    "\n",
    "### <u>Installing with package-manager `conda`</u>:\n",
    "\n",
    "`Conda` is a package manager, which will - before starting to install a package that you want - check which dependencies are needed for the package to be able to be used. \n",
    "\n",
    "- <u>Example</u>: If you use `sktime`, then `conda` can detect that there ma be a conflict with the _current version_ of - for example - the `package Tornado`. *Hence, `conda` will not only download `sktime`, but will also bring the package `Tornado` onto a newer version, such tht it will become compatible with `sktime`.\n",
    "- <u>Note</u>: The installation via `conda` can take a while, since many dependencies will be checked!\n",
    "- <u>Improtant</u>: Use `the terminal` for the following code.\n",
    "\n",
    "#### To install a specific package - for example - `sktime` into an existing virtual-environment called `\"myenv\"`, type the following into the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install --name myenv sktime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the package `sktime` into the current (global) environment (= dh \"normally\"), type the following into the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install sktime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Installing packages with `pip`</u>:\n",
    "\n",
    "Der **Nachteil** von `pip install [some package-name here]` VS. `conda install [some package-name here]` liegt darin, dass `pip` <u>keine</u> Checks macht, ob die Versionen von verschiedenen Packages, die man verwendet, überhaupt kompatibel sind.\n",
    "\n",
    "#### Alternative Installation, by using `pip` within the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sktime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Install some Packages // Modules, or Sub-Modules</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import date2num\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u><a id=\"deinstall-pip\" style=\"color:black; text-decoration: none;\">Deinstall Packages mit `Pip`</a></u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall fbprophet prophet cmdstanpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"help\" style=\"color:black; text-decoration: none;\">Need help & documentation</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to Python 3.8's help utility!\n",
      "\n",
      "If this is your first time using Python, you should definitely check out\n",
      "the tutorial on the Internet at https://docs.python.org/3.8/tutorial/.\n",
      "\n",
      "Enter the name of any module, keyword, or topic to get help on writing\n",
      "Python programs and using Python modules.  To quit this help utility and\n",
      "return to the interpreter, just type \"quit\".\n",
      "\n",
      "To get a list of available modules, keywords, symbols, or topics, type\n",
      "\"modules\", \"keywords\", \"symbols\", or \"topics\".  Each module also comes\n",
      "with a one-line summary of what it does; to list the modules whose name\n",
      "or summary contain a given string such as \"spam\", type \"modules spam\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"path\" style=\"color:black; text-decoration: none;\">Path Handling</a>\n",
    "---\n",
    "\n",
    "*Sehr oft wirst du Modelle, die eventuell stundenlang gelernt haben oder Daten-Files etc... laden & speichern müssen. Hierfür ist es von hoher Bedeutung, dass du gut mit Pfaden umgehen kannst, insbesondere mit der Library `os`!*\n",
    "\n",
    "Zwei Haupt-Formate, die du verwenden wirst für das Abspeichern &amp; Laden von Files, sind:\n",
    "\n",
    "- Das `.csv`-Format.\n",
    "- Das `.pkl`-Format.\n",
    "- etc... Es gibt noch viele andere Formate, aber die obigen beiden sind die wichtigsten, denen ich bisher begegnet bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Set the Path, where your Computer should Save Data / Models</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jomaye/Dokumente/Programming'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data // Load data\n",
    "---import os # this is the library that can handle paths\n",
    "\n",
    "save_folder = os.path.expanduser(os.path.join(\"~\", # our User's \"home\"-directory --> for my Mac it is: \"/Users/jomaye\" \n",
    "                                              \"Dokumente\", # Next, we jump 1 directory called \"Dokumente\" further below\n",
    "                                              \"Programming\")) # Allgemeint: each string after a Komma is a new directory you can set. This can go on infinitively ;)\n",
    "\n",
    "\n",
    "save_folder # check if it worked? --> yes! Now you see your path =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"saving\" style=\"color:black; text-decoration: none;\">Save data</a>\n",
    "---\n",
    "\n",
    "### <u><a id=\"save-csv\" style=\"color:black; text-decoration: none;\">Save as CSV-File</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_name = \"name_of_a_specific_characteristic_of_the_variable_that_you_want_to_save\" # this is needed for \"dynamisches speichern\" via F-String  \n",
    "YOUR_VAR_NAME.to_csv(f\"Name-of-the-CSV-{dynamic_name}.csv\") # I use f-strings since it allows me to adapt the CSV-filenames to a specific characteristic, that I used for a model / method etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"read-data\" style=\"color:black; text-decoration: none;\">Read data // Load data</a>\n",
    "---\n",
    "\n",
    "In order to load datasets, you will need the library `pandas`. For *some* cases, additional libraries may be needed.\n",
    "\n",
    "### <a id=\"load-excel\" style=\"color:black; text-decoration: none;\"><u>Load data from Excel-Sheets</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epex_df = pd.read_excel(\"./Data_V2/Preis_aktuell_Spot_EEX_CH-19-ver-mac.xlsx\", # plug in the correct path\n",
    "                        header=[1], # The dataset Column-names beginnt ab 2. Zeile (--> 1. Zeile ist der Titel des Excel-Files)\n",
    "                        sheet_name='Prices', # If you have more than 1 Excel-Sheet within the Excel-File, you need to specify\n",
    "                        # which sheet you want to load\n",
    "                        engine='openpyxl') # This input will (sometimes) be needed if you load data from an Excel-File via \n",
    "                                           # a Windows-Computer, otherwise it can print an error!\n",
    "epex_df # output the ddset a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"load-csv\" style=\"color:black; text-decoration: none;\"><u>Load Dataset via a CSV-File</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\n",
    "    \"C:/Users/u235051/Downloads/ETS_Database_v38/ETS_Database_v38.csv\", \n",
    "    sep='\\t' # Im CSV-File waren die Spalten via \"Tab\"-Taste separiert, deshalb diese option zwingend anzugeben ist (ansonsten Error!)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>country_code</th>\n",
       "      <th>ETS information</th>\n",
       "      <th>main activity sector name</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>BE</td>\n",
       "      <td>2. Verified emissions</td>\n",
       "      <td>35 Production of pulp</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>102581.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>BE</td>\n",
       "      <td>2. Verified emissions</td>\n",
       "      <td>35 Production of pulp</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>106671.0</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>BE</td>\n",
       "      <td>2. Verified emissions</td>\n",
       "      <td>35 Production of pulp</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>126702.0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>BE</td>\n",
       "      <td>4. Total surrendered units</td>\n",
       "      <td>35 Production of pulp</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>98349.0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>BE</td>\n",
       "      <td>4. Total surrendered units</td>\n",
       "      <td>35 Production of pulp</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>96708.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57389</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>HU</td>\n",
       "      <td>2. Verified emissions</td>\n",
       "      <td>10 Aviation</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>12599748.0</td>\n",
       "      <td>Total 3rd trading period (13-20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57390</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>HU</td>\n",
       "      <td>1.1.3 Free allocation for modernisation of ele...</td>\n",
       "      <td>23 Metal ore roasting or sintering</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Total 3rd trading period (13-20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57391</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>HU</td>\n",
       "      <td>1.1.3 Free allocation for modernisation of ele...</td>\n",
       "      <td>30 Production of lime, or calcination of dolom...</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Total 3rd trading period (13-20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57392</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>HU</td>\n",
       "      <td>4. Total surrendered units</td>\n",
       "      <td>43 Production of hydrogen and synthesis gas</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>883972.0</td>\n",
       "      <td>Total 3rd trading period (13-20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57393</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>IE</td>\n",
       "      <td>1.1.3 Free allocation for modernisation of ele...</td>\n",
       "      <td>10 Aviation</td>\n",
       "      <td>tonne of CO2 equ.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Total 3rd trading period (13-20)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57394 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       country country_code  \\\n",
       "0      Belgium           BE   \n",
       "1      Belgium           BE   \n",
       "2      Belgium           BE   \n",
       "3      Belgium           BE   \n",
       "4      Belgium           BE   \n",
       "...        ...          ...   \n",
       "57389  Hungary           HU   \n",
       "57390  Hungary           HU   \n",
       "57391  Hungary           HU   \n",
       "57392  Hungary           HU   \n",
       "57393  Ireland           IE   \n",
       "\n",
       "                                         ETS information  \\\n",
       "0                                  2. Verified emissions   \n",
       "1                                  2. Verified emissions   \n",
       "2                                  2. Verified emissions   \n",
       "3                             4. Total surrendered units   \n",
       "4                             4. Total surrendered units   \n",
       "...                                                  ...   \n",
       "57389                              2. Verified emissions   \n",
       "57390  1.1.3 Free allocation for modernisation of ele...   \n",
       "57391  1.1.3 Free allocation for modernisation of ele...   \n",
       "57392                         4. Total surrendered units   \n",
       "57393  1.1.3 Free allocation for modernisation of ele...   \n",
       "\n",
       "                               main activity sector name               unit  \\\n",
       "0                                  35 Production of pulp  tonne of CO2 equ.   \n",
       "1                                  35 Production of pulp  tonne of CO2 equ.   \n",
       "2                                  35 Production of pulp  tonne of CO2 equ.   \n",
       "3                                  35 Production of pulp  tonne of CO2 equ.   \n",
       "4                                  35 Production of pulp  tonne of CO2 equ.   \n",
       "...                                                  ...                ...   \n",
       "57389                                        10 Aviation  tonne of CO2 equ.   \n",
       "57390                 23 Metal ore roasting or sintering  tonne of CO2 equ.   \n",
       "57391  30 Production of lime, or calcination of dolom...  tonne of CO2 equ.   \n",
       "57392        43 Production of hydrogen and synthesis gas  tonne of CO2 equ.   \n",
       "57393                                        10 Aviation  tonne of CO2 equ.   \n",
       "\n",
       "            value                              year  \n",
       "0        102581.0                              2009  \n",
       "1        106671.0                              2011  \n",
       "2        126702.0                              2012  \n",
       "3         98349.0                              2007  \n",
       "4         96708.0                              2008  \n",
       "...           ...                               ...  \n",
       "57389  12599748.0  Total 3rd trading period (13-20)  \n",
       "57390         0.0  Total 3rd trading period (13-20)  \n",
       "57391         0.0  Total 3rd trading period (13-20)  \n",
       "57392    883972.0  Total 3rd trading period (13-20)  \n",
       "57393         0.0  Total 3rd trading period (13-20)  \n",
       "\n",
       "[57394 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"load-url\" style=\"color:black; text-decoration: none;\"><u>Load Dataset via a URL from a Website</u></a>:\n",
    "\n",
    "In order to be able to **access a dataset stored as a csv-file on a website**, we will need to use - *besides* `pandas` - an additional library called `requests`. \n",
    "\n",
    "#### <u>Step 1</u>: \n",
    "\n",
    "Make the file ready to be downloaded be sending a query to the remote-server // website that hosts the csv-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download ready.\n"
     ]
    }
   ],
   "source": [
    "import requests # load the library needed\n",
    "\n",
    "download_url = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/nba-elo/nbaallelo.csv\" # absolute URL\n",
    "target_csv_path = \"nba_all_elo.csv\" # name of the .csv-file that contains the data\n",
    "\n",
    "response = requests.get(download_url) # using an API that \"gets\" (= http-protocol language) the data from the server\n",
    "response.raise_for_status()    # Check that the request was successful\n",
    "with open(target_csv_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "print(\"Download ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Step 2</u>:\n",
    "\n",
    "Load the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # load the library needed\n",
    "\n",
    "nba = pd.read_csv(\"nba_all_elo.csv\") # load the data --> ddset is called 'nba'\n",
    "\n",
    "type(nba) # check if it worked? --> should output: <class 'pandas.core.frame.DataFrame'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameorder</th>\n",
       "      <th>game_id</th>\n",
       "      <th>lg_id</th>\n",
       "      <th>_iscopy</th>\n",
       "      <th>year_id</th>\n",
       "      <th>date_game</th>\n",
       "      <th>seasongame</th>\n",
       "      <th>is_playoffs</th>\n",
       "      <th>team_id</th>\n",
       "      <th>fran_id</th>\n",
       "      <th>...</th>\n",
       "      <th>win_equiv</th>\n",
       "      <th>opp_id</th>\n",
       "      <th>opp_fran</th>\n",
       "      <th>opp_pts</th>\n",
       "      <th>opp_elo_i</th>\n",
       "      <th>opp_elo_n</th>\n",
       "      <th>game_location</th>\n",
       "      <th>game_result</th>\n",
       "      <th>forecast</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>194611010TRH</td>\n",
       "      <td>NBA</td>\n",
       "      <td>0</td>\n",
       "      <td>1947</td>\n",
       "      <td>11/1/1946</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>TRH</td>\n",
       "      <td>Huskies</td>\n",
       "      <td>...</td>\n",
       "      <td>40.294830</td>\n",
       "      <td>NYK</td>\n",
       "      <td>Knicks</td>\n",
       "      <td>68</td>\n",
       "      <td>1300.0000</td>\n",
       "      <td>1306.7233</td>\n",
       "      <td>H</td>\n",
       "      <td>L</td>\n",
       "      <td>0.640065</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>194611010TRH</td>\n",
       "      <td>NBA</td>\n",
       "      <td>1</td>\n",
       "      <td>1947</td>\n",
       "      <td>11/1/1946</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NYK</td>\n",
       "      <td>Knicks</td>\n",
       "      <td>...</td>\n",
       "      <td>41.705170</td>\n",
       "      <td>TRH</td>\n",
       "      <td>Huskies</td>\n",
       "      <td>66</td>\n",
       "      <td>1300.0000</td>\n",
       "      <td>1293.2767</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>0.359935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>194611020CHS</td>\n",
       "      <td>NBA</td>\n",
       "      <td>0</td>\n",
       "      <td>1947</td>\n",
       "      <td>11/2/1946</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CHS</td>\n",
       "      <td>Stags</td>\n",
       "      <td>...</td>\n",
       "      <td>42.012257</td>\n",
       "      <td>NYK</td>\n",
       "      <td>Knicks</td>\n",
       "      <td>47</td>\n",
       "      <td>1306.7233</td>\n",
       "      <td>1297.0712</td>\n",
       "      <td>H</td>\n",
       "      <td>W</td>\n",
       "      <td>0.631101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>194611020CHS</td>\n",
       "      <td>NBA</td>\n",
       "      <td>1</td>\n",
       "      <td>1947</td>\n",
       "      <td>11/2/1946</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NYK</td>\n",
       "      <td>Knicks</td>\n",
       "      <td>...</td>\n",
       "      <td>40.692783</td>\n",
       "      <td>CHS</td>\n",
       "      <td>Stags</td>\n",
       "      <td>63</td>\n",
       "      <td>1300.0000</td>\n",
       "      <td>1309.6521</td>\n",
       "      <td>A</td>\n",
       "      <td>L</td>\n",
       "      <td>0.368899</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>194611020DTF</td>\n",
       "      <td>NBA</td>\n",
       "      <td>0</td>\n",
       "      <td>1947</td>\n",
       "      <td>11/2/1946</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>DTF</td>\n",
       "      <td>Falcons</td>\n",
       "      <td>...</td>\n",
       "      <td>38.864048</td>\n",
       "      <td>WSC</td>\n",
       "      <td>Capitols</td>\n",
       "      <td>50</td>\n",
       "      <td>1300.0000</td>\n",
       "      <td>1320.3811</td>\n",
       "      <td>H</td>\n",
       "      <td>L</td>\n",
       "      <td>0.640065</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gameorder       game_id lg_id  _iscopy  year_id  date_game  seasongame  \\\n",
       "0          1  194611010TRH   NBA        0     1947  11/1/1946           1   \n",
       "1          1  194611010TRH   NBA        1     1947  11/1/1946           1   \n",
       "2          2  194611020CHS   NBA        0     1947  11/2/1946           1   \n",
       "3          2  194611020CHS   NBA        1     1947  11/2/1946           2   \n",
       "4          3  194611020DTF   NBA        0     1947  11/2/1946           1   \n",
       "\n",
       "   is_playoffs team_id  fran_id  ...  win_equiv  opp_id  opp_fran  opp_pts  \\\n",
       "0            0     TRH  Huskies  ...  40.294830     NYK    Knicks       68   \n",
       "1            0     NYK   Knicks  ...  41.705170     TRH   Huskies       66   \n",
       "2            0     CHS    Stags  ...  42.012257     NYK    Knicks       47   \n",
       "3            0     NYK   Knicks  ...  40.692783     CHS     Stags       63   \n",
       "4            0     DTF  Falcons  ...  38.864048     WSC  Capitols       50   \n",
       "\n",
       "   opp_elo_i  opp_elo_n  game_location  game_result  forecast notes  \n",
       "0  1300.0000  1306.7233              H            L  0.640065   NaN  \n",
       "1  1300.0000  1293.2767              A            W  0.359935   NaN  \n",
       "2  1306.7233  1297.0712              H            W  0.631101   NaN  \n",
       "3  1300.0000  1309.6521              A            L  0.368899   NaN  \n",
       "4  1300.0000  1320.3811              H            L  0.640065   NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"load-pickle\" style=\"color:black; text-decoration: none;\"><u>Load Pickle-Files</u></a>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Put_Your_Pickle-File_Name_Here\" # Alternativ: os.path.join(data_folder_variable_where_pkl_is_saved, filename)\n",
    "test_loaded_pkl =  pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"vanilla-exploration\" style=\"color:black; text-decoration: none;\">Vanilla Exploration of your Dataset</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>After</u> having told Python to read in your dataset, the **first thing** you will want to do is to <mark>get (very) familiar with your dataset</mark>. This is **key**, otherwise you will <u>not</u> be able to perform a good data analysis!\n",
    "\n",
    "### <a id=\"number-of-rows\" style=\"color:black; text-decoration: none;\"><u>Find out the number of `observations` // rows in your dataset</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nba) # to get the number of observations // rows \n",
    "\n",
    "    # note: 'nba' is the name of the ddset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Find out the number of `rows` &amp; `columns` within your dataset</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.shape # to get number of rows AND columns\n",
    "\n",
    "    # note: 'nba' is the name of the ddset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"view-all-ddset\" style=\"color:black; text-decoration: none;\"><u>`View` the <mark>whole</mark> Dataset</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba # just type in the name of the variable in which your dataframe is stored in --> 'nba' is the name of your dataframe here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"view-first-five\" style=\"color:black; text-decoration: none;\"><u>`View` the first 5-rows of your dataset</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.head() # head() is often used to check whether your dataset really contains data you care about \n",
    "\n",
    "    # here we check: does the ddset really contains data about the NBA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Achtung</u>**: If you have alot of columns, Python will not display them all when you use `head()`. <u>However</u> you can change the settings via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.columns\", None) # this will tell Python: \"show me ALL the columns!\"\n",
    "\n",
    "nba.head() # execute 'head()' again to check if the setting changed correclty? --> yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"view-last-five\" style=\"color:black; text-decoration: none;\">You can also `View` the 5 last rows of your dataset by using `tails()`</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.tail() # View last 5 rows\n",
    "\n",
    "# Viewing very specific rows is also possible: \n",
    "nba.tail(3) # Here, we view the last 3 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"missings-per-column\" style=\"color:black; text-decoration: none;\"><u>Find out the `Data Type` of each column within your dataset &amp; number of `non-missing values`</u></a>:\n",
    "\n",
    "With the following simple code, we can find out, whether the columns are from the type of an `integer`, a `string`, a `boolean` etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.info() # this will output all the types of each column in your dataset & how many NON-missings you have per column\n",
    "\n",
    "    # note: Pay attention to any columns that are from the type 'object'! --> lese bemerkung unten..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To get a beautiful overview over all types that exist in Python, I recommend you to visit the website of W3Schools.com](https://www.w3schools.com/python/python_datatypes.asp)\n",
    "\n",
    "**<u>Bemerkung hier</u>**: It can be, that columns are from the type `object`. In practice, it often means that *all* of the values in an `object`-column are <u>strings</u>. If you encounter any `object`-columns, it is strongly recommended that you convert them into a more apropriate `data-type`, otherwise some of the functions won't work on these `object`-columns..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"find-data-type\" style=\"color:black; text-decoration: none;\">Find the `type` of *any* object in Python</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"drop-columns\" style=\"color:black; text-decoration: none;\"><u>Drop specific Columns</u></a>:\n",
    "\n",
    "Note that `axis = 1` denotes the columns that will be droped, while `axis = 0` (default), will denote the rows that should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy = df_energy.drop(['tInfoDaySin', 'tInfoDayCos', 'tInfoYearSin', 'tInfoYearCos'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"rename-columns\" style=\"color:black; text-decoration: none;\"><u>Re-Name the Columns of a DataFrame</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.rename({'oldName1': 'newName1', 'oldName2': 'newName2'}, axis='columns') # not all columns have to be renamed, only those with a new name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"reset-and-set-idx\" style=\"color:black; text-decoration: none;\"><u>Set &amp; Reset the Index / Row-Label</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy = df_energy.set_index('Date-Time') # to set the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy = df_energy.reset_index() # to reset the index --> df.reset_index(drop= True) will drop the index, which would \n",
    "                                    # otherwise become a new column instead of just dropping it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"summary-ddset\" style=\"color:black; text-decoration: none;\"><u>Make a `Summary` out of all the variables in your dataset</u></a>:\n",
    "\n",
    "Note that, in order to be able to do some **summary-statistics**, you will need the additional library `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # load numpy for summary-statistics\n",
    "\n",
    "nba.describe().round(2) # results will be rounded onto 2 digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Achtung</u>**: If you have columns from the type `object`, you will need a slightly different version of the `describe()` function to display some summary-statistics from such columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.describe(include=object) # if you have some weird columns being of the type 'object'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"unique-values\" style=\"color:black; text-decoration: none;\"><u>Find the unique values from a column</u></a>:\n",
    "\n",
    "This can be useful, when you want to filter all unique `categories` within a column. You can then put all those categories wihtin a new variable in order to loop through them to apply some function to those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gapminder['continent'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"median-and-mean\" style=\"color:black; text-decoration: none;\"><u>Calculating the `mean` or `median` from a particular column</u></a>:\n",
    "\n",
    "For the <u>**mean**</u>, you will need the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_points = reviews.points.mean() # calculate the mean of the column 'points' within the ddset 'reviews'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>For the **median**</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_points = reviews.points.median() # calculate the median of the column 'points' within the ddset 'reviews'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"apply-a-function-based-on-cond\" style=\"color:black; text-decoration: none;\"><u>Transform a column based on conditions with `numpy`</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2., nan, nan, nan, 36., 49., 64., 81.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # In order to be able to perform a transformation, we will need the `numpy`-package\n",
    "\n",
    "# set-up:\n",
    "x = np.arange(10) # this is our column 'x'\n",
    "condlist = [x<3, x>5] # set of conditions, that need to be fullfilled --> which are the value-ranges, on which you will apply \n",
    "# a custom-function [which will be defined next]? --> all numbers below 3 AND all numbers above 5\n",
    "choicelist = [x, x**2] # the custom-function you will apply here: x^2\n",
    "\n",
    "# output:\n",
    "np.select(condlist, choicelist, default=np.nan) # apply x^2 on: x < 3 AND x > 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"trans-series-to-frame\" style=\"color:black; text-decoration: none;\"><u>Transform a Pandas Series into a DataFrame</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vals\n",
       "0    a\n",
       "1    b\n",
       "2    c"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s = pd.Series([\"a\", \"b\", \"c\"],\n",
    "              name=\"vals\")\n",
    "s.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"unique-id\" style=\"color:black; text-decoration: none;\"><u>Assign a unique ID</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df.groupby(['LastName','FirstName']).ngroup() # here, we use the column 'LastName' & 'FirstName' together, to create \n",
    "# a unique ID.\n",
    "\n",
    "# Quelle: https://stackoverflow.com/questions/45685254/q-pandas-how-to-efficiently-assign-unique-id-to-individuals-with-multiple-ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df.groupby(['date']).ngroup() # if Course, you could also simply use 1 column fo the assignment of an unique ID. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sel-and-filt\" style=\"color:black; text-decoration: none;\">Selection &amp; Filtering Rows and Columns</a>\n",
    "---\n",
    "\n",
    "When you will work wit dataframes in `pandas`, one of the most important things you will need to master is how to select some columns, as well as print out `subsets` // particular columns from the dataframe.\n",
    "\n",
    "To get started and become acquainted with common techniques, I recommend you to watch this [beginner tutorial for filtering &amp; selecting columns](https://www.youtube.com/watch?v=htyWDxKVttE).\n",
    "\n",
    "**Load the next cell to be able to run the examples that follow**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>items_ordered</th>\n",
       "      <th>order_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>12/1/17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>65.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>12/1/17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>25.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>12/1/17</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>20.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>12/1/17</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>12/1/17</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id order_date  customer_id  items_ordered  order_total\n",
       "0      1000    12/1/17            1              3        65.00\n",
       "1      1001    12/1/17            3              2        25.75\n",
       "2      1002    12/1/17            6              1        20.75\n",
       "3      1003    12/1/17            8              6        86.00\n",
       "4      1004    12/1/17            8              1        30.00"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # load package needed\n",
    "\n",
    "file_path = './data/filter-and-selection/sample_orders.csv' # type in the path-location of your data\n",
    "dd = pd.read_csv(file_path) # load the data\n",
    "\n",
    "dd.head() # check if it worked? --> yes! --> should print the first 5 rows of your ddset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16 entries, 0 to 15\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   order_id       16 non-null     int64  \n",
      " 1   order_date     16 non-null     object \n",
      " 2   customer_id    16 non-null     int64  \n",
      " 3   items_ordered  16 non-null     int64  \n",
      " 4   order_total    16 non-null     float64\n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 768.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "dd.info() # check also all data-types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"select-1-col-ddset\" style=\"color:black; text-decoration: none;\"><u>Select only 1 single column of your Dataframe</u></a>:\n",
    "\n",
    "In this example, I will select the column `order_id` from my ddset `dd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1000\n",
      "1     1001\n",
      "2     1002\n",
      "3     1003\n",
      "4     1004\n",
      "5     1005\n",
      "6     1006\n",
      "7     1007\n",
      "8     1008\n",
      "9     1009\n",
      "10    1010\n",
      "11    1011\n",
      "12    1012\n",
      "13    1013\n",
      "14    1014\n",
      "15    1015\n",
      "Name: order_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dd['order_id'] # take the dataframe 'dd' and print me only the column called 'order_id' --> this is a subset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"dot-not\" style=\"color:black; text-decoration: none;\"><u>Möglichkeit 2</u>: Selection of columns with `dot-notation`</a>\n",
    "\n",
    "*Nachteil* dieser Methode ist, dass sie <u>nicht</u> funktioniert, wenn es in den Columns `Leerschläge` gibt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1000\n",
      "1     1001\n",
      "2     1002\n",
      "3     1003\n",
      "4     1004\n",
      "5     1005\n",
      "6     1006\n",
      "7     1007\n",
      "8     1008\n",
      "9     1009\n",
      "10    1010\n",
      "11    1011\n",
      "12    1012\n",
      "13    1013\n",
      "14    1014\n",
      "15    1015\n",
      "Name: order_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test = dd.order_id # ACHTUNG: funktioniert nicht, wenn es Leerschläge gibt!!\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"select-multi-col-ddset\" style=\"color:black; text-decoration: none;\"><u>Select *multiple* Columns from your Dataframe</u></a>:\n",
    "\n",
    "Similarly to `R`, we need to pass a \"set\" into the selector of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    order_id  order_total\n",
      "0       1000        65.00\n",
      "1       1001        25.75\n",
      "2       1002        20.75\n",
      "3       1003        86.00\n",
      "4       1004        30.00\n",
      "5       1005        20.00\n",
      "6       1006        21.00\n",
      "7       1007        26.25\n",
      "8       1008        20.75\n",
      "9       1009        15.75\n",
      "10      1010        45.00\n",
      "11      1011        30.00\n",
      "12      1012        15.00\n",
      "13      1013        85.00\n",
      "14      1014        25.00\n",
      "15      1015        15.00\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dd[['order_id', 'order_total']] # take the dataframe 'dd' and print me only the columns called 'order_id' && 'order_total'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"filter-1-row-ddset\" style=\"color:black; text-decoration: none;\"><u>Select AND filter for a particular row, with *one condition*</u></a>: \n",
    "\n",
    "Let's say, we want to select the row // observation with the `\"order_id\" == 1004`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id order_date  customer_id  items_ordered  order_total\n",
      "4      1004    12/1/17            8              1         30.0\n",
      "   order_id order_date  customer_id  items_ordered  order_total\n",
      "4      1004    12/1/17            8              1         30.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dd[\n",
    "        dd['order_id'] == 1004 # note that this INNER bracket will run a function that searches through and would only print \n",
    "                               # a Boolean-List of \"True\" or \"False\" for all rows  --> example-video: ab 2:36-3:34 --> https://www.youtube.com/watch?v=htyWDxKVttE\n",
    "    ]                          # This outer selector will tell Python: \"Select\" only the row where the column 'order_id' == 1004\n",
    ")\n",
    "\n",
    "# short-version:\n",
    "\n",
    "print(dd[dd['order_id'] == 1004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quelle: [Basics of selecting and filtering rows and columns in Python, ab 2:36-3:34](https://www.youtube.com/watch?v=htyWDxKVttE&t=2m36s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"filter-multi-cond-ddset\" style=\"color:black; text-decoration: none;\"><u>Select AND filter for rows with *more than one condition*</u></a>:\n",
    "\n",
    "Now, we want to select the row<u>s</u> // observations, where `\"order_total\" >= 50.00` <u>AND</u> `\"order_date\" == 12/1/17` are fulfilled // filtered. Basically, it is the same as with the selection &amp; filtering with *one condition*, you **just need to wrap up the INNER brackets with an additional `(...)` brackets (for <u>each</u> condition!)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id order_date  customer_id  items_ordered  order_total\n",
      "0      1000    12/1/17            1              3         65.0\n",
      "3      1003    12/1/17            8              6         86.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dd[\n",
    "        (dd['order_total'] >= 50) & (dd['order_date'] == '12/1/17') # in contrast to **one condition**, we just wrap up a (...) for each condition\n",
    "    ]                          # This outer selector will tell Python: \"Select\" only the row where the column 'order_id' == 1004\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Alternative mit \"OR\"**</u>: Statt *AND* kann man auch - zum Beispiel - die *OR*-Condition verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    order_id order_date  customer_id  items_ordered  order_total\n",
      "0       1000    12/1/17            1              3        65.00\n",
      "1       1001    12/1/17            3              2        25.75\n",
      "2       1002    12/1/17            6              1        20.75\n",
      "3       1003    12/1/17            8              6        86.00\n",
      "4       1004    12/1/17            8              1        30.00\n",
      "13      1013    12/3/17            7              5        85.00\n"
     ]
    }
   ],
   "source": [
    "print(dd[(dd['order_total'] >= 50) | (dd['order_date'] == '12/1/17')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"select-only-missings\" style=\"color:black; text-decoration: none;\">Select only rows with Missing-Values</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_data = df[df.isnull().any(axis = 1)] # this will only select the rows that contain at least one missing-value\n",
    "null_data # check, if it worked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id=\"sel-loc-iloc\" style=\"color:black; text-decoration: none;\">Selection with \"accessor operators\" `iloc` &amp; `loc`</a>\n",
    "---\n",
    "\n",
    "Das Package `Pandas` erlaubt die Verwendung von sogenannten `accessor operators`, um Dataframes einfacher zu filtrieren. Dabei unterscheidet man zwischen: \n",
    "\n",
    "- `iloc` (= based on the <u>**Postition**</u> (= Nummer) des Index innerhalb der Spalten & Reihen im ddset),\n",
    "    - <u>Beachte</u>: Reihenfolge der inputs within `iloc` == 1) 'rows', then 2) 'columns'\n",
    "- `loc` (= based on the <u>**Index-Namen**</u> of the Spalten & Zeilen im ddset).\n",
    "    - <u>Beachte</u>: Reihenfolge der inputs within `loc` == 1) 'rows', then 2) 'columns'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"example-iloc-sel\" style=\"color:black; text-decoration: none;\"><u>Selection of the `entry` situated in the 1st row and 5th column only with `iloc`</u></a>\n",
    "\n",
    "To be able to use `iloc`, the **key** is to know *the position* of the row- & column-labels (= index of columns & rows).\n",
    "\n",
    "- <u>Note</u>: `iloc` &amp; `loc` are often used to print a particular *entry* WITHIN a dataframe. However, `loc` and `iloc` also are able to print <u>entire</u> rows // observations and not just one specific value within a row, as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.0\n"
     ]
    }
   ],
   "source": [
    "test = dd.iloc[0,4] # Reihenfolge der inputs == 1) 'rows' (--> \"0\" == 1st row), then 2) 'columns' (--> \"4\" == 5th column)\n",
    "print(test) # check if it worked? --> should print the value '65'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"example-loc-sel\" style=\"color:black; text-decoration: none;\"><u>Selection of the `entry` situated in the 1st row and 5th column only with `loc`</u></a>\n",
    "\n",
    "The **key** to use `loc` is that you know *the names* of the columns & rows. \n",
    "\n",
    "- <u>Wichtige Bemerkung</u>: Der **Default-<u>Name</u>** (= Standard-Name) der `rows` innerhalb eines Dataframes ist einfach die Zahlenabfolge von `0,1,2...,10,11,12,...`. Lustigerweise ist der **row-label // <u>index</u> von rows** auch standardmässig die Zahlenabfolge `0,1,2,...,10,11,12,...`. Deshalb kommt es sehr oft vor, dass `loc` und `iloc` ***denselben*** ersten Input haben! xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.0\n"
     ]
    }
   ],
   "source": [
    "test = dd.loc[0,'order_total'] # Reihenfolge der inputs == 1) 'rows' (--> \"0\" == NAME der 1st row), then 2) 'columns' (--> \"order_total\" == NAME der gewünschten 5th column)\n",
    "print(test) # check if it worked? --> should print the value '65'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"example-sel-with-threshold-loc\" style=\"color:black; text-decoration: none;\"><u>Apply a Threshold-Filter using `loc` on each row</u></a>:\n",
    "\n",
    "This can be useful, when you want to replace some values in certain columns &#8594; see 'Outliers'-chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only display all rows, where the 'pressure'-column is > than the threshold of 1051 bar\n",
    "\n",
    "df_weather.loc[df_weather.pressure > 1051, 'pressure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"example-sel-entire-row-with-iloc\" style=\"color:black; text-decoration: none;\"><u>Selection of an ENTIRE row // record // observation from a ddset with `iloc`</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id            1000\n",
      "order_date       12/1/17\n",
      "customer_id            1\n",
      "items_ordered          3\n",
      "order_total           65\n",
      "Name: 0, dtype: object\n",
      "   order_id order_date  customer_id  items_ordered  order_total\n",
      "0      1000    12/1/17            1              3        65.00\n",
      "1      1001    12/1/17            3              2        25.75\n",
      "2      1002    12/1/17            6              1        20.75\n",
      "3      1003    12/1/17            8              6        86.00\n",
      "5      1005    12/2/17            1              1        20.00\n",
      "8      1008    12/2/17            9              1        20.75\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "## Möglichkeit 1: selection of only 1 row\n",
    "\n",
    "test_row1 = dd.iloc[0,] # Important: 1) rows, then 2) columns --> we want the entire 1st row, which includes ALL columns\n",
    "                        # Note: das Komma NACH dem \"0\" zeigt an, dass wir ALLE columns selektieren wollen!\n",
    "\n",
    "#####\n",
    "## Möglichkeit 2: selection of > 1 row --> notice the additional wrap with [...] WITHIN iloc[]!\n",
    "\n",
    "test_multiRow = dd.iloc[[0,1,2,3,5,8],] # '[0,1,2,3,5,8]' will select the '1st, 2nd, 3rd, 4th, 6th and 9th' row\n",
    "                                            # while also selecting ALL columns simultaneously\n",
    "\n",
    "#####   \n",
    "## check if it worked? --> yes!\n",
    "print(test_row1) # should print only 1 row BUT with ALL the column --> weird output, because the columns sind abgebildet als rows xD\n",
    "print(test_multiRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"example-sel-entire-row-with-loc\" style=\"color:black; text-decoration: none;\"><u>Selection of an ENTIRE row // record // observation from a ddset with `loc`</u></a>:\n",
    "\n",
    "<u>**Tipp**</u>: Von der Eleganz &amp; Effizienz her, empfehle ich dir undbedingt **Möglichkeit 3** im unteren Code-Beispiel!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id            1000\n",
      "order_date       12/1/17\n",
      "customer_id            1\n",
      "items_ordered          3\n",
      "order_total           65\n",
      "Name: 0, dtype: object\n",
      "   order_id order_date  customer_id  items_ordered  order_total\n",
      "0      1000    12/1/17            1              3        65.00\n",
      "1      1001    12/1/17            3              2        25.75\n",
      "2      1002    12/1/17            6              1        20.75\n",
      "3      1003    12/1/17            8              6        86.00\n",
      "5      1005    12/2/17            1              1        20.00\n",
      "8      1008    12/2/17            9              1        20.75\n",
      "    order_id order_date  order_total\n",
      "0       1000    12/1/17        65.00\n",
      "1       1001    12/1/17        25.75\n",
      "2       1002    12/1/17        20.75\n",
      "3       1003    12/1/17        86.00\n",
      "4       1004    12/1/17        30.00\n",
      "5       1005    12/2/17        20.00\n",
      "6       1006    12/2/17        21.00\n",
      "7       1007    12/2/17        26.25\n",
      "8       1008    12/2/17        20.75\n",
      "9       1009    12/3/17        15.75\n",
      "10      1010    12/3/17        45.00\n",
      "11      1011    12/3/17        30.00\n",
      "12      1012    12/3/17        15.00\n",
      "13      1013    12/3/17        85.00\n",
      "14      1014    12/3/17        25.00\n",
      "15      1015    12/3/17        15.00\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "## Möglichkeit 1: selection of only 1 row\n",
    "\n",
    "test_row1 = dd.loc[0,] # das Komma NACH dem \"0\" zeigt an, dass wir ALLE columns selektieren wollen!\n",
    "\n",
    "#####\n",
    "## Möglichkeit 2: selection of > 1 row --> notice the additional wrap [...] WITHIN loc[]!\n",
    "\n",
    "test_multiRow = dd.loc[[0,1,2,3,5,8],] # Weil - per default - die 'row-labels' (= name des Indexes \n",
    "                                            # der Zeilen) dieselben sind, wie die Position, ist der Code \n",
    "                                            # für 'loc' derselbe, wie für 'iloc' hier...\n",
    "        \n",
    "#####\n",
    "## Möglichkeit 3: Beste & schönste Solution (meiner Meinung nach!)\n",
    "rows = list(range(0,16)) # will create a list that goes from 0 to 99 --> this will be for the row-labels\n",
    "columns = ['order_id', 'order_date', 'order_total'] # this will be for the column-labels\n",
    "                                                    # Pro-Tipp: columns = list(data.columns)\n",
    "df = dd.loc[rows, columns]\n",
    "        \n",
    "        \n",
    "#####   \n",
    "## check if it worked? --> yes!\n",
    "print(test_row1) # should print only 1 row BUT with ALL the column --> weird output, because the columns sind abgebildet als rows xD\n",
    "print(test_multiRow)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"replace-values-within-rows\" style=\"color:black; text-decoration: none;\"><u>Replace values within a Column with some new Values</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.ungewichtet > 1, 'ungewichtet'] = 1 # hier werde ich alle Werte der Spalte \"ungewichtet\" > 1 mit dem Wert \"1\" ersetzen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"different-slicing-loc-and-iloc\" style=\"color:black; text-decoration: none;\"><u>Different Slicing when using `iloc` VS. `loc`</u></a>: \n",
    "\n",
    "When using `iloc`, **the range 0:5** will select entries `0,...,4` that is: it indexes EXCLUSIVELY. On the other hand, `loc`, meanwhile, indexes *INCLUSIVELY*. So **the <u>same</u> range 0:5** will select entries `0,...,5`!!!\n",
    "\n",
    "*Hence, if you want the SAME output with `loc` and `iloc`, you simply need to slightly change the `range()`-function.*\n",
    "\n",
    "<u>Example</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1000\n",
      "1    1001\n",
      "2    1002\n",
      "3    1003\n",
      "4    1004\n",
      "Name: order_id, dtype: int64\n",
      "0    1000\n",
      "1    1001\n",
      "2    1002\n",
      "3    1003\n",
      "4    1004\n",
      "Name: order_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Möglichkeit 1: with 'iloc'\n",
    "iloc_test = dd.iloc[0:5,0] # row-position == 0:5 --> first 5 rows; EXCLUDES '5' from the range \"0,1,2,3,4,5\" \n",
    "                                          # --> hence range(0:5) results in --> \"0,1,2,3,4\"\n",
    "                                          # column-position == 0 --> 1st row --> remember: indexing in \n",
    "                                          # Python starts at '0'!\n",
    "\n",
    "    # IMPORTANT: 'iloc' uses the 'Python stdlib' indexing scheme, where the first element of the range is \n",
    "    # included and the last one excluded. So 0:5 will select entries 0,...,4 (= these are the first *5* \n",
    "    # entries!!).\n",
    "\n",
    "## Möglichkeit 2: to get the SAME output with 'loc', we need a slightly DIFFERENT range!\n",
    "loc_test = dd.loc[0:4,'order_id'] # row-position == 0:4 --> first 5 rows; INCLUDES '4' \n",
    "                                  # --> hence range(0:4) results in --> \"0,1,2,3,4\"\n",
    "\n",
    "## check if the output are the same, even though \"range()\" has slightly different inputs? --> yes!\n",
    "print(iloc_test)\n",
    "print(loc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sort-and-filt\" style=\"color:black; text-decoration: none;\">Sorting &amp; Filtering</a>\n",
    "---\n",
    "\n",
    "### <u><a id=\"sort-pearson-corr-from-most-imp-to-least-imp\" style=\"color:black; text-decoration: none;\">How to sort a Data-Frame Column (hier: Pearson-Correlation Matrix) from '_most important_' to '_least important_'?</a></u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the correlations' ranking for the day-ahead electricity price and the rest of the features:\n",
    "\n",
    "# Step 1: Create a Pearson-Korrelation Matrix out of your dataframe:\n",
    "correlations = df.corr(method='pearson') # the variable 'correlations' is a dataframe!\n",
    "\n",
    "# Step 2: use 'sort_values' to sort the column from \"most important\" (highest value) to \"least important\":\n",
    "print(correlations['pricesCH'].sort_values(ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"example-of-sort-corr\" style=\"color:black; text-decoration: none;\"><u>Filter the WHOLE Dataframe after a condition (hier: Correlations > 0.75), select ONLY all observations that fullfill the condition (= 'stack them') and sort them  left in your Data Frame</u></a>:\n",
    "\n",
    "- Dieser Code beruht auf den Abschnitt: [_How to sort a Data-Frame Column (hier: Pearson-Correlation Matrix) from 'most imporant' to 'least important'_](#pearson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_correlated = correlations[correlations > 0.75]\n",
    "print(highly_correlated[highly_correlated < 1.0].stack().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"dealing-with-missings\" style=\"color:black; text-decoration: none;\">Data-Cleaning: Missings</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"count-total-missings-ddset\" style=\"color:black; text-decoration: none;\"><u>Number of Missing Values across the WHOLE Data-Frame</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} missing values or NaNs in df_energy.'\n",
    "      .format(df_energy.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"count-missings-per-columns\" style=\"color:black; text-decoration: none;\"><u>Count the TOTAL number of Missings in each column</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy.isnull().sum(axis=0) # outputs the number of NaNs for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"count-non-missings-only-if-no-0\" style=\"color:black; text-decoration: none;\"><u>Count the non-missings (and non-zero values) in each column</u></a>:\n",
    "\n",
    "**<u>Achtung</u>**: the \"Code-Trick\" below only works, if your columns don't contain values, that are '0'! This is because the number '0' - as a `boolean` - will be printed out to `False`, and hence, we will get the \"wrong\" number of missing values. \n",
    "\n",
    "*Verwende den unteren Code also nur, wenn du zuerst abgecheckt hast, dass du keine `0` in den einzlenen Spalten hast!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of non-missing values in each column\n",
    "\n",
    "print('Non-zero values in each column:\\n', df_energy.astype(bool).sum(axis=0), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above cell only gives out the \"correct\" number of non-missing values, if you have no `0` in your columns, here is code to count how many `0` you have in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_energy == 0).astype(int).sum(axis=0) # count the numbers of '0s' in each column [axis = 0, for columns...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"show-only-rows-with-missings\" style=\"color:black; text-decoration: none;\"><u>Display each row with a Missing-Value</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rows with null // missing values:\n",
    "\n",
    "df_energy[df_energy.isnull().any(axis=1)].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"replace-missings-with-0\" style=\"color:black; text-decoration: none;\"><u>Replace all Missings in a Column with 0s</u></a>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ColumnWithMissings'] = df_tot['ColumnWithMissings'].fillna(0) # replaces all missing-values within the column with 0s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Replace all Values in a Column with `Missings`</u>:\n",
    "    \n",
    "*Assume that we have a time-series that has a row-index with time-stamps*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: define the range in which you want to replace values\n",
    "start = '2020-01-01' # 01. Januar 2020 --> ab hier wollen wir die values der Time-Series mit Missings ersetzen\n",
    "stop = '2020-08-01' # 01. August 2020 --> bis zu diesem Datum sollen die Missings eingefügt werden\n",
    "\n",
    "### Step 2: replace the values with missings via the \".loc[row-indexer, column-indexer]\"\n",
    "df.loc[start:stop, 'y_hat'] = None # This will replace all the values within the 'y_hat'-column - in the range from \n",
    "                                   # 01.01.2020-01.08.2020 with Missing-values (instead of \"normal\"-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"missing-imputations\" style=\"color:black; text-decoration: none;\">Missing Imputation</a>\n",
    "---\n",
    "\n",
    "This list will grow with time, the more I stumble onto various codes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Missing Interpolation</u>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"interpolation-for-time-series\" style=\"color:black; text-decoration: none;\"><u>For a Time Series</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values using interpolation:\n",
    "\n",
    "df_energy.interpolate(method='linear', limit_direction='forward', inplace=True, axis=0) # since we have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"data-duplicates\" style=\"color:black; text-decoration: none;\">Data-Cleaning: Duplicates</a>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"count-all-duplicates\" style=\"color:black; text-decoration: none;\"><u>Number of Duplicates across the WHOLE Data-Frame</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_energy = df_energy.duplicated(keep='first').sum()\n",
    "\n",
    "print('There are {} duplicate rows in df_energy based on all columns.'\n",
    "      .format(temp_energy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"drop-duplicates\" style=\"color:black; text-decoration: none;\"><u>Drop Duplicate values</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variante 1: mit reset_index & neuer set_index\n",
    "df_weather_2 = df_weather.reset_index().drop_duplicates(subset=['time', 'city_name'], # Drop the duplicate, if all the rows are the same // have the same values (Achtung: we only look at the duplicates in the 'time' & 'city_name'-column from this analysis!).\n",
    "                                                        keep='last').set_index('time') # if you have duplicate, keep only the last of the duplicated-rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variante 2: man dropt den \"alten\" Index (VOR merge) und neuem set_index \n",
    "df_unique_dates = df_ferien.drop_duplicates(subset='datum', # only consider the column \"datum\" [= column that has duplicates] when dropping the duplicates \n",
    "                                    keep='first').reset_index(drop=True) # reset the index, otherwise you get weird indizes (mit 10'000 für manche)\n",
    "                                                                         # 'drop = True' means that we do not keep the \"old\" index as a separate 'column'\n",
    "df_unique_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"outliers\" style=\"color:black; text-decoration: none;\">Data-Cleaning: Outliers</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"draw-boxplot\" style=\"color:black; text-decoration: none;\"><u>Draw a Boxplot for a specific column</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x=df_weather['pressure'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><strong><u>Key-Question</u></strong>: <h1><em><center>Are there Outliers? &#8594; Yes / No</center></em></h1></li><br>\n",
    "    <ul>\n",
    "        <li><strong><u>\"Trick\" to answer the question</u></strong>: If you deal with temperature for example, google for \"highest Temperature on earth\" and look it up in Wikipedia to dertermine whether your value is an outlier.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <u>Example</u>: ![Outlier Examples](bilder/outlier-example.jpg) Even a pressure of approximately 100,000 HPa or 10 MPa, which is clearly visible in the above figure, corresponds to a quantity greater than the atmospheric pressure of Venus. In order to be sure, we will set as `NaN`s every value in the `pressure`-column which is **higher than 1051 hPa**, which is just above the highest air pressure ever recorded in the Iberian peninsula. While outliers on the low side are not visible in the boxplot above, it is a good idea to also replace the values which are **lower than 931 hPa**, i.e. the lowest air pressure ever recorded in the Iberian peninsula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Step 2</u>**:<br>\n",
    "<h3><em>If the answer to the above question is 'yes', then set the value above values to <code>NaN</code>s, which are above a certain \"unprobable\" threshold.</em></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers in the `Set_Name_of_Column_with_the_Outlier_hier`-column with `NaN`s\n",
    "\n",
    "df_weather.loc[df_weather.pressure > 1051, 'pressure'] = np.nan\n",
    "df_weather.loc[df_weather.pressure < 931, 'pressure'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"merging-and-splitting\" style=\"color:black; text-decoration: none;\">Merging &amp; Splitting</a>\n",
    "---\n",
    "\n",
    "- <u>**Important note before you start**</u>: If you want to merge 2 dataframes, where _one is smaller than the other_, then you <mark>CANNOT make the bigger dataset become _smaller_ with `merge()`</mark>. I did spend alot of time, but without success. However, there is **another solution**: you just need to drop the duplicates of the bigger dataframe ([see chapter `Duplicates`](#duplicates-chapter) in order to be able to make the dataset smaller! =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"split-big-ddset-by-categories\" style=\"color:black; text-decoration: none;\"><u>Splitting a Dataset into smaller parts, by using categories to split it up</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df_weather into 5 dataframes (one for each of the 5 cities):\n",
    "\n",
    "df_1, df_2, df_3, df_4, df_5 = [x for _, x in df_weather.groupby('city_name')]\n",
    "dfs = [df_1, df_2, df_3, df_4, df_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(planets).replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"merging-by-idx\" style=\"color:black; text-decoration: none;\"><u>Merge 2 separate Data Sets together via their row-index (= row-label)</u></a>:\n",
    "\n",
    "Damit du dieses Merging erfolgreich durchführen kannst, müssen beide Datensätze denselben Zeilen-Index besitzen. \n",
    "\n",
    "- **<u>Ziel</u>**: *Dadurch kannst du zum Beispiel **neue Spalten von einem <u>anderen</u> Datensatz hinzufügen** (passiert sehr oft in der Praxis)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge all the y- & X-Variables from the training-set together:\n",
    "test = pd.merge(\n",
    "    y_train, # der Trainingsdatensatz für die y-Variable\n",
    "    x_train, # der Trainingsdatensatz für alle X-Variablen\n",
    "    how=\"outer\",\n",
    "    left_on=y_train.index, # merging via index // row-label des DataFrames der y-Variable\n",
    "    right_on=x_train.index, # merging via index // row-label des DataFrames der x-Variablen\n",
    ").set_index('key_0') # optional: da wir hier eine Zeitreihe haben, dessen Row-Index den Column-Name 'key_0' animmt beim Merging, wird hier als neuer Row-Index für den gemerged Dataframe gesetzt\n",
    "test # check if it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"merge-multiple-ddset\" style=\"color:black; text-decoration: none;\"><u>Merge different datasets simultaneously together</u></a>:\n",
    "\n",
    "**<u>Ausgangssituation</u>**: Assume that - initially - we have 2 datasets, 1 is for the weather and 1 is for the Energy-Prices. Furthermore, they those two datasets are **time-series**. Hence, they have the same time-index (= row-label), formatted in **UTC**.\n",
    "\n",
    "**<u>Step 1</u>**: Split up the weather-dataset, sorted by cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df_weather into 5 dataframes (one for each of the 5 cities):\n",
    "\n",
    "df_1, df_2, df_3, df_4, df_5 = [x for _, x in df_weather.groupby('city_name')]\n",
    "dfs = [df_1, df_2, df_3, df_4, df_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Step 2</u>**: Merge the 5 sub-datasets with the Energy-Price Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: save a copy, in case you do a wrong merging!\n",
    "df_final = df_energy \n",
    "\n",
    "# Step 2: make a for-loop, to merge all the 6 datasets simultaneously\n",
    "for df in dfs: # here, we loop through every city-group of our list of data frames (see step 1)\n",
    "    city = df['city_name'].unique() # we store the names of the 5 cities - as a list - in a variable\n",
    "    city_str = str(city).replace(\"'\", \"\").replace('[', '').replace(']', '').replace(' ', '') # we perform some string-manipulation to eliminate all the characters that are not necessary\n",
    "    df = df.add_suffix('_{}'.format(city_str)) # we re-name the columns, by adding the name of the city to each column\n",
    "    df_final = df_final.merge(df, # this is the merging-part!\n",
    "                              on=['time'], # we want to merge via the index // row-label of both datasets --> since they are both in UTC-time, this will work!\n",
    "                              how='outer') # 'outer' means: we want the 'union' --> see this youtube-video for a good explanation: https://www.youtube.com/watch?v=h4hOPGo4UVU\n",
    "    df_final = df_final.drop('city_name_{}'.format(city_str), axis=1) # let's drop some columns that we don't need anymore\n",
    "    \n",
    "# Step 3: \"final results\"-check\n",
    "df_final.columns # show the merging-results, by displaying all the column-names --> DONE! =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Step 3**</u>: Make some final-checks\n",
    "\n",
    "<ul>\n",
    "    <li><strong><u>Key-Question</u></strong>: <h1><em><center>Did the merging really worked? &#8594; Yes / No</center></em></h1></li><br>\n",
    "    <ul>\n",
    "        <li><strong><u>\"Trick\" to answer the question</u></strong>: Look at Missings &amp; Duplicates.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of NaNs and duplicates in the final dataframe\n",
    "\n",
    "print('There are {} missing values or NaNs in df_final.'\n",
    "      .format(df_final.isnull().values.sum()))\n",
    "\n",
    "temp_final = df_final.duplicated(keep='first').sum()\n",
    "\n",
    "print('\\nThere are {} duplicate rows in df_energy based on all columns.'\n",
    "      .format(temp_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><em>If the answer is 'yes', then merging should have worked, if you have no missings and no duplicates. <br><br>\n",
    "    If the answer to the above question is 'no', then you need to go back to step 2 and try to figure out what you did wrong when merging the datasets together.</em></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"advanced-exploration\" style=\"color:black; text-decoration: none;\">Advanced Exploration of your data</a>\n",
    "---\n",
    "\n",
    "### <a id=\"create-correlation-matrix\" style=\"color:black; text-decoration: none;\"><u>How to create &amp; plot a `Pearson Correlation Matrix` out of a Dataframe?</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: construct a 'Pearson Correlation Matrix' as a Dataframe:\n",
    "correlations = df.corr(method='pearson')\n",
    "\n",
    "# Step 2: Load Libraries & plot Pearson correlation matrix:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(24, 24)) #\n",
    "sns.heatmap(correlations, annot=True, fmt='.2f') # import seaborn as sns\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"time-series-exploration\" style=\"color:black; text-decoration: none;\">Time-Series: Exploration &amp; Visualizations</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"plot-time-series\" style=\"color:black; text-decoration: none;\"><u>Plotting a Time Series</u></a>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Step 1</u>**: Transform the `date`-column into a `date-time`-column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt # this is the package we need to convert a column into a 'date-time'\n",
    "\n",
    "# Step 1: Define a function that is reading the date-column correctly\n",
    "\n",
    "def parse_date(date):\n",
    "    data=str(date)\n",
    "    if date==\"\": # this condition will never be true, since the column 'Date-Time' NEVER has an empty string; ### raise exception\n",
    "        return None\n",
    "    else:\n",
    "        return pd.to_datetime(date, format='%Y-%m-%d %H:%M:%S', yearfirst = True, utc = True)\n",
    "\n",
    "# Step 2: apply the above function on the Column 'Date-Time' to transform the column into a 'date-time'-type\n",
    "raw_dd[\"Date-Time\"] = raw_dd[\"Date-Time\"].apply(parse_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Step 2</u>**: Set the 'time'-column as the index (= row-label), otherwise the plotting won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the date-column as our row-label:\n",
    "raw_dd = raw_dd.set_index(\"Date-Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Step 3**</u>: Visualize the Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils.plotting import plot_series # use sktime and the 'plot_series'-module for this task\n",
    "\n",
    "# Define our y-variable (= DA-Prices in CH):\n",
    "y = raw_dd[\"pricesCH\"] \n",
    "\n",
    "plot_series(y) # visual check if it worked? --> yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"auto-and-partial-auto-corr\" style=\"color:black; text-decoration: none;\"><u>Plot Auto-Correlation &amp; Partial-Auto-Correlation Functions</u></a>:\n",
    "\n",
    "The partial autocorrelation plot of the eletricity price time series shows that the direct relationship between an observation at a given hour (t) is strongest with the observations at t-1, t-2, t-24 and t-25 time-steps and diminishes afterwards. **Thus, we are going to use the 25 previous values of each time series which will constitute a feature for our models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: load library\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # to plot \n",
    "\n",
    "# Step 2: Plot autocorrelation and partial autocorrelation plots\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 6))\n",
    "plot_acf(df_energy['pricesCH'], lags=50, ax=ax1)\n",
    "plot_pacf(df_energy['pricesCH'], lags=50, ax=ax2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"cross-corr\" style=\"color:black; text-decoration: none;\"><u>Plot the Cross-Correlation(Y<sub>t</sub>, X<sub>t - k</sub>)</u></a>:\n",
    "\n",
    "It would quite definitely be **more beneficial if we only chose to use specific past values (observations at certain time-lags) of a given feature, based on the cross-correlation between the electricity price and each one of the features in the dataset**. For example, below we can see the cross-correlation between the electricity price and the Renewable Generation from Switzerland. We see that there are many time-lags with a correlation which is close to zero and could be ommited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import ccf # to plot the cross-correlation function\n",
    "\n",
    "# Step 2: plot the cross-correlation between the day-ahead price \"today\" and with each of the first 50-lags of the column 'RenGen' \n",
    "cross_corr = ccf(df_energy['RenGenCH'], df_energy['pricesCH'])\n",
    "plt.plot(cross_corr[0:50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"feature-engineering-time-series\" style=\"color:black; text-decoration: none;\">Feature Engineering for Time-Series</a>\n",
    "---\n",
    "\n",
    "Wenn du mit Variablen arbeitest, welche mit \"Zeit\" zu tun haben, brauchst du einige \"Tricks\", um Data Cleaning betreiben zu können mit sogenannten `Date-Time Objects` in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"time-stamps-as-index\" style=\"color:black; text-decoration: none;\"><u>Creating an index for Time-Series</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-01 01:00:00+00:00',\n",
       "               '2020-01-01 02:00:00+00:00', '2020-01-01 03:00:00+00:00',\n",
       "               '2020-01-01 04:00:00+00:00', '2020-01-01 05:00:00+00:00',\n",
       "               '2020-01-01 06:00:00+00:00', '2020-01-01 07:00:00+00:00',\n",
       "               '2020-01-01 08:00:00+00:00', '2020-01-01 09:00:00+00:00',\n",
       "               ...\n",
       "               '2020-12-31 14:00:00+00:00', '2020-12-31 15:00:00+00:00',\n",
       "               '2020-12-31 16:00:00+00:00', '2020-12-31 17:00:00+00:00',\n",
       "               '2020-12-31 18:00:00+00:00', '2020-12-31 19:00:00+00:00',\n",
       "               '2020-12-31 20:00:00+00:00', '2020-12-31 21:00:00+00:00',\n",
       "               '2020-12-31 22:00:00+00:00', '2020-12-31 23:00:00+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', length=8784, freq='H')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = '2020-01-01'\n",
    "stop = '2021-01-01' # ACHTUNG: wird am 31.12.2020 enden\n",
    "ts_index = pd.date_range(start, stop, freq = 'h', closed = 'left', tz = 'UTC')\n",
    "ts_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>(Optionale) Erweiterung</u>: Index-Slicing\n",
    "\n",
    "Der **Nachteil** am obigen Index ist, dass er immer erst **ab 0:00 Uhr beginnt**. Das **Problem** dabei ist, dass *du nicht immer einen Index benötigen wirst, welcher punktgenau um 0:00 Uhr beginnt, sondern vielleicht einmal um 23:00 Uhr*. \n",
    "\n",
    "- <u>Lösung</u>: <strong>Verwende `Slicing`, um ab der ersten Beobachtung vom Jahr 2020, um 00:00 zu starten, beispielsweise so: `DF2 = DF1.loc[:'2019-05-26 13:00:00+00:00']` für einen DataFrame, beziehungsweise so: `ts_index[4:]` für eine Pandas-Series</strong>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"copy-index-as-columns\" style=\"color:black; text-decoration: none;\"><u>Make a copy of the index as a Column of a DataFrame</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = p_train.reset_index()\n",
    "p_train['date'] = p_train['Date-Time']\n",
    "p_train.set_index('Date-Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"make-datetime-and-set-index\" style=\"color:black; text-decoration: none;\"><u>Converting a Time-Column into a Datetime-Column and set it as an Index (= row label)</u></a>:\n",
    "\n",
    "**If you work with time-series**, it will oftentimes be the case that you will need to convert your date-column - which **oftentimes are strings** - into an actual `date-type` column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy['Date-Time'] = pd.to_datetime(df_energy['Date-Time'], utc=True, infer_datetime_format=True)\n",
    "df_energy = df_energy.set_index('Date-Time') # set the 'Date-Time'-Column as the index // row-label of our data-frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>If you want to set the format by yourself</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Delivery day to a date time column\n",
    "epex_df['Delivery day'] = pd.to_datetime(epex_df['Delivery day'], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"create-range-of-dates\" style=\"color:black; text-decoration: none;\"><u>Creating a range of dates in Python</u></a>:\n",
    "\n",
    "Specify start and end, with the default daily frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n",
       "               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.date_range(start='1/1/2018', end='1/08/2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"rechnen-mit-date-time-objects\" style=\"color:black; text-decoration: none;\">Rechnen mit Date-Time Objects: Addition und Subtraktion von Zeiten</a>\n",
    "---\n",
    "\n",
    "Oftmals wirst du Zeitpunkte in der Zukunft oder Vergangenheit definieren müssen. Deshalb ist es essentiell, dass du lernst, wie man mit `Date-Time Objekten` rechnet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"today\" style=\"color:black; text-decoration: none;\"><u>Finde den genauen Zeitpunkt \"jetzt\" // \"heute\" heraus</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 13:54:53.425398\n"
     ]
    }
   ],
   "source": [
    "import datetime # Da wir mit \"Zeit\" arbeiten, brauche ich das Package \"datetime\"\n",
    "\n",
    "jetzt = datetime.datetime.now() # current time\n",
    "\n",
    "print(jetzt) # check: should output the current time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"zukunft-relativ-zu-jetzt\" style=\"color:black; text-decoration: none;\"><u>Zukunftszeitpunkt mit dem \"jetzt\" als Referenzpunkt // Startpunkt</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 15:00:59.045610\n"
     ]
    }
   ],
   "source": [
    "import datetime # Da wir mit \"Zeit\" arbeiten, brauche ich das Package \"datetime\"\n",
    "\n",
    "jetzt = datetime.datetime.now() # current time\n",
    "added_time = datetime.timedelta(hours= 1, minutes= 1) # anzahl an Stunden & Minuten, die du dazu addieren willst\n",
    "ende = jetzt + added_time\n",
    "\n",
    "print(ende) # check: should output \"current time + 5h 2 min\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"subtraktion-von-zeitpunkt\" style=\"color:black; text-decoration: none;\"><u>Rechnen mit Datum: Subtraktion mit `fixed time-point` als Referenzpunkt // Startpunkt</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0711\n"
     ]
    }
   ],
   "source": [
    "import datetime # Da wir mit \"Zeit\" arbeiten, brauche ich das Package \"datetime\"\n",
    "\n",
    "fixe_zeit = datetime.datetime(2021, 7, 1, 8, 12) # datetime(year, month, day, hour, minute, second, microsecond), \n",
    "# wobei die ersten 3 Argumente (Jahr, Monat, Tag) OBLIGATORISCH sind! --> hier interessiert uns die letzten 2 Inputs, \n",
    "# nämlich die \"8\" und die \"12\" --> diese wiederspiegeln meine fixe 8h 12 min Arbeitszeit bei der SBB\n",
    "added_time = datetime.timedelta(hours= 1, minutes= 1) # anzahl an Stunden, die du noch arbeiten musst\n",
    "wie_lange_noch = fixe_arbeitszeit - added_time\n",
    "\n",
    "print(wie_lange_noch.strftime(\"%H%M\")) # check: should output 7h 11min --> yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"wochentag-herausfinden\" style=\"color:black; text-decoration: none;\"><u>Day of the Week given a Date</u></a>:\n",
    "\n",
    "If you want to know - for example - which \"Wochentag\" z.B. the `21.03.2020` was, simply use this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "fixe_zeit = datetime.datetime(2020, 12, 12) # datetime(year, month, day, hour, minute, second, microsecond)\n",
    "fixe_zeit.weekday() \n",
    "\n",
    "# --- Gemäss Documentation\n",
    "\n",
    "# 0 = Monday\n",
    "# 1 = Tuesday \n",
    "# 2 = Wednesday\n",
    "# 3 = Thursday\n",
    "# 4 = Friday\n",
    "# 5 = Saturday\n",
    "# 6 = Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 3, 22, 0, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addierte_anzahl_tage = datetime.timedelta(days = 28)\n",
    "next_day = fixe_zeit + addierte_anzahl_tage\n",
    "next_next_day = next_day + addierte_anzahl_tage\n",
    "next_next_next_day = next_next_day + addierte_anzahl_tage\n",
    "next_next_next_next_day = next_next_next_day + addierte_anzahl_tage\n",
    "next_next_next_next_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `21.03.2020` was a **Saturday**. I checked it with the calendar: it's correct =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy-Variables\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"create-normal-dummies\" style=\"color:black; text-decoration: none;\"><u>Create a \"normal\" Dummy</u></a>:\n",
    "\n",
    "In Python, you need the `where`-method to create a dummy on a single condition. \n",
    "\n",
    "This is equivalent to the `ifelse()`-Function in `R`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.where(maupayment['log_month'] == maupayment['install_month'], 'install', 'existing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"flagg-dummy-by-merging\" style=\"color:black; text-decoration: none;\"><u>Create a Dummy by `merging` &amp; `flagging`</u></a>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful, when you need to track which of the variable was part of the \"left\" and which one was part of the \"right\" dataset. You literally create a dummy-variable when you do this and it is called the `_merge`-column.\n",
    "\n",
    "- <u>Andwendungsfall 1</u>: als ich bei der SBB versucht habe, einen \"Ferien-Dummy\" zu kreieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.merge(df_year_2019, \n",
    "                 on='date', # in this case, we merge over the \"date\"-column; note that 'df' has 365 rows, while 'df_year_2019' only 270 rows\n",
    "                 how='outer', # we need to set\n",
    "                 indicator=True) # the flag is \"indicator\"\n",
    "df2 # we get a ddset with 360 rows. On the outside, the 'testo'-ddset seems NOT to be different form the 'df', however, the purpose of the merge was to create a new dummy-variable to see from which dataset it came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: we want some more meaningful values within the 'merge'-column --> apply an if-statement onto the (string-) column\n",
    "\n",
    "df2.loc[df2['_merge'] == 'both', 'ferien'] = 1 # Here, we create a column 'Ferien', which will be equal to \"1\", if the value in the '_merge'-column is equal to the string \"both\"\n",
    "df2.loc[df2['_merge'] != 'both', 'ferien'] = 'False' # Ferien-Dummy equals zero otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"dummy-variables-for-time-variables\" style=\"color:black; text-decoration: none;\">Dummy-Variables: Create Columns for specific hours, seasons of the year etc...</a>\n",
    "---\n",
    "\n",
    "In the following, I will use a column called `Date-Time` to create *new* columns to extract some 'time-information' (hours, quarters etc...) needed.\n",
    "\n",
    "### <a id=\"dummy-for-hours\" style=\"color:black; text-decoration: none;\"><u>Extract `hours` and put it into a separate column</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dd['Hour'] = raw_dd['Date-Time'].apply(lambda x: x.hour) # Werte von 0 [= Stunde 0:00-1:00] bis 23 [= Stunde 23:00-24:00]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"dummy-for-quarters\" style=\"color:black; text-decoration: none;\"><u>Extract `Seasons` (= \"qarters\") and put it into a separate column</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dd['Quarter'] = raw_dd['Date-Time'].apply(lambda x: x.quarter) # Werte von 1 [Januar bis März] bis 4 [Oktober bis Dezember]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"dummy-for-days\" style=\"color:black; text-decoration: none;\"><u>Extract `Days` and put it into a separate column</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dd['day_of_week'] = raw_dd['Date-Time'].apply(lambda x: x.weekday()) # Werte von 0 [= Montag] bis 6 [= Sonntag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"scaling-variables\" style=\"color:black; text-decoration: none;\">Scaling Variables</a>\n",
    "---\n",
    "\n",
    "When applying machine learning, one condition to use those fancy models will be to scale our variables first. \n",
    "\n",
    "The main **methods for scaling can be seen in [this Stack-Exchange Post](https://stats.stackexchange.com/questions/70553/what-does-normalization-mean-and-how-to-verify-that-a-sample-or-a-distribution)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"normalization-scaling\" style=\"color:black; text-decoration: none;\"><u>Normalization</u></a>:\n",
    "\n",
    "If you normalize variables, then it means that the RANGE of possible values (= Definitionsbereich), is set between 0 and 1. \n",
    "\n",
    "#### <u>Scale _all_ X-Variables and your Y-variable</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # Der Name der Klasse für Normalisierung = 'MinMaxScaler' (--> mega weird xDD)\n",
    "\n",
    "# step 1: initialize the Class\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: wende den Scaler uf die X- & Y-Variablen (dabei nur auf das Training-Set) an (Achtung: funktioniert nur auf numy-arrays, \n",
    "# aber nicht auf Series // Data-Frame Columns!)\n",
    "scaler_X.fit(X_compl_df[:train_end_idx]) # alternativ: scaler_y.fit(X_train.to_numpy().reshape(-1, 1))\n",
    "scaler_y.fit(y_compl_df[:train_end_idx]) # alternativ: scaler_y.fit(y_train.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: nach dem \"fit\" haben wir die Werte noch nicht als \"DataFrame\"-Typ gespeichert, weshalb wir nun noch 'transform' anwenden\n",
    "X_norm = scaler_X.transform(X_compl_df)\n",
    "y_norm = scaler_y.transform(y_compl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"umwandlungen\" style=\"color:black; text-decoration: none;\">Umwandlungen</a>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"series-to-array\" style=\"color:black; text-decoration: none;\"><u>Convert a Series into an array</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Series.to_numpy() # this also works on a column of a dataframe =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply If-Conditions on a DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a really good reference: https://datatofish.com/if-condition-in-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"data-viz\" style=\"color:black; text-decoration: none;\">Visualization of the Data</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"plot-and-save-image\" style=\"color:black; text-decoration: none;\"><u>Plot &amp; Save an Image</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import the library matplotlib for visualization\n",
    "import matplotlib.pyplot as plt # for the settings of the graph, such as title, windows-size etc...\n",
    "\n",
    "# Step 2: make the plot\n",
    "plt.figure(figsize=(14,6))\n",
    "fig, ax = plot_series(y_train[10290:10320], # only take the last 30 observations from the trainings-set\n",
    "            y_test, \n",
    "            y_pred, \n",
    "            labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "plt.xticks(rotation=90) # rotates Beschriftung der X-Achse um 90-Grad, damit man überhaupt die X-Achse lesen kann\n",
    "\n",
    "# Step 3: save the plot\n",
    "fig.savefig(\"monochrome-2-test.png\") # save the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"multi-plots-for-loop\" style=\"color:black; text-decoration: none;\"><u>For-Loops to iterate over lots of variables and save them</u></a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: create a list of all the columns, you want to iterate through\n",
    "\n",
    "cov_list = X_train.columns # I select all columns in my dataframe 'X_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2:\n",
    "\n",
    "for i in cov_list:\n",
    "    print(i) # just as a check, to see how the loop progresses\n",
    "    covariate_name = i # everytime I iterate over a new variable, I store it into this variable --> I will use this variable later to save each \n",
    "    covariate = X_train[i] # I need each variable to be of type \"Series\" when creating a plot\n",
    "    x = list(range(0, 720, 24)) # this will put a tick-mark after each day\n",
    "    x.append(719) # I also add '719', since the range()-function does not include this number\n",
    "    fig, ax = plot_series(covariate[-720:]) # this plots the last month\n",
    "    plt.xticks(x); # to only display some dates\n",
    "    plt.xticks(rotation=90); # we need to rotate the x-axis, otherwise you cannot read it\n",
    "    fig.savefig(\"visualisation_30_days-{}.png\".format(covariate_name)) # save the graph for each column. The \"trick\" here is, that we can use \n",
    "    # f-strings to create a unique name for each file =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"magic-commands\" style=\"color:black; text-decoration: none;\">Python Magic Commands</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><a id=\"view-all-columns\" style=\"color:black; text-decoration: none;\">Show ALL Columns of a DataFrames</a></u>\n",
    "\n",
    "Wenn grosse Datensätze geprinted werden, ist beim Output oftmals ein Teil der Spalten maskiert, da schlichtweg der Platz fehlt, um alle Spalten anzuzeigen. Allerdings gibt es einen Weg, die Einstellungen via `Pandas` zu ändern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "pd.set_option(\"display.max.columns\", None) # this will tell Python: \"show me ALL the columns!\"\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"html-with-jupyter-notebooks\" style=\"color:black; text-decoration: none;\"><u>Working with HTML within Jupyter-Notebooks</u></a>\n",
    "\n",
    "\n",
    "### <u>How to use anchor-tags to make references within your Jupyter-Notebook</u>?\n",
    "\n",
    "[Klick here to move to the very top of this Jupyter-Notebook](#top). This is achieved by using the `<a>`-HTML-Tag, and some unique ID-name &amp; `CSS`-styling on the `<a>`-Tag.\n",
    "\n",
    "```html\n",
    "<a id=\"gebe-hier-passenden-id-namen\" style=\"color:black; text-decoration: none;\">Hier kommt der Text, auf welchem du zeigen willst...</a>\n",
    "```\n",
    "\n",
    "```markdown\n",
    "[Und das ist der Text, welcher dich weiterleitet](#top)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"share-variables-between-notebooks\" style=\"color:black; text-decoration: none;\"><u>Share Variables between Notebooks</u></a>\n",
    "\n",
    "This `Magic Commands` allows you to share any variable between different Jupyter Notebooks. You need to pass the original variable with the magic command. To retrieve the variable, you need to pass the same command with the `-r` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'myData' (str)\n"
     ]
    }
   ],
   "source": [
    "myData = \"The World Makes Sense\"\n",
    "%store myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you 'saved' the variable `myData` with the `Magic Commands`, **go and open another of your Jupyter Notebooks** and type in the following:\n",
    "\n",
    "```python\n",
    "%store -r myData # step one: run this line in the first cell\n",
    "myData # step 2: run this line in the secons cell\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"useful-tricks\" style=\"color:black; text-decoration: none;\">Useful \"Tricks\" I stumbled upon</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"reshape-1-1\" style=\"color:black; text-decoration: none;\"><u>Reshape(-1, 1)</u></a>:\n",
    "\n",
    "> What does `reshape(-1,1)` do?\n",
    "\n",
    "If you have an array or list, like this: `[1,2,3,4,5]`...\n",
    "\n",
    "Using `reshape(-1,1)` on it, will create a new **list of lists, with only 1 element in each sub-list**, e.g. the output will be `[[1], [2], [3], [4], [5]]`. \n",
    "\n",
    "*See also [this Stack-Overflow answer](https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"math-tricks\" style=\"color:black; text-decoration: none;\">Math Tricks for Hacks</a>\n",
    "---\n",
    "\n",
    "### <a id=\"round-downwards\" style=\"color:black; text-decoration: none;\"><u>How to _always_ round downwards</u></a>?:\n",
    "\n",
    "Mit der Funktion `round`, kann man auf- oder abwärts runden, wie wir es gewohnt sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.96"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(47/24, ndigits = 2) # runde auf 2 Nachkommastellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mit der `floor`-Funktion, kann allerdings stets abwärts gerundet werden, wie das Beispiel zeigt**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math # um 'floo'\n",
    "\n",
    "math.floor(47/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"calc-median\" style=\"color:black; text-decoration: none;\"><u>Calculate the Median?</u></a>\n",
    "\n",
    "Since there is no built-in function for the median, you can write a function which will be able to calculate the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: make a function for the case \"median calculation for an ODD (= ungerade) list of numbers\"\n",
    "\n",
    "def _median_odd(xs) -> float:\n",
    "    return sorted(xs)[len(xs) // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: make a function for the case \"median calculation for an EVEN (= gerade) list of numbers\"\n",
    "\n",
    "def _median_even(xs) -> float:\n",
    "    sorted_xs = sorted(xs)\n",
    "    hi_midpoint = len(xs) // 2\n",
    "    return (sorted_xs[hi_midpoint - 1] + sorted_xs[hi_midpoint]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use the 2 above functions to finally be able to build the median-function\n",
    "\n",
    "def median(v) -> float:\n",
    "    return _median_even(v) if len(v) % 2 == 0 else _median_odd(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert median([1, 10, 2, 9, 5]) == 5 # check if the above function was written correctly (you can change the number '5' and see\n",
    "# what happens, if this expression becomes incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun\n",
    "---\n",
    "\n",
    "In this section, I will just put some important concepts &amp; some of my \"inventions\" that I found useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(10, 2),\n",
    "                  columns=['Col1', 'Col2'])\n",
    "df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n",
    "                     'B', 'B', 'B', 'B', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.616909</td>\n",
       "      <td>-0.998962</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036902</td>\n",
       "      <td>-0.991193</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.418524</td>\n",
       "      <td>-0.070984</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.207328</td>\n",
       "      <td>1.896855</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.376757</td>\n",
       "      <td>-0.333235</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.295216</td>\n",
       "      <td>1.488426</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.226809</td>\n",
       "      <td>-1.351006</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.476242</td>\n",
       "      <td>-0.431204</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.565872</td>\n",
       "      <td>1.174200</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.469251</td>\n",
       "      <td>-0.353409</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Col1      Col2  X\n",
       "0  1.616909 -0.998962  A\n",
       "1  0.036902 -0.991193  A\n",
       "2 -1.418524 -0.070984  A\n",
       "3  1.207328  1.896855  A\n",
       "4  1.376757 -0.333235  A\n",
       "5  0.295216  1.488426  B\n",
       "6  1.226809 -1.351006  B\n",
       "7  0.476242 -0.431204  B\n",
       "8 -1.565872  1.174200  B\n",
       "9 -0.469251 -0.353409  B"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,el in enumerate(list(df.columns.values)[:-1]):\n",
    "    a = df.boxplot(el, by ='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Arbeitszeit (left to work...) Berechnung</u>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu sehen, **wie viele Stunden &amp; Minuten ich heute noch arbeiten muss**, habe ich - als Übung - eine Funktion `hours_to_go` selbst erfasst, welche mir die Anzahl an noch zu verbleibenden Arbeitsstunden am Tag berechnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2880"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*30*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0807\n"
     ]
    }
   ],
   "source": [
    "import datetime # Da wir mit \"Zeit\" arbeiten, brauche ich das Package \"datetime\"\n",
    "\n",
    "\n",
    "def hours_to_go(Stunden, Minuten): # Input: wie viele Stunden & Minuten hast du heute bereits gearbeitet?\n",
    "    \"\"\"gibt an, wie lange ich noch heute \"Schaffen\" muss --> gebe dafür bloss die Anzahl stunden & minuten ein, die du \"heute\" \n",
    "    bereits gearbeitet hast\n",
    "    > hours_to_go(3,5) --> 0507 \n",
    "    interpretation: ich habe fix 8h 12min zu arbeiten. Davon ziehe ich nun 3h 5 min ab --> 0507 heisst, ich habe noch 5h 7min \n",
    "    zu arbeiten, was stimmt!\"\"\"\n",
    "    \n",
    "    fixe_arbeitszeit = datetime.datetime(2021, 7, 1, 8, 12) # datetime(year, month, day, hour, minute, second, microsecond), \n",
    "    # wobei die ersten 3 Argumente (Jahr, Monat, Tag) OBLIGATORISCH sind! --> hier interessiert uns die letzten 2 Inputs, \n",
    "    # nämlich die \"8\" und die \"12\" --> diese wiederspiegeln meine fixe 8h 12 min Arbeitszeit bei der SBB\n",
    "    gearbeitet = datetime.timedelta(hours= Stunden, minutes= Minuten) # anzahl an Stunden, die du noch arbeiten musst\n",
    "    wie_lange_noch = fixe_arbeitszeit - gearbeitet\n",
    "    print(wie_lange_noch.strftime(\"%H%M\"))\n",
    "    \n",
    "hours_to_go(0,5) # call the function to output, how many hours are left to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu sehen, **um welche Uhrzeit ich heute Feierabend machen darf**, habe ich - als Übung - eine Funktion `arbeit_ende` selbst erfasst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-29 15:53:13.919695\n"
     ]
    }
   ],
   "source": [
    "import datetime # Da wir mit \"Zeit\" arbeiten, brauche ich das Package \"datetime\"\n",
    "\n",
    "def arbeit_ende(Stunden, Minuten): \n",
    "    \"\"\"gibt an, wann ich heute mit \"Schaffen\" fertig bin --> gebe dafür bloss die Anzahl stunden & minuten ein, die du \"heute\" \n",
    "    noch arbeiten musst, zum Beispiel arbeite ich heute noch '4h 44 min':\n",
    "    > arbeit_ende(4, 44) --> 2021-07-01 17:53:02.907698 \"\"\"\n",
    "    \n",
    "    jetzt = datetime.datetime.now() # current time\n",
    "    added_time = datetime.timedelta(hours= Stunden, minutes= Minuten) # anzahl an Stunden, die du noch arbeiten musst\n",
    "    ende = jetzt + added_time\n",
    "    print(ende)\n",
    "    \n",
    "    \n",
    "arbeit_ende(8,7) # call the function to output, until when I need to work today "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
