# Foundations of Data Science

In meinem Data Scientist Job werde ich häufig auf ähnliche Probleme stossen mit der Zeit. Hier habe ich eine Reihe an Fragen aufgelistet, welche ich fähig sein muss, zu beantworten, wenn ich effizient in meinem Beruf sein will!

## Allgemeine Roadmap für Data-Science Projekte

Die meisten Data-Science Projekte werden wie folgt ablaufen:

![Übersicht zu den 4 Teil-Etappen in Data-Science](./bilder/foundation-ds/roadmap-uebersicht.jpg)

## Management eines DS-Projektes | The Step-by-Step Guide

### Phase 0 - Correct initialization of a Project

BEFORE you even start a project, **you need to make sure that your project always will work in a reliable way, even when you don't work on your own machine**:

```
1) Create a **new folder** with the project's name.
2) Create a **virtual environment** `venv` for the project (in order to solve dependency issues).
3) Create **a `readme.md`-file** on how to set-up your computer.
  - Copy-Paste this from a previous project.
```

### Phase 1 - Get the Data & erste Recherchearbeit

The basis of every data-science project is your commodity (= Rohstoff): the data that you will work with. That's why, you should use your "Googling ability" to try and find what you are looking for online. Anohter way to get to the data you want is through web-crawlers:

- `beautifulsoup`
- Twitter API
- Twitch API
- Youtube API
- etc...

Then, the steps are relatively clear:

```
4) Get the data.
5) NICHT OBLIGATORISCHER SCHRITT, WENN DU KEINE WISSENSCHAFTLICHE ARBEIT SCHREIBST: Acquire some (theoretical) understanding of your topic // research-area (Achtung: not too deep yet!).
  - Goal (in academia): Formulate a resarch-question. 
    - What would be interesting questions to answer with this type of data that you have? 
    - Can you formulate a hypothesis?
  - Google some books on the topics.
    - Having a look at Bibliotheks-Bücher can also be helpful.
  - Read some introductions on the topic.
    - Are there any White-Papers?
    - Have you seen any meta-paper on the topic? These give a really great overview on the topic!
```

### Phase 2 - Load the Data & Vanilla Exploration

When Loading the data

```
6) Load the data
  - The goal: get acquianted with your data --> Ultimately: Can this data be useful for the problem we are trying to solve?
    - How many columns & rows?
    - `Dtype` of each column?
    - What data does each column represent?
      - Problem you want to solve here: Oftentimes, columns have weird names and you don't know what those abbreviations stand for.
    - What are the possible ranges of values for each categorical(!) data-column?
```

<mark>**Der wichtigste Meilenstein bei dieser Etappe ist vermutlich die Frage: 'Sind meine Daten "gut genug", um das Problem zu lösen oder sind sie einfach nur Müll?"**</mark>

_Wenn du nämlich die obige Frage mit "NEIN" beantwortest, dann brauchst du schon gar nicht mit dem Projekt zu beginnen, denn es wäre ineffizient (oder du machst dich auf die Suche nach neuen Daten...)_ ;)

### Phase 3 - After having concluded: "My data is OK!" | Wie geht es weiter?

Wenn die Abschlussfrage im vorherigen Schritt (garbage-data or not?) mit: "nein, wir haben hier wirklich brauchbare Daten für unser Problem" beantwortet werden kann

```
7) Acquire even more "Expert-Knowledge" --> vertiefteres Einlesen in den Themenbereich.
  - Ziel hier: figure out how we can:
    1) visualize things AND 
    2) how to model / problem has already been solved by others --> AKA "intelligent copying" ;)
      - Lese Papers, 
      - Schaue ZF auf Youtube,
      - Twitter-Threads zum Thema,
      - Are there any Kaggle-Datasets on the topic?
      - etc...
8) Make a first "deeper" data exploration.
  - Goal: Let the data tell its story, e.g. **do some very good data-visualisations** in order for your boss to be able to understand with what data you are working AND _what insights it is possible to get with it_. 
    - Which visualization-libraries do you choose?
    - Clean the data
      - How to deal with Outliers?
      - Merging & Splitting different Datasets?
      - Which data-imputation-strategy should you choose?
      - etc...
``` 

More details to "step 7", on how to acquire "Expert-Knowledge":

- Make a list of Papers on the problem (Google Scholar), but only read the "introduction"!
  - <u>Key here is _"intelligent copying"-concept_</u>: You need to pick 2-3 papers that are the most relevant to your problem. Start with those.
- For any topics that you don't understand enough: 
  - Find Youtube-Videos on the topic(s). 
  - Twitter-Threads is also great. 
  - Furthermore, give Reddit a try!
- Next, choose which libraries are the most apropriate for the problem?
  - <u>Key here</u>: **Your end-result will only be as good as your tools, with which you will be able to tackle the problem**. 
    - <u>Example</u>: Remember my MA-thesis! The libraries I needed were in `Python`, but I solved it - very painfully & inefficiently - in `R`!
    
> How to find Answers quickly, especially when a Concept is complicated?

Aus Erfahrung weiss ich jetzt, dass **Youtube bisher *immer*, die beste Quelle war, um mir etwas schnell &amp; effizient beizubringen**. 

Twitter-Community von Wissenschaftlern sind ebenfalls sehr wertvoll. Als **Beispiel wäre [dieser Twitter-Post](https://twitter.com/seanjtaylor/status/1123278380369973248?s=11) vom Prophet-Gründer**.

> How to read Notebooks from other People?

*Es geht am Anfang um die Gesamtübersicht und noch nicht um die Details // Eigenheiten im Code oder im Datensatz! Am wichtigsten ist es, dass du [diese Fragen](#after-having-found-your-data-the-questions-you-need-to-have-an-answer-on) zunächst beantworten kannst, wenn du das Notebook liest.*
    
### Phase 4 - Model Estimation | Applying Machine-Learning on the Data

In this phase, we will want to generate predictions (most of the time). Here are the steps you need to consider:

```
9) Create // Estimate a model to make predictions.
  - Was ist das Prognoseobjekt (=$\hat{y}$ + dessen Einheit)?
  - Welche ML-Libraries willst du verwenden?
  - Clean the data
    - How to deal with Outliers?
    - Merging & Splitting of different Datasets?
    - Feature Engineering Strategy?
    - Which data-imputation-strategy should you choose?
    - etc...
  - Split the data into training & testing data
  - Estimate // Train your model (with the training data)
  - Evaluate your model (with Cross-Validation):
    - How good did it "learn"? // Performance on the training-data (MAE, MAPE,...)
    - How good did it "extrapolate"? // Performance on the testing-data (MAE, MAPE...)
  - Conclusions --> can we do better? 
    - Bedingung: know how to interpret the results & draw conclusions from it.
```

#### Further Questions that you need to have Answers on | Efficient (technical) Communication

**To efficiently communicate your results _with other Data Scientists_**, consider to send them answers to the following list of questions:

```
- Woher stammen die verwendeten Daten? --> deine Datenbasis
- Was ist das Prognoseobjekt? --> dein y_(dach)
- Was ist die "Input-Output-Struktur" der Analyse?
  - <u>(Beispiels-) Output</u>: Day ahead Strompreis CH
	- <u>(Beispiels-) Inputs</u>: jeweils 24h für ca. 20 Prädiktoren.
- Was ist die Daten-Imputation Methode? 
  - Beispiel: Missing Values mit Mittelwerten ersetzt.
- Welche Metrik hast du in deinen Notebooks verwendet? --> MAPE, MAE und / oder RMSE für Cross-Validation?
- Wie ist das Trainings- & Validierungs-Dataset aufgebaut?
- Welche Daten sind im Test-Set, welche für den Benchmark verwendet werden?
- Welche (bereits bestehende) Prognoseansätze wurden angewendet? --> Resultate aus deiner Literatur-Recherche 
- Welche Daten sind effektiv genutzt & welche sind verfügbar?
- Etc…
```

<mark>_With those **key-questions above**, a Data-Scientist should be able to assess the question: "is my data good or bad?", WITHOUT even creating ANY Jupyter-Notebook and running ANY lines of code in it!_</mark> 

### Phase 5 - Deployment: Erstellung eines Minimal Viable Product | Abschluss des Projektes

**If you are happy with your model in "step 9"**, you have a last step ahead of you: 

```
- Create your "minimal viable product" (MVP // Prototyp) from the (best) Model-Predictions that you got. 
  - Goal: Create a dashboard (oder sowas in der Art).
    - Choose which library to use:
      - dash, 
      - plotly, 
      - bokeh, 
      - R shiny-app, 
      - sphinx for documentation, 
      - bookdown 
      - etc...
```

## Fragestellungen beantworten

**In Data-Science - wie auch in der Wissenschaft - dreht sich alles um die Beantwortung von (relevanten) Fragen**. 

_Somit stellt die Datenanalyse schlussendlich nur das Werkzeug, mit welchem man eine bestimmte Frage(stellung) beantworten kann_.

In diesem Kapitel habe ich anhand von Beispielen versucht, **eine Liste zu mit diversen Methoden & konkreten Daten-Features** zu erstellen, mit denen man interessante Fragen zu beantworten versucht.

### Methoden | Liste an diverser Technologien, um Fragestellungen zu beantworten

- _Difference-in-Differences_
  - <u>Geeignet für</u>: **Kausaleffekte**, wobei hier ein Vergleich zweier _praktisch gleicher_ Gruppen über die Zeit stattfindet, um den Effekt eines "Treatments" (= treated OR not treated) zu messen.
  
## Find Data

*Der erste grosse Schritt bei allen Data-Science Projekten, ist der "Rohstoff" namens Daten! Ohne sie, kannst du jegliche DS-Projekte vergessen!*

Here is a list of **websites, that offer free to use datasets**:

- Kaggle
- UCI Machine Learning Repository
- [awesome-public-datasets](https://project-awesome.org/awesomedata/awesome-public-datasets)
- Google Dataset Search
- https://datasetsearch.research.google.com/

_Wenn du deine eigenen Daten sammeln willst, empfehle ich dir auch die Library `Beautiful-Soup`. Eventuell gibt es auch **einen Twitter-, Reddit- oder Youtube-Crawler API**, mit denen du wertvolle Daten sammeln könntest!_

### Sport-Daten

- https://football-data.co.uk/
  - <u>Description</u>: Football-Data is a free football betting portal providing historical results & odds to help football betting enthusiasts analyse many years of data quickly and efficiently to gain an edge over the bookmaker.
  
## Die "Kunst des Feature-Engineering" für gute Modellierung | Effizienter Modellieren

Was "gute" Modelle von "Schlechten" unterscheidet, ist oftmals, wie du deine `X`-Variablen für das Modell kallibriest. Das wird in der Data-Science Branche als "Feature Engineering" bezeichnet. 

**Hierbei geht es grundsätzlich um die Approximierung von "abstrakten Grössen" - zum Beispiel die Variable "Talent" bei der Prediction von "Student's Test-Scores" - mittels tatsächlich zur Verfügung stehende Daten**. 

<mark>Der _Key-Point_, den es hier zu verstehen gilt: "Talent" gibt es zwar schon, aber die Kunst besteht darin, eine _echte_, messbare Daten-Grösse zu haben, welche "Talent" approximiert</mark>.

### Geographie

- Approx. von "_Propensity to work in another country_" OR "_Propensity to buy cheap food abroad_"
  - <u>Benötigte Variable</u>: **Distance to Boarder**
  - <u>Themen</u>: Wurde in Card & Krueger (1992) Diff-in-Diff Methode verwendet

### Finance

- Approx. von "_Marktmacht_"
  - <u>Benötigte Variable</u>: **Marktanteil within a country**. _Hierfür musst du die Unternehmen der Branchen & deren Umsätze kennen_...
    - <u>Berechnung</u>: $Marktanteil-Firma_i= \frac{Umsatz_i}{\sum_{i=1}^N Umsatz_i}$
    - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
    - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).
- Approx. von "_unternehmerischer Erfolg_"
  - <u>Variable</u>: **Bruttomargen von Unternehmen**. 
    - <u>Definition & Kontext</u>: Sie messen, was Unternehmen vom Umsatz für sich behalten, _nachdem_ die Zahlungen an die Zulieferer von Waren abgezogen worden sind. Die Bruttomargen spiegeln im Grossen und Ganzen den Kostenblock, den sich die Unternehmen leisten. Ist er gross, spiegelt sich das auch in höheren Endpreise wider. Laut Brancheninsidern sollte eine gut geführte Supermarktkette mit einer Bruttomarge von 25 Prozent des Umsatzes auskommen, um damit ihre Aufwendungen für Personal, Mieten, Verwaltung, Werbung, Abschreibungen auf Maschinen und Weiteres zu bestreiten. 
      - <u>Wichtige Bemerkung</u>: **Viele Unternehmen machen aus ihren Bruttomargen ein grosses Geheimnis. publizieren ihre Bruttomargen nur auf Ebene des Gesamtkonzerns**.
  - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
  - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).
- Approx. für "_unternehmerische Effizienz_"
  - <u>Variablen</u>: 
    - <u>Auf der Seite der Einnahmen</u>: **Umsatz pro Quadratmeter an Unternehmensfläche**.
      - <u>Berechnung</u>: $Einnahmen-Effizienz_{i,j}= \frac{\sum_{j=1}^NUmsatz_j}{\sum_{j=1}^N Quadratmeter_j}$, wobei _i_ = Firma "i" & _j_ = Filiale "j"
    - <u>Auf der Seite der Ausgaben</u>: **Anzahl Mitarbeiter pro Quadratmeter an Unternehmensfläche**.
      - <u>Berechnung</u>: $Kosten-Effizienz_{i,j}= \frac{\sum_{j=1}^NMitarbeiter_j}{\sum_{j=1}^N Quadratmeter_j}$, wobei _i_ = Firma "i" & _j_ = Filiale "j"
        - <u>Einheit</u>: z.B. Mitarbeiter pro 1'000m<sup>2</sup>.
  - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
  - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).

## Allgemeine Schwächen von Data-Science

### Visualisierungen für Erklärungen verwenden

Aus eigener Erfahrung habe ich gemerkt, wie schwierig es ist, **extrem komplexe Themen innerhalb von wenigen Minuten einem _Laien_ zu erklären**. Das fällt jedoch nicht nur mir schwer. Das ist ein allgemeines Problem von der Data.

Obwohl das Problem schwierig zu meistern ist, liegt hier jedoch eines der grössten Potentiale: denn häufig **werden deine Cheffen Manager sein, die sich nicht gut in Data Science auskennen**. Entsprechend ist es sehr wichtig, schnell & effizient die richtigen Erklärungen zu finden, denn **diese finanzieren das Projekt**. 

_Wenn sie nicht überzeugt sind, dann wird genau deine Karriere als Data Scientist sehr kurz enden_! xD

- <u>Lösung</u>: Hier musst du insbesondere weiterhin auf deine Präsentations-Künste mittels Web-Development beharren. Das wird sich auszahlen!

### Nur 20% der Data-Science Projekte haben "Erfolg" | Apropos Geld...

Das ist eine absolut wahnsinnige Zahl! Dies zeigt, dass **Data Science Projekte relativ riskant sind, aus der Perspektive von Unternehmen**. 

- <u>Lösung</u>: Ich bin weiterhin überzeugt, dass das Anbieten einer "Freelancing"-Option für Data-Science eine sehr attraktive Lösung darstellt. So kannst du den Unternehmen genügend Flexibilität anbieten (indem du für weniger Lohn arbeitest) und gleichzeitig gewinnst Erfahrung.

## Statistics-Theory

### P-Hacking

> Was ist <u>p-hacking</u>?

In der Statistik gibt es den **p-Wert** ein: *Man nimmt an die Hypothese sei wahr und berechnet dann die Wahrscheinlichkeit, dass die beobachtete Statistik mindestens so extrem ausfallen würde (für die Gegner von Wischi-Waschi [hier die Wikipedia-Definition](https://de.wikipedia.org/wiki/P-Wert#Mathematische_Formulierung))*. Falls diese Wahrscheinlichkeit unter 5% liegt, dann sei das Resultat "statistisch signifikant" (yay!) und die Nullhypothese kann verworfen werden, was oftmals die Absicht ist. 

<u>Das **Problem** ist nur</u>: *Hypothesen gibt es viele und z.T. auch recht ähnliche*. Wenn man genug Hypothesen aufstellt - **vor allem, <u>nachdem</u> man sich die Daten angeschaut hat** - dann ist es durchaus möglich, dass man ein statistisch signifikantes Resultat erhält, unabhängig davon, ob das Resultat tatsächlich auch stimmt. **Das nennt man p-Hacking**. Es kommt häufig in der Forschung vor, aber es kommt sicher auch in der SBB vor (dennoch hier eine +1 für Hypothesen-basiertes arbeiten!). Wie einfach man in die "falsche Signifikanz Falle" tappen kann, wird hübsch in dieser Gallerie falscher Korrelationen illustriert.

## Appendix for the Future

> Welche Zeitperiode sind am geeignetsten für Zeitreihenanalysen mit Machine Learning?

