# Foundations of Data Science

In meinem Data Scientist Job werde ich häufig auf ähnliche Probleme stossen mit der Zeit. Hier habe ich eine Reihe an Fragen aufgelistet, welche ich fähig sein muss, zu beantworten, wenn ich effizient in meinem Beruf sein will!

## Allgemeine Roadmap für Data-Science Projekte

Die meisten Data-Science Projekte werden wie folgt ablaufen:

```
- Define the problem that you are trying to solve.
- Get the data you need to solve the problem
- Start with "understanding" your data:
  - What "X"-variables do you have? In which "Einheit"?
  - What is the Y-Variable? In which "Einheit"?
  - Make some very BASIC summary-statistics // just screen the data a little bit.
    - Key: You need to see whether your data is good enough OR garbage. Because then you don't even need to put efforts into trying to solve the problem! xD
  - If the data is OK: start with "Literatur-Recherche"
    - Einlesen ins Thema
    - Are there researchers that faced similar problems? What methods did they use?
- Clean the data
- Split the data into training & testing data
- Estimate // Train your model (with the training data)
- Evaluate your model:
  - How good did it "learn"? // Performance on the training-data (MAE, MAPE,...)
  - How good did it "extrapolate"? // Performance on the testing-data (MAE, MAPE...)
- Conclusions
- If you are happy with your model: Make a "Minimum Viable Product" (MVP):
  - For example, a dashboard on the web.
```

<mark>**Der erste und wichtigste Meilenstein ist vermutlich die Frage: 'Sind meine Daten "gut genug", um das Problem zu lösen oder sind sie einfach nur Müll?"**</mark>

_Wenn du nämlich die obige Frage mit "NEIN" beantwortest, dann brauchst du schon gar nicht mit dem Projekt zu beginnen, denn es wäre ineffizient_ ;)

### Find Data

*Der erste grosse Schritt bei allen Data-Science Projekten, ist der "Rohstoff" namens Daten! Ohne sie, kannst du jegliche DS-Projekte vergessen!*

Here is a list of **websites, that offer free to use datasets**:

- Kaggle
- UCI Machine Learning Repository
- [awesome-public-datasets](https://project-awesome.org/awesomedata/awesome-public-datasets)
- Google Dataset Search
- https://datasetsearch.research.google.com/

_Wenn du deine eigenen Daten sammeln willst, empfehle ich dir auch die Library `Beautiful-Soup`. Eventuell gibt es auch **einen Twitter-, Reddit- oder Youtube-Crawler API**, mit denen du wertvolle Daten sammeln könntest!_

### After having found your Data: the Questions you need to have an answer on

BEFORE you start writing your R-Scripts or Jupyter-Notebooks, you **first need to think about several key-things** &amp; -questions, <mark>because this will help you to come to the conclusion: "is my data good or bad?":

```
- Welche Datenbasis haben wir?
- Was ist das Prognoseobjekt?
- Welche Metrik hast du in deinen Notebooks verwendet?
- Wie ist das Trainings- & Validierungs-Dataset aufgebaut?
- Welche Daten sind im Test-Set, welche für den Benchmark verwendet werden?
- Welche (bereits bestehende) Prognoseansätze wurden angewendet? 
- Welche Daten sind effektiv genutzt & welche sind verfügbar?
- Etc…
```

- <u>Datenbasis // Woher kommen die Daten?</u>: 

ENTSOE exklusiv. Das ist der Dachverband der TSO (kennen Nachfrage und Angebot)

- <u>Prognoseobjekt (= y-dach)</u>: 

Stündliche EUR/kWh für Folgetag.

- <u>Metrik</u>: 

Zeigt, wie gut das Modell "lernt" (= training error) & "generalisiert" // exrapoliert (= generalization error). Im Notebook von Philipp werden 2 verschiedene 
Metriken verwendet, nämlich: 

	- Mean Absolute Error, sowie 
	- [Root] Mean Squared Error.

- <u>Training Dataset</u>: Daten von 2019.

- <u>Validation Dataset</u>: Random choice of 20% of the hours within this year.

- <u>Mit welchem *true y-value* werden die Predictions verglichen? // Was ist der Benchmark? // Test-Set</u>: EFFEX Spotpreise benutzt für Benchmark // true [y-]values.

- <u>Struktur [der Analyse]</u>:

	- <u>Output</u>: Day ahead Strompreis CH
	- <u>Input</u>: jeweils 24h für ca. 20 Prädiktoren. Reshape: K dimensionen = #Prediktors x 24h
	
- <u>Was ist die Daten-Imputation Methode?</u>: Missing Values mit Mittelwerten


### After having concluded: "My data is OK!" | Wie geht es weiter?

Hier mein Vorgehen, um jedes mögliche DS-Problem zu lösen,.

The list below basically starts at the "Literatur-Recherche" point:

```
- Make a list of Papers on the problem (Google Scholar), but only read the "introduction"!
  - Key here is "intelligent copying"-concept: You need to pick 2-3 papers that are the most relevant to your problem. Start with those.
- For any topics that you don't understand enough: Find Youtube-Videos on the topic(s). Twitter-Threads is also great. Furthermore, give Reddit a try!
- Next, choose which libraries are the most apropriate for the problem?
  - Key here: Your end-result will only be as good as your tools, with which you will be able to tackle the problem. Remember my MA-thesis: the libraries I needed were in Python, but I solved it (very painfully & inefficiently) in R!
- Start with the data cleaning.
- Estimate your Model.
- Evaluate the Model (with CV).
  - Conclusions? --> Key: Can we do better?
- Make a MVP, for example a dashboard.
```

> How to find Answers quickly, especially when a Concept is complicated?

Aus Erfahrung weiss ich jetzt, dass **Youtube bisher *immer*, die beste Quelle war, um mir etwas schnell &amp; effizient beizubringen**. 

Twitter-Community von Wissenschaftlern sind ebenfalls sehr wertvoll. Als **Beispiel wäre [dieser Twitter-Post](https://twitter.com/seanjtaylor/status/1123278380369973248?s=11) vom Prophet-Gründer**.

> How to read Notebooks from other People?

*Es geht am Anfang um die Gesamtübersicht und noch nicht um die Details // Eigenheiten im Code oder im Datensatz! Am wichtigsten ist es, dass du [diese Fragen](#after-having-found-your-data-the-questions-you-need-to-have-an-answer-on) zunächst beantworten kannst, wenn du das Notebook liest.*

## Fragestellungen beantworten

**In Data-Science - wie auch in der Wissenschaft - dreht sich alles um die Beantwortung von (relevanten) Fragen**. 

_Somit stellt die Datenanalyse schlussendlich nur das Werkzeug, mit welchem man eine bestimmte Frage(stellung) beantworten kann_.

In diesem Kapitel habe ich anhand von Beispielen versucht, **eine Liste zu mit diversen Methoden & konkreten Daten-Features** zu erstellen, mit denen man interessante Fragen zu beantworten versucht.

### Methoden | Liste an diverser Technologien, um Fragestellungen zu beantworten

- _Difference-in-Differences_
  - <u>Geeignet für</u>: **Kausaleffekte**, wobei hier ein Vergleich zweier _praktisch gleicher_ Gruppen über die Zeit stattfindet, um den Effekt eines "Treatments" (= treated OR not treated) zu messen.
  
## Die "Kunst des Feature-Engineering" für gute Modellierung | Effizienter Modellieren

Was "gute" Modelle von "Schlechten" unterscheidet, ist oftmals, wie du deine `X`-Variablen für das Modell kallibriest. Das wird in der Data-Science Branche als "Feature Engineering" bezeichnet. 

**Hierbei geht es grundsätzlich um die Approximierung von "abstrakten Grössen" - zum Beispiel die Variable "Talent" bei der Prediction von "Student's Test-Scores" - mittels tatsächlich zur Verfügung stehende Daten**. 

<mark>Der _Key-Point_, den es hier zu verstehen gilt: "Talent" gibt es zwar schon, aber die Kunst besteht darin, eine _echte_, messbare Daten-Grösse zu haben, welche "Talent" approximiert</mark>.

### Geographie

- Approx. von "_Propensity to work in another country_" OR "_Propensity to buy cheap food abroad_"
  - <u>Benötigte Variable</u>: **Distance to Boarder**
  - <u>Themen</u>: Wurde in Card & Krueger (1992) Diff-in-Diff Methode verwendet

### Finance

- Approx. von "_Marktmacht_"
  - <u>Benötigte Variable</u>: **Marktanteil within a country**. _Hierfür musst du die Unternehmen der Branchen & deren Umsätze kennen_...
    - <u>Berechnung</u>: $Marktanteil-Firma_i= \frac{Umsatz_i}{\sum_{i=1}^N Umsatz_i}$
    - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
    - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).
- Approx. von "_unternehmerischer Erfolg_"
  - <u>Variable</u>: **Bruttomargen von Unternehmen**. 
    - <u>Definition & Kontext</u>: Sie messen, was Unternehmen vom Umsatz für sich behalten, _nachdem_ die Zahlungen an die Zulieferer von Waren abgezogen worden sind. Die Bruttomargen spiegeln im Grossen und Ganzen den Kostenblock, den sich die Unternehmen leisten. Ist er gross, spiegelt sich das auch in höheren Endpreise wider. Laut Brancheninsidern sollte eine gut geführte Supermarktkette mit einer Bruttomarge von 25 Prozent des Umsatzes auskommen, um damit ihre Aufwendungen für Personal, Mieten, Verwaltung, Werbung, Abschreibungen auf Maschinen und Weiteres zu bestreiten. 
      - <u>Wichtige Bemerkung</u>: **Viele Unternehmen machen aus ihren Bruttomargen ein grosses Geheimnis. publizieren ihre Bruttomargen nur auf Ebene des Gesamtkonzerns**.
  - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
  - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).
- Approx. für "_unternehmerische Effizienz_"
  - <u>Variablen</u>: 
    - <u>Auf der Seite der Einnahmen</u>: **Umsatz pro Quadratmeter an Unternehmensfläche**.
      - <u>Berechnung</u>: $Einnahmen-Effizienz_{i,j}= \frac{\sum_{j=1}^NUmsatz_j}{\sum_{j=1}^N Quadratmeter_j}$, wobei _i_ = Firma "i" & _j_ = Filiale "j"
    - <u>Auf der Seite der Ausgaben</u>: **Anzahl Mitarbeiter pro Quadratmeter an Unternehmensfläche**.
      - <u>Berechnung</u>: $Kosten-Effizienz_{i,j}= \frac{\sum_{j=1}^NMitarbeiter_j}{\sum_{j=1}^N Quadratmeter_j}$, wobei _i_ = Firma "i" & _j_ = Filiale "j"
        - <u>Einheit</u>: z.B. Mitarbeiter pro 1'000m<sup>2</sup>.
  - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
  - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).


## Statistics-Theory

### P-Hacking

> Was ist <u>p-hacking</u>?

In der Statistik gibt es den **p-Wert** ein: *Man nimmt an die Hypothese sei wahr und berechnet dann die Wahrscheinlichkeit, dass die beobachtete Statistik mindestens so extrem ausfallen würde (für die Gegner von Wischi-Waschi [hier die Wikipedia-Definition](https://de.wikipedia.org/wiki/P-Wert#Mathematische_Formulierung))*. Falls diese Wahrscheinlichkeit unter 5% liegt, dann sei das Resultat "statistisch signifikant" (yay!) und die Nullhypothese kann verworfen werden, was oftmals die Absicht ist. 

<u>Das **Problem** ist nur</u>: *Hypothesen gibt es viele und z.T. auch recht ähnliche*. Wenn man genug Hypothesen aufstellt - **vor allem, <u>nachdem</u> man sich die Daten angeschaut hat** - dann ist es durchaus möglich, dass man ein statistisch signifikantes Resultat erhält, unabhängig davon, ob das Resultat tatsächlich auch stimmt. **Das nennt man p-Hacking**. Es kommt häufig in der Forschung vor, aber es kommt sicher auch in der SBB vor (dennoch hier eine +1 für Hypothesen-basiertes arbeiten!). Wie einfach man in die "falsche Signifikanz Falle" tappen kann, wird hübsch in dieser Gallerie falscher Korrelationen illustriert.

## Appendix for the Future

> Welche Zeitperiode sind am geeignetsten für Zeitreihenanalysen mit Machine Learning?

