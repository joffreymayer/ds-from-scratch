# Foundations of Data Science

In meinem Data Scientist Job werde ich häufig auf ähnliche Probleme stossen mit der Zeit. Hier habe ich eine Reihe an Fragen aufgelistet, welche ich fähig sein muss, zu beantworten, wenn ich effizient in meinem Beruf sein will!

## Allgemeine Roadmap für Data-Science Projekte

Die meisten Data-Science Projekte werden wie folgt ablaufen:

```
- Define the problem that you are trying to solve.
- Get the data you need to solve the problem
- Start with "understanding" your data:
  - What "X"-variables do you have? In which "Einheit"?
  - What is the Y-Variable? In which "Einheit"?
  - Make some very BASIC summary-statistics // just screen the data a little bit.
    - Key: You need to see whether your data is good enough OR garbage. Because then you don't even need to put efforts into trying to solve the problem! xD
  - If the data is OK: start with "Literatur-Recherche"
    - Einlesen ins Thema
    - Are there researchers that faced similar problems? What methods did they use?
- Clean the data
- Split the data into training & testing data
- Estimate // Train your model (with the training data)
- Evaluate your model:
  - How good did it "learn"? // Performance on the training-data (MAE, MAPE,...)
  - How good did it "extrapolate"? // Performance on the testing-data (MAE, MAPE...)
- Conclusions
- If you are happy with your model: Make a "Minimum Viable Product" (MVP):
  - For example, a dashboard on the web.
```

<mark>**Der erste und wichtigste Meilenstein ist vermutlich die Frage: 'Sind meine Daten "gut genug", um das Problem zu lösen oder sind sie einfach nur Müll?"**</mark>

_Wenn du nämlich die obige Frage mit "NEIN" beantwortest, dann brauchst du schon gar nicht mit dem Projekt zu beginnen, denn es wäre ineffizient_ ;)

### Find Data

*Der erste grosse Schritt bei allen Data-Science Projekten, ist der "Rohstoff" namens Daten! Ohne sie, kannst du jegliche DS-Projekte vergessen!*

Here is a list of **websites, that offer free to use datasets**:

- Kaggle
- UCI Machine Learning Repository
- [awesome-public-datasets](https://project-awesome.org/awesomedata/awesome-public-datasets)
- Google Dataset Search
- https://datasetsearch.research.google.com/

_Wenn du deine eigenen Daten sammeln willst, empfehle ich dir auch die Library `Beautiful-Soup`. Eventuell gibt es auch **einen Twitter-, Reddit- oder Youtube-Crawler API**, mit denen du wertvolle Daten sammeln könntest!_

### After having found your Data: the Questions you need to have an answer on

BEFORE you start writing your R-Scripts or Jupyter-Notebooks, you **first need to think about several key-things** &amp; -questions, <mark>because this will help you to come to the conclusion: "is my data good or bad?":

```
- Welche Datenbasis haben wir?
- Was ist das Prognoseobjekt?
- Welche Metrik hast du in deinen Notebooks verwendet?
- Wie ist das Trainings- & Validierungs-Dataset aufgebaut?
- Welche Daten sind im Test-Set, welche für den Benchmark verwendet werden?
- Welche (bereits bestehende) Prognoseansätze wurden angewendet? 
- Welche Daten sind effektiv genutzt & welche sind verfügbar?
- Etc…
```

- <u>Datenbasis // Woher kommen die Daten?</u>: 

ENTSOE exklusiv. Das ist der Dachverband der TSO (kennen Nachfrage und Angebot)

- <u>Prognoseobjekt (= y-dach)</u>: 

Stündliche EUR/kWh für Folgetag.

- <u>Metrik</u>: 

Zeigt, wie gut das Modell "lernt" (= training error) & "generalisiert" // exrapoliert (= generalization error). Im Notebook von Philipp werden 2 verschiedene 
Metriken verwendet, nämlich: 

	- Mean Absolute Error, sowie 
	- [Root] Mean Squared Error.

- <u>Training Dataset</u>: Daten von 2019.

- <u>Validation Dataset</u>: Random choice of 20% of the hours within this year.

- <u>Mit welchem *true y-value* werden die Predictions verglichen? // Was ist der Benchmark? // Test-Set</u>: EFFEX Spotpreise benutzt für Benchmark // true [y-]values.

- <u>Struktur [der Analyse]</u>:

	- <u>Output</u>: Day ahead Strompreis CH
	- <u>Input</u>: jeweils 24h für ca. 20 Prädiktoren. Reshape: K dimensionen = #Prediktors x 24h
	
- <u>Was ist die Daten-Imputation Methode?</u>: Missing Values mit Mittelwerten


### After having concluded: "My data is OK!" | Wie geht es weiter?

Hier mein Vorgehen, um jedes mögliche DS-Problem zu lösen,.

The list below basically starts at the "Literatur-Recherche" point:

```
- Make a list of Papers on the problem (Google Scholar), but only read the "introduction"!
  - Key here is "intelligent copying"-concept: You need to pick 2-3 papers that are the most relevant to your problem. Start with those.
- For any topics that you don't understand enough: Find Youtube-Videos on the topic(s). Twitter-Threads is also great. Furthermore, give Reddit a try!
- Next, choose which libraries are the most apropriate for the problem?
  - Key here: Your end-result will only be as good as your tools, with which you will be able to tackle the problem. Remember my MA-thesis: the libraries I needed were in Python, but I solved it (very painfully & inefficiently) in R!
- Start with the data cleaning.
- Estimate your Model.
- Evaluate the Model (with CV).
  - Conclusions? --> Key: Can we do better?
- Make a MVP, for example a dashboard.
```

> How to find Answers quickly, especially when a Concept is complicated?

Aus Erfahrung weiss ich jetzt, dass **Youtube bisher *immer*, die beste Quelle war, um mir etwas schnell &amp; effizient beizubringen**. 

Twitter-Community von Wissenschaftlern sind ebenfalls sehr wertvoll. Als **Beispiel wäre [dieser Twitter-Post](https://twitter.com/seanjtaylor/status/1123278380369973248?s=11) vom Prophet-Gründer**.

> How to read Notebooks from other People?

*Es geht am Anfang um die Gesamtübersicht und noch nicht um die Details // Eigenheiten im Code oder im Datensatz! Am wichtigsten ist es, dass du [diese Fragen](#after-having-found-your-data-the-questions-you-need-to-have-an-answer-on) zunächst beantworten kannst, wenn du das Notebook liest.*

## Fragestellungen beantworten

**In Data-Science - wie auch in der Wissenschaft - dreht sich alles um die Beantwortung von (relevanten) Fragen**. 

_Somit stellt die Datenanalyse schlussendlich nur das Werkzeug, mit welchem man eine bestimmte Frage(stellung) beantworten kann_.

In diesem Kapitel habe ich anhand von Beispielen versucht, **eine Liste zu mit diversen Methoden & konkreten Daten-Features** zu erstellen, mit denen man interessante Fragen zu beantworten versucht.

### Methoden | Liste an diverser Technologien, um Fragestellungen zu beantworten

- _Difference-in-Differences_
  - <u>Geeignet für</u>: **Kausaleffekte**, wobei hier ein Vergleich zweier _praktisch gleicher_ Gruppen über die Zeit stattfindet, um den Effekt eines "Treatments" (= treated OR not treated) zu messen.
  
## Die "Kunst des Feature-Engineering" für gute Modellierung | Effizienter Modellieren

Was "gute" Modelle von "Schlechten" unterscheidet, ist oftmals, wie du deine `X`-Variablen für das Modell kallibriest. Das wird in der Data-Science Branche als "Feature Engineering" bezeichnet. 

**Hierbei geht es grundsätzlich um die Approximierung von "abstrakten Grössen" - zum Beispiel die Variable "Talent" bei der Prediction von "Student's Test-Scores" - mittels tatsächlich zur Verfügung stehende Daten**. 

<mark>Der _Key-Point_, den es hier zu verstehen gilt: "Talent" gibt es zwar schon, aber die Kunst besteht darin, eine _echte_, messbare Daten-Grösse zu haben, welche "Talent" approximiert</mark>.

### Geographie

- Approx. von "_Propensity to work in another country_" OR "_Propensity to buy cheap food abroad_"
  - <u>Benötigte Variable</u>: **Distance to Boarder**
  - <u>Themen</u>: Wurde in Card & Krueger (1992) Diff-in-Diff Methode verwendet

### Finance

- Approx. von "_Marktmacht_"
  - <u>Benötigte Variable</u>: **Marktanteil within a country**. _Hierfür musst du die Unternehmen der Branchen & deren Umsätze kennen_...
    - <u>Berechnung</u>: $Marktanteil-Firma_i= \frac{Umsatz_i}{\sum_{i=1}^N Umsatz_i}$
    - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
    - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).
- Approx. von "_unternehmerischer Erfolg_"
  - <u>Variable</u>: **Bruttomargen von Unternehmen**. 
    - <u>Definition & Kontext</u>: Sie messen, was Unternehmen vom Umsatz für sich behalten, _nachdem_ die Zahlungen an die Zulieferer von Waren abgezogen worden sind. Die Bruttomargen spiegeln im Grossen und Ganzen den Kostenblock, den sich die Unternehmen leisten. Ist er gross, spiegelt sich das auch in höheren Endpreise wider. Laut Brancheninsidern sollte eine gut geführte Supermarktkette mit einer Bruttomarge von 25 Prozent des Umsatzes auskommen, um damit ihre Aufwendungen für Personal, Mieten, Verwaltung, Werbung, Abschreibungen auf Maschinen und Weiteres zu bestreiten. 
      - <u>Wichtige Bemerkung</u>: **Viele Unternehmen machen aus ihren Bruttomargen ein grosses Geheimnis. publizieren ihre Bruttomargen nur auf Ebene des Gesamtkonzerns**.
  - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
  - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).
- Approx. für "_unternehmerische Effizienz_"
  - <u>Variablen</u>: 
    - <u>Auf der Seite der Einnahmen</u>: **Umsatz pro Quadratmeter an Unternehmensfläche**.
      - <u>Berechnung</u>: $Einnahmen-Effizienz_{i,j}= \frac{\sum_{j=1}^NUmsatz_j}{\sum_{j=1}^N Quadratmeter_j}$, wobei _i_ = Firma "i" & _j_ = Filiale "j"
    - <u>Auf der Seite der Ausgaben</u>: **Anzahl Mitarbeiter pro Quadratmeter an Unternehmensfläche**.
      - <u>Berechnung</u>: $Kosten-Effizienz_{i,j}= \frac{\sum_{j=1}^NMitarbeiter_j}{\sum_{j=1}^N Quadratmeter_j}$, wobei _i_ = Firma "i" & _j_ = Filiale "j"
        - <u>Einheit</u>: z.B. Mitarbeiter pro 1'000m<sup>2</sup>.
  - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
  - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).

## Machine-Learning Theory

### Difference between Training-, Validation- &amp; Test-dataset

Since I started with Machine Learning, I was always confused about the concept of **data splitting**, e.g. which sub-set of the *entire* dataset is now considered to be the *training dataset*, which one is the *validation dataset* and which one is the *testing dataset*. 

<u>In the graph below, you see how it is defined correctly</u>:

![Training Set VS. Validation Set VS. Test Set](./bilder/training-VS-validation-VS-test-ddset.jpg)

- <u>Important to note</u>: There is oftentimes confusion between the definition of *validation dataset* VS. *testing dataset*, because there is no consens about it. **Therefore - and if we take the above picture as the "true" definition - some people will call the *validation dataset* the "test dataset" and vice versa, e.g. the *test data* as the "validation data"! xD** 

#### Validation-Set VS. Test-Set

> What is the difference between a `Validation Dataset` and a `Testing Dataset`?

Es gibt keinen Konsens dafür, was nun das `validation dataset` und welches das `test set` genau ist. **Nima** (= mein Mentor bei der SBB) verwendet den Begriff `test set` für dasjenige Dataset, welches `hold out` // separat für die (spätere) `Prediction` verwendet wird.

- <u>Regel zu Unterscheidung der beiden Terme</u>: Wenn steht: `we hold out [name of the set]`, dann ist es das `testing dataset`.

#### Reason for a Validation-Set

> Why do we need `validation set`?

At the end, you want the best model possible. If you want to tune your hyperparameters, you will need your `test set`. **The big problem here, however, is that - *if we use the test-set more than once when wanting to find out the best hyperparameters* - our model will know the data by heart and the predictions will be too good, <u>but only</u> on this dataset that you are currently using**! The model will not generalize well on new data. 

*That's why we need this additional subsetting of the training-dataset!*

### Concept of Stratification

Angenommen du hast eine *kategorische* Y-Variable, welche entweder Nullen oder Einsen als Werte annimmt. Nehme nun an, dass - *im Training-Dataset* - der Anteil der Nullen 60% beträgt. Wenn du nun ein `stratified Sample` willst, dann wird das *Test-Dataset* ebenfalls einen Anteil von 60% an Nullen enthalten!

- <u>Quelle</u>: [Train-Test-Split in Sklearn and Cross-Validation](https://www.youtube.com/watch?v=KEBS7Kyc0Po)

### Cross-Validation

The concept of *cross-validation* can be splitted into 2 parts:

- <u>Step 1</u>: Split your dataset into 1 training- &amp; 1 test-set.
	- <u>Rule of Thumb</u>: Usually, the split is **70-90% training set** and **10-30% for the test-set**.
	- <u>Code Example</u>: 

![Example of Code for Train-Test Split](./bilder/code-example-cross-validation.png)

- <u>Step 2</u>: Now, we divide the *training set* further, such that it will contain - *if we assume a 3-fold cross-validation as an example* -  **3 <u>different</u> validation sets** and **3 training sets**.

![Allgemeine Cross-Validation](./bilder/example-of-a-general-3-fold-cross-validation.png)

#### <u>Special Case</u>: Cross-Validation for Time-Series Data

Because time-series are ordered, since *the flow of time* is only going forward, the graph shown above for *step 2* is <u>not</u> valid and we need another approach: 

#### Why do we use Cross-Validation?

There are 2 reasons:

1) It allows us **to compare the score (MAPE, MAE or RMSE) of different model set-ups**, for example: 
  - *CV-Scheme changes*, e.g. a _Time-Series_ Prophet-Model with a `Sliding-Window` of 4 Months VS. a Prophet-Model with an `Expanding-Window` (full past).
  - OR *changes in the covariates*, e.g. a model that includes an important covariate, while the other model does not.
  **This will allow you to draw conclusions regarding the selection of "the best" model**.
2) It allows you **to assess the performance** of different machine-learning methods. In a _classification-setting_ for example, you can use the _confusion-matrix_. 

![Cross-Validation for Time Series](./bilder/example-3-fold-cross-validation-for-time-series.png)

This picture shows **an example of a 3-fold cross-validation for a time-series**. 

### Normalisierung der Daten

> Was versteht man unter `Normalisierung`? Was sollte man dabei beachten? Warum wird in einem Pre-Processing Schritt eine Normalisierung durchgeführt?

`Normalisierung` == **Subtrahiere den Mittelwert und dividiere durch die Standardabweichung jedes Merkmals**. Es sollte beachtet werden, dass der Mittelwert und die Standardabweichung **nur anhand der Trainings-Daten berechnet werden sollte**, damit die Modelle keinen Zugriff auf die Werte in den Validierungs- und Testsätzen haben.

**Es ist wichtig, Features zu skalieren, bevor ein neuronales Netzwerk trainiert wird**. Normalisierung ist eine gängige Methode für diese Skalierung.

### RNN

> Was macht ein `Reccurrent Neural Network (RNN)`?

Ein Beispiel für ein RNN wäre ein `Long Short Term Memory` Modell (LSTM Modell). Dabei nimmt das RNN zunächst ein ganz kleines Vergangenheits-Intervall, macht eine Modell-Estimation und dann - im nächsten Schritt - wird ein grösseres Vergangenheits-Intervall verwendet (**inkl. predicted Y-Variable aus dem vorherigen Modell**), um eine neue Modell-Estimation zu machen etc.

![Beispiel eines RNN: hier ein LSTM](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/lstm_many_window.png?raw=1)

### Data-Pipelines

**Data Pipelines use an input to produce an output and then - on a second step - use the produced output as an input to produce another output etc...**

- <u>Visualisierung</u>: Du kannst dir unter _Data Pipelines_ nichts anderes als eine "Guetzli Fabrik" vorstellen, welche diverse Produktionsmaschinen verwendet - zum Beispiel einen Teig-Cutter, dann einen Butter-Schmierer, sowie einen fetten Ofen und einen Sortierer von "guten VS. schlechte Guetzli" - um die Inputs immer weiter zu verarbeiten, sodass schlussendlich ein Endprodukt (= die fertigen "Guetzli") ensteht, dass man verkaufen kann.

> [How to create good Data-Pipelines in Scikit-Learn?](https://www.youtube.com/watch?v=w9IGkBfOoic)

 - <u>Ziel von Data-Pipeline</u>: Write more clean // readable code, especially when you do <em>data cleaning</em>. **A datapipeline is basically a way of standardizing your code**.
 - <u>Warum sind Data-Pipelines so geil?</u>: Because you can **compare many different regression-models (Linear-Regression Vs. Logistic-Regression Vs. RandomForrest ...), applying different "scaling-techniques" (= normalize a variable with mean 0 and standard-deviation of 1), as well as using "data cleaning techniques" (= reduce dimensions via PCA, reduce missing-values etc...)**. Another cool thing to note is, that **you can choose the order, in which the cleaning, scaling and fitting occures!**
   - See also the summary of the guy [on Youtube ab 10:00-11:00](https://www.youtube.com/watch?v=w9IGkBfOoic).
- <u>Link to Github for an example</u>: <a href="https://github.com/krishnaik06/Pipelines-Using-Sklearn/blob/master/SklearnPipeline.ipynb" target="_blank">Jupyter-Notebook code-example on how to create a little Data-Pipeline by yourself</a>

## Programming Theory

### Data-Types

In `R` oder `Python` ist es wichtig zu verstehen, dass gewisse Funktionen nur dann funktionieren, wenn die Inputs, die wir in die Funktion eingeben wollen, einen bestimmten Data-Type aufweisen müssen.

On the website *W3-Schools*, I found this [extremely good overview of all data-types](https://www.w3schools.com/python/python_datatypes.asp), which is crucial concept to understand when doing data cleaning. 

![Overview of different Data-Types](./bilder/data-types-overview.jpg)
*Data-Types are a key-thing to understand. Otherwise, you won't be able to apply some algorithms on your dataset!*

### Global Variables VS. Local Variables

Variables that are created **outside of a function** are known as *global variables*.

Global variables can be used by everyone, both **inside of functions <u>and</u> outside**.

- <u>Example of a global variable</u>:

```
x = "awesome"

def myfunc():
  print("Python is " + x)

myfunc()

>>> Python is awesome
```

In contrast, if you create a variable *with the same name* <u>inside</u> a function, this variable will be **local**, and **can only be used <u>inside</u> the function**. The global variable with the *same* name will remain as it was, global and with the original value.

- <u>Example of a local variable</u>:

```
x = "awesome"

def myfunc():
  x = "fantastic"
  print("Python is " + x)

myfunc()

print("Python is " + x)
```
- Output of this: [click here](https://www.w3schools.com/python/trypython.asp?filename=demo_variables_global2)

### Array

> What is an `array`?

An `Array` is a List of Data. In a `DataFrame`-Object, you can think of a `column` *or* a `row` to be `arrays`. It is a data structure, which contains "n" objects within a list. 
	- <u>Quelle</u>: [The Coding Train 3:10-3:22](https://www.youtube.com/watch?v=NptnmWvkbTw)


## Statistics-Theory

### P-Hacking

> Was ist <u>p-hacking</u>?

In der Statistik gibt es den **p-Wert** ein: *Man nimmt an die Hypothese sei wahr und berechnet dann die Wahrscheinlichkeit, dass die beobachtete Statistik mindestens so extrem ausfallen würde (für die Gegner von Wischi-Waschi [hier die Wikipedia-Definition](https://de.wikipedia.org/wiki/P-Wert#Mathematische_Formulierung))*. Falls diese Wahrscheinlichkeit unter 5% liegt, dann sei das Resultat "statistisch signifikant" (yay!) und die Nullhypothese kann verworfen werden, was oftmals die Absicht ist. 

<u>Das **Problem** ist nur</u>: *Hypothesen gibt es viele und z.T. auch recht ähnliche*. Wenn man genug Hypothesen aufstellt - **vor allem, <u>nachdem</u> man sich die Daten angeschaut hat** - dann ist es durchaus möglich, dass man ein statistisch signifikantes Resultat erhält, unabhängig davon, ob das Resultat tatsächlich auch stimmt. **Das nennt man p-Hacking**. Es kommt häufig in der Forschung vor, aber es kommt sicher auch in der SBB vor (dennoch hier eine +1 für Hypothesen-basiertes arbeiten!). Wie einfach man in die "falsche Signifikanz Falle" tappen kann, wird hübsch in dieser Gallerie falscher Korrelationen illustriert.

## Appendix for the Future

> Welche Zeitperiode sind am geeignetsten für Zeitreihenanalysen mit Machine Learning?




