# Foundations of Data Science

In meinem Data Scientist Job werde ich häufig auf ähnliche Probleme stossen mit der Zeit. Hier habe ich eine Reihe an Fragen aufgelistet, welche ich fähig sein muss, zu beantworten, wenn ich effizient in meinem Beruf sein will!

## Allgemeine Roadmap für Data-Science Projekte

Die meisten Data-Science Projekte werden wie folgt ablaufen:

![Übersicht zu den 4 Teil-Etappen in Data-Science](./bilder/foundation-ds/roadmap-uebersicht.jpg)

### Management eines DS-Projektes | The Step-by-Step Guide

1) Create a **new folder** with the project's name.
2) Create a **virtual environment** `venv` for the project (in order to solve dependency issues).
3) Create **a `readme.md`-file** on how to set-up your computer.
  - Copy-Paste this from a previous project.
4) Get the data.
5) Acquire _some_ (minimal) understanding of your topic // research-area (Achtung: not too deep yet!).
  - <u>Goal</u>: Formulate a resarch-question. 
    - What would be interesting questions to answer with this type of data that you have? 
    - Can you formulate a hypothesis?
  - Google some books on the topics.
    - Having a look at Bibliotheks-Bücher can also be helpful.
  - Read some introductions on the topic.
    - Are there any White-Papers?
    - Have you seen any meta-paper on the topic? These give a really great overview on the topic!
6) Load the data: the <u>goal</u> here is to **get acquianted with your data**. What is your first impression of what you got here?
  - How many columns & rows?
  - `Dtype` of each column?
  - What data does each column represent?
    - <u>Problem you want to solve</u>: Oftentimes, columns have weird names and you don't know what those abbreviations stand for.
  - What are the possible ranges of values for each <u>categorical</u> data-column?
  - <u>Final goal here</u>: _You need to decide **whether your data is good enough OR garbage**. Because then you don't even need to put efforts into trying to solve the problem!_ xD
7) <u>Wenn die Abschlussfrage im vorherigen Schritt (garbage-data or not?) mit: "nein, wir haben hier wirklich brauchbare Daten für unser Problem" beantwortet werden kann</u>: Acquire even more "Expert-Knowledge" --> vertiefteres Einlesen in den Themenbereich.
  - <u>Ziel hier</u>: figure out what (similar) model / problem has already been solved by others --> AKA "intelligent copying" ;)
    - Lese Papaers, 
    - Schaue ZF auf Youtube,
    - Twitter-Threads zum Thema,
    - Are there any Kaggle-Datasets on the topic?
    - etc...
8) Make a first "deeper" data exploration.
  - <u>Goal</u>: Let the data tell its story, e.g. **do some very good data-visualisations** in order for your boss to be able to understand with what data you are working AND _what insights it is possible to get with it_. 
9) Create // Estimate a model to make predictions.
  - Was ist das Prognoseobjekt (=$\hat{y}$ + dessen Einheit)?
  - Welche ML-Libraries willst du verwenden?
  - Clean the data
  - Split the data into training & testing data
  - Estimate // Train your model (with the training data)
  - Evaluate your model:
    - How good did it "learn"? // Performance on the training-data (MAE, MAPE,...)
    - How good did it "extrapolate"? // Performance on the testing-data (MAE, MAPE...)
  - Conclusions: know how to interpret the results & draw conclusions from it.
10) If you are happy with your model in "step 9": create your "minimal viable product" (MVP // Prototyp) from the (best) Model-Predictions that you got. 
  - <u>Goal</u>: Create a dashboard (oder sowas in der Art).
    - Choose which library to use (dash, plotly, bokeh, shiny-app, sphinx, bookdown etc...)

## Data-Science, aus "Uni"-Sicht

Dies ist eine alternative Übersicht, wie man ein Data-Science Projekt von A bis Z durchführt (eventuell veraltet?):

```
- Define the problem that you are trying to solve.
- Get the data you need to solve the problem
- Start with "understanding" your data:
  - What "X"-variables do you have? In which "Einheit"?
  - What is the Y-Variable? In which "Einheit"?
  - Make some very BASIC summary-statistics // just screen the data a little bit.
    - Key: You need to see whether your data is good enough OR garbage. Because then you don't even need to put efforts into trying to solve the problem! xD
  - If the data is OK: start with "Literatur-Recherche"
    - Einlesen ins Thema
    - Are there researchers that faced similar problems? What methods did they use?
- Clean the data
- Split the data into training & testing data
- Estimate // Train your model (with the training data)
- Evaluate your model:
  - How good did it "learn"? // Performance on the training-data (MAE, MAPE,...)
  - How good did it "extrapolate"? // Performance on the testing-data (MAE, MAPE...)
- Conclusions
- If you are happy with your model: Make a "Minimum Viable Product" (MVP):
  - For example, a dashboard on the web.
```

<mark>**Der erste und wichtigste Meilenstein ist vermutlich die Frage: 'Sind meine Daten "gut genug", um das Problem zu lösen oder sind sie einfach nur Müll?"**</mark>

_Wenn du nämlich die obige Frage mit "NEIN" beantwortest, dann brauchst du schon gar nicht mit dem Projekt zu beginnen, denn es wäre ineffizient_ ;)

### After having found your Data: the Questions you need to have an answer on

BEFORE you start writing your R-Scripts or Jupyter-Notebooks, you **first need to think about several key-things** &amp; -questions, <mark>because this will help you to come to the conclusion: "is my data good or bad?":

```
- Welche Datenbasis haben wir?
- Was ist das Prognoseobjekt?
- Welche Metrik hast du in deinen Notebooks verwendet?
- Wie ist das Trainings- & Validierungs-Dataset aufgebaut?
- Welche Daten sind im Test-Set, welche für den Benchmark verwendet werden?
- Welche (bereits bestehende) Prognoseansätze wurden angewendet? 
- Welche Daten sind effektiv genutzt & welche sind verfügbar?
- Etc…
```

- <u>Datenbasis // Woher kommen die Daten?</u>: 

ENTSOE exklusiv. Das ist der Dachverband der TSO (kennen Nachfrage und Angebot)

- <u>Prognoseobjekt (= y-dach)</u>: 

Stündliche EUR/kWh für Folgetag.

- <u>Metrik</u>: 

Zeigt, wie gut das Modell "lernt" (= training error) & "generalisiert" // exrapoliert (= generalization error). Im Notebook von Philipp werden 2 verschiedene 
Metriken verwendet, nämlich: 

	- Mean Absolute Error, sowie 
	- [Root] Mean Squared Error.

- <u>Training Dataset</u>: Daten von 2019.

- <u>Validation Dataset</u>: Random choice of 20% of the hours within this year.

- <u>Mit welchem *true y-value* werden die Predictions verglichen? // Was ist der Benchmark? // Test-Set</u>: EFFEX Spotpreise benutzt für Benchmark // true [y-]values.

- <u>Struktur [der Analyse]</u>:

	- <u>Output</u>: Day ahead Strompreis CH
	- <u>Input</u>: jeweils 24h für ca. 20 Prädiktoren. Reshape: K dimensionen = #Prediktors x 24h
	
- <u>Was ist die Daten-Imputation Methode?</u>: Missing Values mit Mittelwerten


### After having concluded: "My data is OK!" | Wie geht es weiter?

Hier mein Vorgehen, um jedes mögliche DS-Problem zu lösen,.

The list below basically starts at the "Literatur-Recherche" point:

```
- Make a list of Papers on the problem (Google Scholar), but only read the "introduction"!
  - Key here is "intelligent copying"-concept: You need to pick 2-3 papers that are the most relevant to your problem. Start with those.
- For any topics that you don't understand enough: Find Youtube-Videos on the topic(s). Twitter-Threads is also great. Furthermore, give Reddit a try!
- Next, choose which libraries are the most apropriate for the problem?
  - Key here: Your end-result will only be as good as your tools, with which you will be able to tackle the problem. Remember my MA-thesis: the libraries I needed were in Python, but I solved it (very painfully & inefficiently) in R!
- Start with the data cleaning.
- Estimate your Model.
- Evaluate the Model (with CV).
  - Conclusions? --> Key: Can we do better?
- Make a MVP, for example a dashboard.
```

> How to find Answers quickly, especially when a Concept is complicated?

Aus Erfahrung weiss ich jetzt, dass **Youtube bisher *immer*, die beste Quelle war, um mir etwas schnell &amp; effizient beizubringen**. 

Twitter-Community von Wissenschaftlern sind ebenfalls sehr wertvoll. Als **Beispiel wäre [dieser Twitter-Post](https://twitter.com/seanjtaylor/status/1123278380369973248?s=11) vom Prophet-Gründer**.

> How to read Notebooks from other People?

*Es geht am Anfang um die Gesamtübersicht und noch nicht um die Details // Eigenheiten im Code oder im Datensatz! Am wichtigsten ist es, dass du [diese Fragen](#after-having-found-your-data-the-questions-you-need-to-have-an-answer-on) zunächst beantworten kannst, wenn du das Notebook liest.*

## Fragestellungen beantworten

**In Data-Science - wie auch in der Wissenschaft - dreht sich alles um die Beantwortung von (relevanten) Fragen**. 

_Somit stellt die Datenanalyse schlussendlich nur das Werkzeug, mit welchem man eine bestimmte Frage(stellung) beantworten kann_.

In diesem Kapitel habe ich anhand von Beispielen versucht, **eine Liste zu mit diversen Methoden & konkreten Daten-Features** zu erstellen, mit denen man interessante Fragen zu beantworten versucht.

### Methoden | Liste an diverser Technologien, um Fragestellungen zu beantworten

- _Difference-in-Differences_
  - <u>Geeignet für</u>: **Kausaleffekte**, wobei hier ein Vergleich zweier _praktisch gleicher_ Gruppen über die Zeit stattfindet, um den Effekt eines "Treatments" (= treated OR not treated) zu messen.
  
## Find Data

*Der erste grosse Schritt bei allen Data-Science Projekten, ist der "Rohstoff" namens Daten! Ohne sie, kannst du jegliche DS-Projekte vergessen!*

Here is a list of **websites, that offer free to use datasets**:

- Kaggle
- UCI Machine Learning Repository
- [awesome-public-datasets](https://project-awesome.org/awesomedata/awesome-public-datasets)
- Google Dataset Search
- https://datasetsearch.research.google.com/

_Wenn du deine eigenen Daten sammeln willst, empfehle ich dir auch die Library `Beautiful-Soup`. Eventuell gibt es auch **einen Twitter-, Reddit- oder Youtube-Crawler API**, mit denen du wertvolle Daten sammeln könntest!_

### Sport-Daten

- https://football-data.co.uk/
  - <u>Description</u>: Football-Data is a free football betting portal providing historical results & odds to help football betting enthusiasts analyse many years of data quickly and efficiently to gain an edge over the bookmaker.
  
## Die "Kunst des Feature-Engineering" für gute Modellierung | Effizienter Modellieren

Was "gute" Modelle von "Schlechten" unterscheidet, ist oftmals, wie du deine `X`-Variablen für das Modell kallibriest. Das wird in der Data-Science Branche als "Feature Engineering" bezeichnet. 

**Hierbei geht es grundsätzlich um die Approximierung von "abstrakten Grössen" - zum Beispiel die Variable "Talent" bei der Prediction von "Student's Test-Scores" - mittels tatsächlich zur Verfügung stehende Daten**. 

<mark>Der _Key-Point_, den es hier zu verstehen gilt: "Talent" gibt es zwar schon, aber die Kunst besteht darin, eine _echte_, messbare Daten-Grösse zu haben, welche "Talent" approximiert</mark>.

### Geographie

- Approx. von "_Propensity to work in another country_" OR "_Propensity to buy cheap food abroad_"
  - <u>Benötigte Variable</u>: **Distance to Boarder**
  - <u>Themen</u>: Wurde in Card & Krueger (1992) Diff-in-Diff Methode verwendet

### Finance

- Approx. von "_Marktmacht_"
  - <u>Benötigte Variable</u>: **Marktanteil within a country**. _Hierfür musst du die Unternehmen der Branchen & deren Umsätze kennen_...
    - <u>Berechnung</u>: $Marktanteil-Firma_i= \frac{Umsatz_i}{\sum_{i=1}^N Umsatz_i}$
    - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
    - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).
- Approx. von "_unternehmerischer Erfolg_"
  - <u>Variable</u>: **Bruttomargen von Unternehmen**. 
    - <u>Definition & Kontext</u>: Sie messen, was Unternehmen vom Umsatz für sich behalten, _nachdem_ die Zahlungen an die Zulieferer von Waren abgezogen worden sind. Die Bruttomargen spiegeln im Grossen und Ganzen den Kostenblock, den sich die Unternehmen leisten. Ist er gross, spiegelt sich das auch in höheren Endpreise wider. Laut Brancheninsidern sollte eine gut geführte Supermarktkette mit einer Bruttomarge von 25 Prozent des Umsatzes auskommen, um damit ihre Aufwendungen für Personal, Mieten, Verwaltung, Werbung, Abschreibungen auf Maschinen und Weiteres zu bestreiten. 
      - <u>Wichtige Bemerkung</u>: **Viele Unternehmen machen aus ihren Bruttomargen ein grosses Geheimnis. publizieren ihre Bruttomargen nur auf Ebene des Gesamtkonzerns**.
  - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
  - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).
- Approx. für "_unternehmerische Effizienz_"
  - <u>Variablen</u>: 
    - <u>Auf der Seite der Einnahmen</u>: **Umsatz pro Quadratmeter an Unternehmensfläche**.
      - <u>Berechnung</u>: $Einnahmen-Effizienz_{i,j}= \frac{\sum_{j=1}^NUmsatz_j}{\sum_{j=1}^N Quadratmeter_j}$, wobei _i_ = Firma "i" & _j_ = Filiale "j"
    - <u>Auf der Seite der Ausgaben</u>: **Anzahl Mitarbeiter pro Quadratmeter an Unternehmensfläche**.
      - <u>Berechnung</u>: $Kosten-Effizienz_{i,j}= \frac{\sum_{j=1}^NMitarbeiter_j}{\sum_{j=1}^N Quadratmeter_j}$, wobei _i_ = Firma "i" & _j_ = Filiale "j"
        - <u>Einheit</u>: z.B. Mitarbeiter pro 1'000m<sup>2</sup>.
  - <u>Where to get the Data?</u>: Beispielsweise aus **Geschäftsberichten**.
  - <u>Themen</u>: Übertragung der Teuerung der Lebensmittel auf die Konsumenten von Migros & Coop (wegen Krieg in der Ukraine, 2022).

## Allgemeine Schwächen von Data-Science

### Visualisierungen für Erklärungen verwenden

Aus eigener Erfahrung habe ich gemerkt, wie schwierig es ist, **extrem komplexe Themen innerhalb von wenigen Minuten einem _Laien_ zu erklären**. Das fällt jedoch nicht nur mir schwer. Das ist ein allgemeines Problem von der Data.

Obwohl das Problem schwierig zu meistern ist, liegt hier jedoch eines der grössten Potentiale: denn häufig **werden deine Cheffen Manager sein, die sich nicht gut in Data Science auskennen**. Entsprechend ist es sehr wichtig, schnell & effizient die richtigen Erklärungen zu finden, denn **diese finanzieren das Projekt**. 

_Wenn sie nicht überzeugt sind, dann wird genau deine Karriere als Data Scientist sehr kurz enden_! xD

- <u>Lösung</u>: Hier musst du insbesondere weiterhin auf deine Präsentations-Künste mittels Web-Development beharren. Das wird sich auszahlen!

### Nur 20% der Data-Science Projekte haben "Erfolg" | Apropos Geld...

Das ist eine absolut wahnsinnige Zahl! Dies zeigt, dass **Data Science Projekte relativ riskant sind, aus der Perspektive von Unternehmen**. 

- <u>Lösung</u>: Ich bin weiterhin überzeugt, dass das Anbieten einer "Freelancing"-Option für Data-Science eine sehr attraktive Lösung darstellt. So kannst du den Unternehmen genügend Flexibilität anbieten (indem du für weniger Lohn arbeitest) und gleichzeitig gewinnst Erfahrung.

## Statistics-Theory

### P-Hacking

> Was ist <u>p-hacking</u>?

In der Statistik gibt es den **p-Wert** ein: *Man nimmt an die Hypothese sei wahr und berechnet dann die Wahrscheinlichkeit, dass die beobachtete Statistik mindestens so extrem ausfallen würde (für die Gegner von Wischi-Waschi [hier die Wikipedia-Definition](https://de.wikipedia.org/wiki/P-Wert#Mathematische_Formulierung))*. Falls diese Wahrscheinlichkeit unter 5% liegt, dann sei das Resultat "statistisch signifikant" (yay!) und die Nullhypothese kann verworfen werden, was oftmals die Absicht ist. 

<u>Das **Problem** ist nur</u>: *Hypothesen gibt es viele und z.T. auch recht ähnliche*. Wenn man genug Hypothesen aufstellt - **vor allem, <u>nachdem</u> man sich die Daten angeschaut hat** - dann ist es durchaus möglich, dass man ein statistisch signifikantes Resultat erhält, unabhängig davon, ob das Resultat tatsächlich auch stimmt. **Das nennt man p-Hacking**. Es kommt häufig in der Forschung vor, aber es kommt sicher auch in der SBB vor (dennoch hier eine +1 für Hypothesen-basiertes arbeiten!). Wie einfach man in die "falsche Signifikanz Falle" tappen kann, wird hübsch in dieser Gallerie falscher Korrelationen illustriert.

## Appendix for the Future

> Welche Zeitperiode sind am geeignetsten für Zeitreihenanalysen mit Machine Learning?

