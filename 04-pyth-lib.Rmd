# Python Libraries

## List of Useful Python-Libraries 

- `numpy`, for **mathematical operations**.
- `pandas`, for **handling data**, for example:
  - _Drop missing values_.
  - _Feature Engineering_.
- `statsmodels`, for **regression analysis in "econometrics-style"**.
  - Run a linear model (OLS).
  - Run a binary-regression (logit or probit).
  - Run a choice-model.
- `sktime`, for **time-series analysis inkl. machine-learning**.
- `sklearn`, for **machine-learning in "computer-science style"**.
- `matplotlib`, for **data-visualization**.
- `seaborn`, for **data-visualization**.
- `datetime`, for **dealing with dates**.

## Basic `Python`

In this part, I will give you some useful `Python`-Code, which can be helpful in _any_ project.

### Apply a `function` on a Column

This can be useful, when you need to do some transformation to a column of a `df`:

```
# Step 1: Define any function, that you will need to be applied, in order to transform 

def my_function():
  ... # hier kommen die verschiedenen Anweisungen
  ... # hier kommen die verschiedenen Anweisungen
  return some_variable

# Step 2: Apply this newly specified function on your column of your data-frame, that you wish to transform

df['new_column'] = df['column_1'].apply(my_function)
```

### Put a time-stamp on each Python-Cell

The `ipython-autotime` library is particularly useful to get an overview of **how long each Python-Cell** takes to be executed. 

You can even use it for  debugging in combination with the `datetime`-library in order to use `print()`-statements that gives you **the run-time of - for example - each model-fiting iteration (in a loop)**.

#### Installation

To install this wonderful option

```
pip install ipython-autotime
```

#### Activation

It uses a `Python Magic-Command` (= note the `%`) in order to be able to use this library:

```
%load_ext autotime
```

## Pandas

### How to display all the columns?

```pd.set_option('display.max_columns', None)```

### Drop Missing-values?

```
df = df.dropna() # drop the missing values
```

### `loc`- VS. `iloc`-Selection

`loc` &amp; `iloc` are the "accessor operators" in the `pandas`-library. With these,, you can <u>select</u> rows and columns in your dataframe:

- <u>For `loc`</u>: Based on the _name_ of the rows (= row-label // row-index)
and columns.

- <u>For `iloc`</u>: Based on the <u>position</u> // where the specific rows &amp; columns are within the dataset.

## Statsmodels

### Useful Nice-to-Knows

- Use `statsmodels` in combination with the `patsy`-library. For instance, if we have: some variable y, and we want to regress it against some other variables x, a, b, and the interaction of a and b, then we simply write:

```
patsy.dmatrices("y ~ x + a + b + a:b", data)
```

### Define the `y`-variable & `X`-variable<u>s</u>

```
y, X = dmatrices('Lottery ~ Literacy + Wealth + Region', data=df, return_type='dataframe')
```

<u>Notice that `dmatrices`-function above does the following things</u>:

- **It creates Dummies**: Split the <u>categorical-variable</u> _Region_ into a set of indicator-variables (= Dummy-Variables).
- **Added a _constant_** (= `Intercept`-Column) to the exogenous regressors matrix.
- **Returned `pandas`-DataFrames** instead of simple `numpy`-arrays.

### Estimate a Model

```
model = sm.OLS(y, X) # Step 1) Choose the model type --> here, we choose the model to be of the class "OLS" (= instantiate the model: which type of model do you want, for example OLS, GLM, RLM etc...?)
results = model.fit() # Step 2: Fit the model
print(results.summary()) # Step 3: Summarize model
```

#### Extract the $\hat\beta$-Parameters

```
results.params
```

## String-Manipulation

Handling Data that are in `string`-Format is important, since knowledge & human language is mostly based on "writing", and therefore comes in as `string`-formatted data.

### Split a String at a specific point

```
timestamps = time_series_str_column.str.split("+", expand=True) 
  # "+" is the Buchstabe, at which we will split the string.
  # "expand = True" says that we want to keep both parts of the string that was being split.
```

### Cut a String in half

Here, we define 2 `functions`, which will cut the string in half:

```
def splitstring_p1(value):
  string1, string2 = value[:len(value) // 2], value[len(value) // 2:]
  return string1

def splitstring_p2(value):
  string1, string2 = value[:len(value) // 2], value[len(value) // 2:]
  return string2
```

## Working with `Date`-Columns

_Since you will - most probably - often work with time-series data, you MUST learn how to handle this data-type well_! 

- **For <u>any</u> project with time-series data**, the following questions about your `Date-Time`-Column need to be answered:

```
- What is the current 'dtype' of your 'Date-Time'-Column?
- If you work with multiple time-series: are all time-series already in the UTC-format? If NOT, then you better do it, otherwise you have the wrong hours across different time-zones!
```

### Convert a `string`-Date Column INTO a `date-time`-format

We can use `pandas` for this:

```
df['Date-Time'] = pd.to_datetime(df['Date-Time'], format='%Y-%m-%d %H:%M:%S')
df['Date-Time'] # check, if it worked?
```

### <u>Umgekehrt</u>: Convert a `date-time`-Column, into a `str`-format

```
df['Date-Time'] = df['Date-Time'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S%z'))
```

> Why would we do this?

Sometimes, there will be the **problem, that we want to merge different time-series together, but they have different `Date-Time`-Formats to begin with**! 

Hence, _BEFORE_ we apply the `UTC`-universal `Date-Time`-Format, we will need to bring every separate data-frame into the same `Date-Time`-Format. 

Since `Python` is really good for handling transformations on the `str`-format, we can use this to our advantages and:

1) Bring the `Date-Time`-**String** into the way we want it to be on all our _separate_ `Date-Time`-Columns across those different datasets.
2) And then, we will be able to convert them back into the `UTC`-Format! =)

_In order to see, how to manipulate strings, I strongly recommend you to look at the sub-chapter "String-Manipulation"._

### <u>For UTC-Conversions</u>: Keep a copy of the "Local"-Time

Here, we create a new column `Local-Time`, in order to have both, the "local" & the "UTC" time next to each other:

```
df['Local-Time'] = df['Date-Time'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))
```

### Create a _unique ID_ based on a `Date`-Column

```
df = df.reset_index().rename({'index': 'Date-Time-ID'}, axis='columns')
```

## Time-Series Forecasting

Here, I will explore different libraries that will allow you to make forecasts into the future.


### Facebook Prophet

Aus einer didaktischen Perspektive ist die offizielle Dokumentation der **Prophet-Library wunderbar, um die Unterschiede zwischen `R` &amp; `Python` (Objekt-Orientierte Programmierung) zu entdecken**.

- <u>Link zur offiziellen Dokumentation</u>: https://facebook.github.io/prophet/docs/quick_start.html

### SkLearn

> In some functions, you will need to set the option `random_state`. **If you want reproducible results**, how do you need to handle `random_state`?

Just set `random_state` to an integer. Dieser integer kann *beliebig* gewählt werden, das ist `SkLearn` egal!

### SkTime

#### SkTime API

> Überischt zum API in `sktime`?

Grundsätzlich verwendet `sktime` fit-, predict-, und transform-class-methods. Um zu verstehen, was dies genau bedeutet, hier eine globalere Übersicht mit Hilfe einer Visualisierung veranschaulicht:

![API of sktime](./bilder/api_sktime.jpg)

- For *estimator classes* (a.k.a. model classes), `sktime` provides:
 	- a fit-method (= <u>goal</u>: training // estimating the model) and 
 	- a predict-method (= <u>goal</u>: generate new predictions).

- For *transformer classes*, `sktime` provides:
	- various fit-methods,
	- various transform-methods to transform data that comes in series.
	
#### Reduction

> What does it mean: "Reduction is composable"?  

Ein Synonym wäre: "addieren" &#8594; E.g. you can split a difficult task into a bunch of smaller tasks (= reduction) and "add" them together to solve the bigger task at the end (= "reduction is composable").

#### Forecasting-Horizon

> Welche Arten von *Forecasting-Horizon* gibt es?

1) Es gibt einen **<u>relativen</u> forecasting-horizon** (FH), dh "relativ" zur Trainings-Periode ist hier gemeint!
2) Es gibt auch einen **<u>absoluten</u> FH**. Hier werden die *absolute time-points in the future* verwendet für die Prediction. 

- <u>Quelle</u>: [Im "examples"-Repository auf Github zur `Sktime`-Library > 01_forecasting.ipynd](https://github.com/alan-turing-institute/sktime/blob/main/examples/01_forecasting.ipynb)

#### Train-Test Split

> What do you need for parameters // inputs to write a train-test split function for time series data?

You need **3 arguments // inputs** for such a function in time series:

1) the data
2) the window length, e.g. how many periods the training dataset should have.
3) the length of the forecasting horizon, e.g. how many periods into the future our validation-period go.

- <u>Quelle</u>: [Youtube-Video ab 46:25](https://www.youtube.com/watch?v=Wf2naBHRo8Q&t=46m25s)

#### Konkurrenz-Libraries für Time-Series 

> What are other useful libraries when working with Time Series in Python?

Besides `Sktime`, there are 5 other libraries in Python which you can use for different purposes:

- Stop [this Video at 5:35](https://www.youtube.com/watch?v=Wf2naBHRo8Q&t=5m35s) um zu sehen, für welche Zwecke sie sich am besten eignen.

#### Example on Github

> In the [`example` Repository on Github](https://github.com/alan-turing-institute/sktime/blob/main/examples/01_forecasting.ipynb), was ist die Einheit der *y-variable* in `01_forecasting.ipynd`?

- <u>Einheit der y-Variable</u>: in Monate

