# <a id="top" style="color:black; text-decoration: none;">Python Mustercodes</a>
---

In diesem Dokument habe ich eine **Liste von diversen Python-Codes**, welche mir in meiner Arbeit als Data Scientist behilflich sein könnten. I hope you find what you are looking for =)

## Inhaltsverzeichnis

```
<a id="gebe-hier-passenden-id-namen" style="color:black; text-decoration: none;"></a>
```

- [List Comprehensions](#list-compr)
- [String Manipulation](#str-manip)
- [Package Installation](#package-install)
    - [De-Install Packages with `pip`](#deinstall-pip)
- [Help](#help)
- [Path handling](#path)
- [Save Data](#saving)
    - [... as `.CSV`](#save-csv)
- [Read Data](#read-data)
    - [Load `Excel`](#load-excel)
    - [Load `.CSV`](#load-csv)
    - [... via a Website's URL](#load-url)
    - [Load Pickle](#load-pickle)
- [Vanilla Data Exploration](#vanilla-exploration)
    - [Number of Rows](#number-of-rows)
    - [View whole Dataset](#view-all-ddset)
    - [View 5 first observations in the Dataset](#view-first-five)
    - [View 5 last observations in the Dataset](#view-last-five)
    - [Number of Missings per Column within a dataset](#missings-per-column)
    - [Find the `Data Type` of a Variable](#find-data-type)
    - [Drop 1 OR multiple Columns](#drop-columns)
    - [Re-Name 1 OR multiple Columns](#rename-columns)
    - [`.reset_index()` and `.set_index()`](#reset-and-set-idx)
    - [Summary of a Dataset](#summary-ddset)
    - [Show only `unique()` Columns](#unique-values)
    - [Median &amp; Mean for a particular Column](#median-and-mean)
    - [Apply a Fustom-Function `f(x)`, but only on certain rows within a Dataframe via `numpy`](#apply-a-function-based-on-cond)
    - [Transform a Series to a Dataframe](#trans-series-to-frame)
    - [Assign a `unique ID`](#unique-id)
- [Selection &amp; Filtering of Rows &amp; Columns](#sel-and-filt)
    - [Select 1 Column in a Dataframe](#select-1-col-ddset)
        - [Example with "dot-notation", e.g. `df.a_Column`](#dot-not)
    - [Select multiple Columns in a Dataframe](#select-multi-col-ddset)
    - [Select AND Filter for Rows with _1 Condition_ within Dataframe](#filter-1-row-ddset)
    - [Select AND Filter for Rows with _multiple Conditions_ within a Dataframe](#filter-multi-cond-ddset)
    - [Select only Rows with Missing-Values](#select-only-missings)
    - [Selection with "Accessor Operators" `loc` &amp; `iloc`](#sel-loc-iloc)
        - [Example: Select 1st Row and 5th Column with `iloc`](#example-iloc-sel)
        - [Example: Select 1st Row and 5th Column with `loc`](#example-loc-sel)
        - [Example: Select with a _Threshold-Value_ via `loc`](#example-sel-with-threshold-loc)
        - [Example: Select _entire Rows_ via `iloc`](#example-sel-entire-row-with-iloc)
        - [Example: Select _entire Rows_ via `loc`](#example-sel-entire-row-with-loc)
    - [Replace Values within Rows](#replace-values-within-rows)
    - [Slicing is different when using `loc` or `iloc`](#different-slicing-loc-and-iloc)
- [Sorting &amp; Filtering](#sort-and-filt)
    - [Sorting (Pearson) Correlation _from most-important to least-important](#sort-pearson-corr-from-most-imp-to-least-imp)
        - [Example: Select only highest Correlations, then Sort them and `print()` them](#example-of-sort-corr)
- [Dealing with Missings](#dealing-with-missings)
    - [Count total Number of Missings within a Dataframe](#count-total-missings-ddset)
    - [Count Missings per Columns](#count-missings-per-columns)
    - [Count _Non_-Missings, but _ONLY IF_ you don't have the value `0` in it](#count-non-missings-only-if-no-0)
    - [Show only Rows with missings](#show-only-rows-with-missings)
    - [Replace all Missings with 0's](#replace-missings-with-0)
    - [Missing-Imputations](#missing-imputations)
        - [Missing-Interpolation for Time-Series](#interpolation-for-time-series)
- [Duplicate Data](#data-duplicates)
    - [Count _all_ Duplicates in the Dataframe](#count-all-duplicates)
    - [Drop Duplicates](#drop-duplicates)
- [Outliers](#outliers)
    - [Draw a Boxplot](#draw-boxplot)
- [Merging &amp; Splitting](#merging-and-splitting)
    - [Split bigger Dataset by Categories into Subsets](#split-big-ddset-by-categories)
    - [Merging 2 Dataframes by their (row) index](#merging-by-idx)
    - [Merging _multiple_ Datasets _simultaneously](#merge-multiple-ddset)
- [Advanced Data Exploration](#advanced-exploration)
    - [Create a Correlation-Matrix from a Dataframe](#create-correlation-matrix)
- [Time Series Exploration](#time-series-exploration)
    - [Plot Time-Series](#plot-time-series)
    - [Autocorrelation &amp; Partial-Autocorrelation](#auto-and-partial-auto-corr)
    - [Cross-Correlation](#cross-corr)
    - [Feature Engineering](#feature-engineering-time-series)
        - [Creating a Time-Index](#time-stamps-as-index)
        - [Copy Index as a Column](#copy-index-as-columns)
        - [Transform a "Pseudo"-Time-Column into a `Date-Time`-Column and set it as the index](#make-datetime-and-set-index)
        - [Create a `range()` of Dates](#create-range-of-dates)
        - [Rechnen (Addition &amp; Substraktion) mit `Date-Time`](#rechnen-mit-date-time-objects)
            - [Genauer Zeitpunkt "jetzt" (Heute // `Today`) herausfinden](#today)
            - [Zukunft relativ zu "jetzt"](#zukunft-relativ-zu-jetzt)
            - [Subtraktion, um vergangenen Zeitpunkt zu berechnen](#subtraktion-von-zeitpunkt)
            - [Herausfinden, welcher Wochentag (Mo / Di / Mi etc...) ein bestimmtes Datum war](#wochentag-herausfinden)
        - [Create Dummy-Variables for Time-Variables](#dummy-variables-for-time-variables)
            - [Create "normal" Dummies](#create-normal-dummies)
            - ["Flagg"-Dummy by Merging](#flagg-dummy-by-merging)
            - [Dummy for Hours](#dummy-for-hours)
            - [Dummy for Quarters](#dummy-for-quarters)
            - [Dummy for Days](#dummy-for-days)
- [Scaling Variables](#scaling-variables)
    - [Normalization](#normalization-scaling)
    - [Series to Array](#series-to-array)
- [Data Visualization](#data-viz)
    - [Plot and Save Image](#plot-and-save-image)
    - [_Multiple_ Plots with For-Loops](#multi-plots-for-loop)
- [Umwandlungen von `Data Types` in einen anderen `Data Type`](#umwandlungen)
    - [Convert Series to Array](#series-to-array)
- [Useful "tricks" I stumbled upon](#useful-tricks)
    - [`Reshape(-1,1)` to create a list _within_ a list](#reshape-1-1)
- [Math-Tricks for "Hacks"](#math-tricks)
    - [Round Downwards](#round-downwards)
    - [Calculate the Median](#calc-median)
- [Python Magic Commands](#magic-commands)
    - [View all Columns](#view-all-columns)
    - [Working with HTML within Juypter-Notebooks](#html-with-jupyter-notebooks)
    - [Share Variables between Notebooks](#share-variables-between-notebooks)

### <u><a id="list-compr" style="color:black; text-decoration: none;">List Comprehensions</a></u>:

List Comprehensions are a concept that is on of `Python`'s most beloved and unique features. It's basically a loop where you apply a function to each of the elements during the loop. Its Output is a `list`. 

- <u>**Examples**</u>:

### <u>List comprehension where we apply a `function`</u>: x<sup>2</sup>

List comprehensions are really nice if you want to iterate over a column and apply a function on a particular column of your dataset.


```python
squares = [n**2 for n in range(10)] # apply f(x) = x^2 on a list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
squares
```




    [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]




```python
print(type(squares)) # Check output: should be a 'list'
```

    <class 'list'>


### <u>List comprehension with an `if` condition</u>:


```python
planets = ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune'] # this is the list we will loop through

short_planets = [planet for planet in planets if len(planet) < 6]
short_planets
```




    ['Venus', 'Earth', 'Mars']



### <u><a id="str-manip" style="color:black; text-decoration: none;">String Manipulation</a></u>:

### <u>Transform a `list` that contains strings into a WHOLE string, only separated by commas</u>:


```python
# step 0: create a list with many strings in it
planets = ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune']

# Step 1: Transform the whole list into a single "string"
str(planets) # should output: "['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune']"

# Step 2: Now, we replace all characters that are unnecessary - such as ' [ and ] -such that we return a whole string,
        # only separated by commas, with no whitespace in between them:
n = str(planets).replace("'", "").replace('[', '').replace(']', '').replace(' ', '') # replace everthing by empty-strings
print(n) # Final output 
```

    Mercury,Venus,Earth,Mars,Jupiter,Saturn,Uranus,Neptune


## <a id="package-install" style="color:black; text-decoration: none;">Installation of Packages</a>
---

### <u>Installing with package-manager `conda`</u>:

`Conda` is a package manager, which will - before starting to install a package that you want - check which dependencies are needed for the package to be able to be used. 

- <u>Example</u>: If you use `sktime`, then `conda` can detect that there ma be a conflict with the _current version_ of - for example - the `package Tornado`. *Hence, `conda` will not only download `sktime`, but will also bring the package `Tornado` onto a newer version, such tht it will become compatible with `sktime`.
- <u>Note</u>: The installation via `conda` can take a while, since many dependencies will be checked!
- <u>Improtant</u>: Use `the terminal` for the following code.

#### To install a specific package - for example - `sktime` into an existing virtual-environment called `"myenv"`, type the following into the terminal:


```python
conda install --name myenv sktime
```

#### Install the package `sktime` into the current (global) environment (= dh "normally"), type the following into the terminal:


```python
conda install sktime
```

### <u>Installing packages with `pip`</u>:

Der **Nachteil** von `pip install [some package-name here]` VS. `conda install [some package-name here]` liegt darin, dass `pip` <u>keine</u> Checks macht, ob die Versionen von verschiedenen Packages, die man verwendet, überhaupt kompatibel sind.

#### Alternative Installation, by using `pip` within the terminal:


```python
pip install sktime
```

### <u>Install some Packages // Modules, or Sub-Modules</u>:


```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.dates import date2num
import pandas as pd
from scipy import stats
from datetime import datetime
```

### <u><a id="deinstall-pip" style="color:black; text-decoration: none;">Deinstall Packages mit `Pip`</a></u>:


```python
pip uninstall fbprophet prophet cmdstanpy
```

## <a id="help" style="color:black; text-decoration: none;">Need help & documentation</a>
---


```python
help()
```

    
    Welcome to Python 3.8's help utility!
    
    If this is your first time using Python, you should definitely check out
    the tutorial on the Internet at https://docs.python.org/3.8/tutorial/.
    
    Enter the name of any module, keyword, or topic to get help on writing
    Python programs and using Python modules.  To quit this help utility and
    return to the interpreter, just type "quit".
    
    To get a list of available modules, keywords, symbols, or topics, type
    "modules", "keywords", "symbols", or "topics".  Each module also comes
    with a one-line summary of what it does; to list the modules whose name
    or summary contain a given string such as "spam", type "modules spam".
    


# <a id="path" style="color:black; text-decoration: none;">Path Handling</a>
---

*Sehr oft wirst du Modelle, die eventuell stundenlang gelernt haben oder Daten-Files etc... laden & speichern müssen. Hierfür ist es von hoher Bedeutung, dass du gut mit Pfaden umgehen kannst, insbesondere mit der Library `os`!*

Zwei Haupt-Formate, die du verwenden wirst für das Abspeichern &amp; Laden von Files, sind:

- Das `.csv`-Format.
- Das `.pkl`-Format.
- etc... Es gibt noch viele andere Formate, aber die obigen beiden sind die wichtigsten, denen ich bisher begegnet bin.

### <u>Set the Path, where your Computer should Save Data / Models</u>


```python
# Read data // Load data
---import os # this is the library that can handle paths

save_folder = os.path.expanduser(os.path.join("~", # our User's "home"-directory --> for my Mac it is: "/Users/jomaye" 
                                              "Dokumente", # Next, we jump 1 directory called "Dokumente" further below
                                              "Programming")) # Allgemeint: each string after a Komma is a new directory you can set. This can go on infinitively ;)


save_folder # check if it worked? --> yes! Now you see your path =)
```




    '/Users/jomaye/Dokumente/Programming'



# <a id="saving" style="color:black; text-decoration: none;">Save data</a>
---

### <u><a id="save-csv" style="color:black; text-decoration: none;">Save as CSV-File</u></a>:


```python
dynamic_name = "name_of_a_specific_characteristic_of_the_variable_that_you_want_to_save" # this is needed for "dynamisches speichern" via F-String  
YOUR_VAR_NAME.to_csv(f"Name-of-the-CSV-{dynamic_name}.csv") # I use f-strings since it allows me to adapt the CSV-filenames to a specific characteristic, that I used for a model / method etc...
```

# <a id="read-data" style="color:black; text-decoration: none;">Read data // Load data</a>
---

In order to load datasets, you will need the library `pandas`. For *some* cases, additional libraries may be needed.

### <a id="load-excel" style="color:black; text-decoration: none;"><u>Load data from Excel-Sheets</u></a>:


```python
epex_df = pd.read_excel("./Data_V2/Preis_aktuell_Spot_EEX_CH-19-ver-mac.xlsx", # plug in the correct path
                        header=[1], # The dataset Column-names beginnt ab 2. Zeile (--> 1. Zeile ist der Titel des Excel-Files)
                        sheet_name='Prices', # If you have more than 1 Excel-Sheet within the Excel-File, you need to specify
                        # which sheet you want to load
                        engine='openpyxl') # This input will (sometimes) be needed if you load data from an Excel-File via 
                                           # a Windows-Computer, otherwise it can print an error!
epex_df # output the ddset a
```

### <a id="load-csv" style="color:black; text-decoration: none;"><u>Load Dataset via a CSV-File</u></a>:


```python
test = pd.read_csv(
    "C:/Users/u235051/Downloads/ETS_Database_v38/ETS_Database_v38.csv", 
    sep='\t' # Im CSV-File waren die Spalten via "Tab"-Taste separiert, deshalb diese option zwingend anzugeben ist (ansonsten Error!)
) 
```


```python
test
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>country_code</th>
      <th>ETS information</th>
      <th>main activity sector name</th>
      <th>unit</th>
      <th>value</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Belgium</td>
      <td>BE</td>
      <td>2. Verified emissions</td>
      <td>35 Production of pulp</td>
      <td>tonne of CO2 equ.</td>
      <td>102581.0</td>
      <td>2009</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Belgium</td>
      <td>BE</td>
      <td>2. Verified emissions</td>
      <td>35 Production of pulp</td>
      <td>tonne of CO2 equ.</td>
      <td>106671.0</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Belgium</td>
      <td>BE</td>
      <td>2. Verified emissions</td>
      <td>35 Production of pulp</td>
      <td>tonne of CO2 equ.</td>
      <td>126702.0</td>
      <td>2012</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Belgium</td>
      <td>BE</td>
      <td>4. Total surrendered units</td>
      <td>35 Production of pulp</td>
      <td>tonne of CO2 equ.</td>
      <td>98349.0</td>
      <td>2007</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Belgium</td>
      <td>BE</td>
      <td>4. Total surrendered units</td>
      <td>35 Production of pulp</td>
      <td>tonne of CO2 equ.</td>
      <td>96708.0</td>
      <td>2008</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>57389</th>
      <td>Hungary</td>
      <td>HU</td>
      <td>2. Verified emissions</td>
      <td>10 Aviation</td>
      <td>tonne of CO2 equ.</td>
      <td>12599748.0</td>
      <td>Total 3rd trading period (13-20)</td>
    </tr>
    <tr>
      <th>57390</th>
      <td>Hungary</td>
      <td>HU</td>
      <td>1.1.3 Free allocation for modernisation of ele...</td>
      <td>23 Metal ore roasting or sintering</td>
      <td>tonne of CO2 equ.</td>
      <td>0.0</td>
      <td>Total 3rd trading period (13-20)</td>
    </tr>
    <tr>
      <th>57391</th>
      <td>Hungary</td>
      <td>HU</td>
      <td>1.1.3 Free allocation for modernisation of ele...</td>
      <td>30 Production of lime, or calcination of dolom...</td>
      <td>tonne of CO2 equ.</td>
      <td>0.0</td>
      <td>Total 3rd trading period (13-20)</td>
    </tr>
    <tr>
      <th>57392</th>
      <td>Hungary</td>
      <td>HU</td>
      <td>4. Total surrendered units</td>
      <td>43 Production of hydrogen and synthesis gas</td>
      <td>tonne of CO2 equ.</td>
      <td>883972.0</td>
      <td>Total 3rd trading period (13-20)</td>
    </tr>
    <tr>
      <th>57393</th>
      <td>Ireland</td>
      <td>IE</td>
      <td>1.1.3 Free allocation for modernisation of ele...</td>
      <td>10 Aviation</td>
      <td>tonne of CO2 equ.</td>
      <td>0.0</td>
      <td>Total 3rd trading period (13-20)</td>
    </tr>
  </tbody>
</table>
<p>57394 rows × 7 columns</p>
</div>



### <a id="load-url" style="color:black; text-decoration: none;"><u>Load Dataset via a URL from a Website</u></a>:

In order to be able to **access a dataset stored as a csv-file on a website**, we will need to use - *besides* `pandas` - an additional library called `requests`. 

#### <u>Step 1</u>: 

Make the file ready to be downloaded be sending a query to the remote-server // website that hosts the csv-file.


```python
import requests # load the library needed

download_url = "https://raw.githubusercontent.com/fivethirtyeight/data/master/nba-elo/nbaallelo.csv" # absolute URL
target_csv_path = "nba_all_elo.csv" # name of the .csv-file that contains the data

response = requests.get(download_url) # using an API that "gets" (= http-protocol language) the data from the server
response.raise_for_status()    # Check that the request was successful
with open(target_csv_path, "wb") as f:
    f.write(response.content)
print("Download ready.")
```

    Download ready.


#### <u>Step 2</u>:

Load the actual data.


```python
import pandas as pd # load the library needed

nba = pd.read_csv("nba_all_elo.csv") # load the data --> ddset is called 'nba'

type(nba) # check if it worked? --> should output: <class 'pandas.core.frame.DataFrame'>
```




    pandas.core.frame.DataFrame




```python
nba.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gameorder</th>
      <th>game_id</th>
      <th>lg_id</th>
      <th>_iscopy</th>
      <th>year_id</th>
      <th>date_game</th>
      <th>seasongame</th>
      <th>is_playoffs</th>
      <th>team_id</th>
      <th>fran_id</th>
      <th>...</th>
      <th>win_equiv</th>
      <th>opp_id</th>
      <th>opp_fran</th>
      <th>opp_pts</th>
      <th>opp_elo_i</th>
      <th>opp_elo_n</th>
      <th>game_location</th>
      <th>game_result</th>
      <th>forecast</th>
      <th>notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>194611010TRH</td>
      <td>NBA</td>
      <td>0</td>
      <td>1947</td>
      <td>11/1/1946</td>
      <td>1</td>
      <td>0</td>
      <td>TRH</td>
      <td>Huskies</td>
      <td>...</td>
      <td>40.294830</td>
      <td>NYK</td>
      <td>Knicks</td>
      <td>68</td>
      <td>1300.0000</td>
      <td>1306.7233</td>
      <td>H</td>
      <td>L</td>
      <td>0.640065</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>194611010TRH</td>
      <td>NBA</td>
      <td>1</td>
      <td>1947</td>
      <td>11/1/1946</td>
      <td>1</td>
      <td>0</td>
      <td>NYK</td>
      <td>Knicks</td>
      <td>...</td>
      <td>41.705170</td>
      <td>TRH</td>
      <td>Huskies</td>
      <td>66</td>
      <td>1300.0000</td>
      <td>1293.2767</td>
      <td>A</td>
      <td>W</td>
      <td>0.359935</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>194611020CHS</td>
      <td>NBA</td>
      <td>0</td>
      <td>1947</td>
      <td>11/2/1946</td>
      <td>1</td>
      <td>0</td>
      <td>CHS</td>
      <td>Stags</td>
      <td>...</td>
      <td>42.012257</td>
      <td>NYK</td>
      <td>Knicks</td>
      <td>47</td>
      <td>1306.7233</td>
      <td>1297.0712</td>
      <td>H</td>
      <td>W</td>
      <td>0.631101</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>194611020CHS</td>
      <td>NBA</td>
      <td>1</td>
      <td>1947</td>
      <td>11/2/1946</td>
      <td>2</td>
      <td>0</td>
      <td>NYK</td>
      <td>Knicks</td>
      <td>...</td>
      <td>40.692783</td>
      <td>CHS</td>
      <td>Stags</td>
      <td>63</td>
      <td>1300.0000</td>
      <td>1309.6521</td>
      <td>A</td>
      <td>L</td>
      <td>0.368899</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>194611020DTF</td>
      <td>NBA</td>
      <td>0</td>
      <td>1947</td>
      <td>11/2/1946</td>
      <td>1</td>
      <td>0</td>
      <td>DTF</td>
      <td>Falcons</td>
      <td>...</td>
      <td>38.864048</td>
      <td>WSC</td>
      <td>Capitols</td>
      <td>50</td>
      <td>1300.0000</td>
      <td>1320.3811</td>
      <td>H</td>
      <td>L</td>
      <td>0.640065</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 23 columns</p>
</div>



### <a id="load-pickle" style="color:black; text-decoration: none;"><u>Load Pickle-Files</u></a>:



```python
filename = "Put_Your_Pickle-File_Name_Here" # Alternativ: os.path.join(data_folder_variable_where_pkl_is_saved, filename)
test_loaded_pkl =  pickle.load(open(filename, 'rb'))
```

# <a id="vanilla-exploration" style="color:black; text-decoration: none;">Vanilla Exploration of your Dataset</a>
---

<u>After</u> having told Python to read in your dataset, the **first thing** you will want to do is to <mark>get (very) familiar with your dataset</mark>. This is **key**, otherwise you will <u>not</u> be able to perform a good data analysis!

### <a id="number-of-rows" style="color:black; text-decoration: none;"><u>Find out the number of `observations` // rows in your dataset</u></a>:


```python
len(nba) # to get the number of observations // rows 

    # note: 'nba' is the name of the ddset
```

### <u>Find out the number of `rows` &amp; `columns` within your dataset</u>:


```python
nba.shape # to get number of rows AND columns

    # note: 'nba' is the name of the ddset
```

### <a id="view-all-ddset" style="color:black; text-decoration: none;"><u>`View` the <mark>whole</mark> Dataset</u></a>:


```python
nba # just type in the name of the variable in which your dataframe is stored in --> 'nba' is the name of your dataframe here
```

### <a id="view-first-five" style="color:black; text-decoration: none;"><u>`View` the first 5-rows of your dataset</u></a>:


```python
nba.head() # head() is often used to check whether your dataset really contains data you care about 

    # here we check: does the ddset really contains data about the NBA?
```

**<u>Achtung</u>**: If you have alot of columns, Python will not display them all when you use `head()`. <u>However</u> you can change the settings via:


```python
pd.set_option("display.max.columns", None) # this will tell Python: "show me ALL the columns!"

nba.head() # execute 'head()' again to check if the setting changed correclty? --> yes!
```

#### <a id="view-last-five" style="color:black; text-decoration: none;">You can also `View` the 5 last rows of your dataset by using `tails()`</a>:


```python
nba.tail() # View last 5 rows

# Viewing very specific rows is also possible: 
nba.tail(3) # Here, we view the last 3 rows
```

### <a id="missings-per-column" style="color:black; text-decoration: none;"><u>Find out the `Data Type` of each column within your dataset &amp; number of `non-missing values`</u></a>:

With the following simple code, we can find out, whether the columns are from the type of an `integer`, a `string`, a `boolean` etc... 


```python
nba.info() # this will output all the types of each column in your dataset & how many NON-missings you have per column

    # note: Pay attention to any columns that are from the type 'object'! --> lese bemerkung unten...
```

[To get a beautiful overview over all types that exist in Python, I recommend you to visit the website of W3Schools.com](https://www.w3schools.com/python/python_datatypes.asp)

**<u>Bemerkung hier</u>**: It can be, that columns are from the type `object`. In practice, it often means that *all* of the values in an `object`-column are <u>strings</u>. If you encounter any `object`-columns, it is strongly recommended that you convert them into a more apropriate `data-type`, otherwise some of the functions won't work on these `object`-columns...

#### <a id="find-data-type" style="color:black; text-decoration: none;">Find the `type` of *any* object in Python</a>:


```python
x = 5
print(type(x))
```

### <a id="drop-columns" style="color:black; text-decoration: none;"><u>Drop specific Columns</u></a>:

Note that `axis = 1` denotes the columns that will be droped, while `axis = 0` (default), will denote the rows that should be dropped.


```python
df_energy = df_energy.drop(['tInfoDaySin', 'tInfoDayCos', 'tInfoYearSin', 'tInfoYearCos'], axis=1)
```

### <a id="rename-columns" style="color:black; text-decoration: none;"><u>Re-Name the Columns of a DataFrame</u></a>:


```python
df2 = df.rename({'oldName1': 'newName1', 'oldName2': 'newName2'}, axis='columns') # not all columns have to be renamed, only those with a new name
```

### <a id="reset-and-set-idx" style="color:black; text-decoration: none;"><u>Set &amp; Reset the Index / Row-Label</u></a>:


```python
df_energy = df_energy.set_index('Date-Time') # to set the index
```


```python
df_energy = df_energy.reset_index() # to reset the index --> df.reset_index(drop= True) will drop the index, which would 
                                    # otherwise become a new column instead of just dropping it!
```

### <a id="summary-ddset" style="color:black; text-decoration: none;"><u>Make a `Summary` out of all the variables in your dataset</u></a>:

Note that, in order to be able to do some **summary-statistics**, you will need the additional library `numpy`.


```python
import numpy as np # load numpy for summary-statistics

nba.describe().round(2) # results will be rounded onto 2 digits
```

**<u>Achtung</u>**: If you have columns from the type `object`, you will need a slightly different version of the `describe()` function to display some summary-statistics from such columns.


```python
nba.describe(include=object) # if you have some weird columns being of the type 'object'
```

### <a id="unique-values" style="color:black; text-decoration: none;"><u>Find the unique values from a column</u></a>:

This can be useful, when you want to filter all unique `categories` within a column. You can then put all those categories wihtin a new variable in order to loop through them to apply some function to those. 


```python
gapminder['continent'].unique()
```

### <a id="median-and-mean" style="color:black; text-decoration: none;"><u>Calculating the `mean` or `median` from a particular column</u></a>:

For the <u>**mean**</u>, you will need the following code.


```python
mean_points = reviews.points.mean() # calculate the mean of the column 'points' within the ddset 'reviews'
```

<u>For the **median**</u>:


```python
median_points = reviews.points.median() # calculate the median of the column 'points' within the ddset 'reviews'
```

### <a id="apply-a-function-based-on-cond" style="color:black; text-decoration: none;"><u>Transform a column based on conditions with `numpy`</u></a>:


```python
import numpy as np # In order to be able to perform a transformation, we will need the `numpy`-package

# set-up:
x = np.arange(10) # this is our column 'x'
condlist = [x<3, x>5] # set of conditions, that need to be fullfilled --> which are the value-ranges, on which you will apply 
# a custom-function [which will be defined next]? --> all numbers below 3 AND all numbers above 5
choicelist = [x, x**2] # the custom-function you will apply here: x^2

# output:
np.select(condlist, choicelist, default=np.nan) # apply x^2 on: x < 3 AND x > 5
```




    array([ 0.,  1.,  2., nan, nan, nan, 36., 49., 64., 81.])



### <a id="trans-series-to-frame" style="color:black; text-decoration: none;"><u>Transform a Pandas Series into a DataFrame</u></a>:


```python
import pandas as pd

s = pd.Series(["a", "b", "c"],
              name="vals")
s.to_frame()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>vals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>a</td>
    </tr>
    <tr>
      <th>1</th>
      <td>b</td>
    </tr>
    <tr>
      <th>2</th>
      <td>c</td>
    </tr>
  </tbody>
</table>
</div>



### <a id="unique-id" style="color:black; text-decoration: none;"><u>Assign a unique ID</u></a>:


```python
df['id'] = df.groupby(['LastName','FirstName']).ngroup() # here, we use the column 'LastName' & 'FirstName' together, to create 
# a unique ID.

# Quelle: https://stackoverflow.com/questions/45685254/q-pandas-how-to-efficiently-assign-unique-id-to-individuals-with-multiple-ent
```


```python
df['id'] = df.groupby(['date']).ngroup() # if Course, you could also simply use 1 column fo the assignment of an unique ID. 
```

# <a id="sel-and-filt" style="color:black; text-decoration: none;">Selection &amp; Filtering Rows and Columns</a>
---

When you will work wit dataframes in `pandas`, one of the most important things you will need to master is how to select some columns, as well as print out `subsets` // particular columns from the dataframe.

To get started and become acquainted with common techniques, I recommend you to watch this [beginner tutorial for filtering &amp; selecting columns](https://www.youtube.com/watch?v=htyWDxKVttE).

**Load the next cell to be able to run the examples that follow**:


```python
import pandas as pd # load package needed

file_path = './data/filter-and-selection/sample_orders.csv' # type in the path-location of your data
dd = pd.read_csv(file_path) # load the data

dd.head() # check if it worked? --> yes! --> should print the first 5 rows of your ddset
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>order_id</th>
      <th>order_date</th>
      <th>customer_id</th>
      <th>items_ordered</th>
      <th>order_total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1000</td>
      <td>12/1/17</td>
      <td>1</td>
      <td>3</td>
      <td>65.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1001</td>
      <td>12/1/17</td>
      <td>3</td>
      <td>2</td>
      <td>25.75</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1002</td>
      <td>12/1/17</td>
      <td>6</td>
      <td>1</td>
      <td>20.75</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1003</td>
      <td>12/1/17</td>
      <td>8</td>
      <td>6</td>
      <td>86.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1004</td>
      <td>12/1/17</td>
      <td>8</td>
      <td>1</td>
      <td>30.00</td>
    </tr>
  </tbody>
</table>
</div>




```python
dd.info() # check also all data-types
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 16 entries, 0 to 15
    Data columns (total 5 columns):
     #   Column         Non-Null Count  Dtype  
    ---  ------         --------------  -----  
     0   order_id       16 non-null     int64  
     1   order_date     16 non-null     object 
     2   customer_id    16 non-null     int64  
     3   items_ordered  16 non-null     int64  
     4   order_total    16 non-null     float64
    dtypes: float64(1), int64(3), object(1)
    memory usage: 768.0+ bytes


### <a id="select-1-col-ddset" style="color:black; text-decoration: none;"><u>Select only 1 single column of your Dataframe</u></a>:

In this example, I will select the column `order_id` from my ddset `dd`:


```python
print(
    dd['order_id'] # take the dataframe 'dd' and print me only the column called 'order_id' --> this is a subset
)
```

    0     1000
    1     1001
    2     1002
    3     1003
    4     1004
    5     1005
    6     1006
    7     1007
    8     1008
    9     1009
    10    1010
    11    1011
    12    1012
    13    1013
    14    1014
    15    1015
    Name: order_id, dtype: int64


#### <a id="dot-not" style="color:black; text-decoration: none;"><u>Möglichkeit 2</u>: Selection of columns with `dot-notation`</a>

*Nachteil* dieser Methode ist, dass sie <u>nicht</u> funktioniert, wenn es in den Columns `Leerschläge` gibt!


```python
test = dd.order_id # ACHTUNG: funktioniert nicht, wenn es Leerschläge gibt!!
print(test)
```

    0     1000
    1     1001
    2     1002
    3     1003
    4     1004
    5     1005
    6     1006
    7     1007
    8     1008
    9     1009
    10    1010
    11    1011
    12    1012
    13    1013
    14    1014
    15    1015
    Name: order_id, dtype: int64


### <a id="select-multi-col-ddset" style="color:black; text-decoration: none;"><u>Select *multiple* Columns from your Dataframe</u></a>:

Similarly to `R`, we need to pass a "set" into the selector of columns.


```python
print(
    dd[['order_id', 'order_total']] # take the dataframe 'dd' and print me only the columns called 'order_id' && 'order_total'
)
```

        order_id  order_total
    0       1000        65.00
    1       1001        25.75
    2       1002        20.75
    3       1003        86.00
    4       1004        30.00
    5       1005        20.00
    6       1006        21.00
    7       1007        26.25
    8       1008        20.75
    9       1009        15.75
    10      1010        45.00
    11      1011        30.00
    12      1012        15.00
    13      1013        85.00
    14      1014        25.00
    15      1015        15.00


### <a id="filter-1-row-ddset" style="color:black; text-decoration: none;"><u>Select AND filter for a particular row, with *one condition*</u></a>: 

Let's say, we want to select the row // observation with the `"order_id" == 1004`.


```python
print(
    dd[
        dd['order_id'] == 1004 # note that this INNER bracket will run a function that searches through and would only print 
                               # a Boolean-List of "True" or "False" for all rows  --> example-video: ab 2:36-3:34 --> https://www.youtube.com/watch?v=htyWDxKVttE
    ]                          # This outer selector will tell Python: "Select" only the row where the column 'order_id' == 1004
)

# short-version:

print(dd[dd['order_id'] == 1004])
```

       order_id order_date  customer_id  items_ordered  order_total
    4      1004    12/1/17            8              1         30.0
       order_id order_date  customer_id  items_ordered  order_total
    4      1004    12/1/17            8              1         30.0


- Quelle: [Basics of selecting and filtering rows and columns in Python, ab 2:36-3:34](https://www.youtube.com/watch?v=htyWDxKVttE&t=2m36s)

### <a id="filter-multi-cond-ddset" style="color:black; text-decoration: none;"><u>Select AND filter for rows with *more than one condition*</u></a>:

Now, we want to select the row<u>s</u> // observations, where `"order_total" >= 50.00` <u>AND</u> `"order_date" == 12/1/17` are fulfilled // filtered. Basically, it is the same as with the selection &amp; filtering with *one condition*, you **just need to wrap up the INNER brackets with an additional `(...)` brackets (for <u>each</u> condition!)**. 


```python
print(
    dd[
        (dd['order_total'] >= 50) & (dd['order_date'] == '12/1/17') # in contrast to **one condition**, we just wrap up a (...) for each condition
    ]                          # This outer selector will tell Python: "Select" only the row where the column 'order_id' == 1004
)

```

       order_id order_date  customer_id  items_ordered  order_total
    0      1000    12/1/17            1              3         65.0
    3      1003    12/1/17            8              6         86.0


<u>**Alternative mit "OR"**</u>: Statt *AND* kann man auch - zum Beispiel - die *OR*-Condition verwenden


```python
print(dd[(dd['order_total'] >= 50) | (dd['order_date'] == '12/1/17')])
```

        order_id order_date  customer_id  items_ordered  order_total
    0       1000    12/1/17            1              3        65.00
    1       1001    12/1/17            3              2        25.75
    2       1002    12/1/17            6              1        20.75
    3       1003    12/1/17            8              6        86.00
    4       1004    12/1/17            8              1        30.00
    13      1013    12/3/17            7              5        85.00


### <a id="select-only-missings" style="color:black; text-decoration: none;">Select only rows with Missing-Values</a>


```python
null_data = df[df.isnull().any(axis = 1)] # this will only select the rows that contain at least one missing-value
null_data # check, if it worked?
```

#  <a id="sel-loc-iloc" style="color:black; text-decoration: none;">Selection with "accessor operators" `iloc` &amp; `loc`</a>
---

Das Package `Pandas` erlaubt die Verwendung von sogenannten `accessor operators`, um Dataframes einfacher zu filtrieren. Dabei unterscheidet man zwischen: 

- `iloc` (= based on the <u>**Postition**</u> (= Nummer) des Index innerhalb der Spalten & Reihen im ddset),
    - <u>Beachte</u>: Reihenfolge der inputs within `iloc` == 1) 'rows', then 2) 'columns'
- `loc` (= based on the <u>**Index-Namen**</u> of the Spalten & Zeilen im ddset).
    - <u>Beachte</u>: Reihenfolge der inputs within `loc` == 1) 'rows', then 2) 'columns'

### <a id="example-iloc-sel" style="color:black; text-decoration: none;"><u>Selection of the `entry` situated in the 1st row and 5th column only with `iloc`</u></a>

To be able to use `iloc`, the **key** is to know *the position* of the row- & column-labels (= index of columns & rows).

- <u>Note</u>: `iloc` &amp; `loc` are often used to print a particular *entry* WITHIN a dataframe. However, `loc` and `iloc` also are able to print <u>entire</u> rows // observations and not just one specific value within a row, as we will see.


```python
test = dd.iloc[0,4] # Reihenfolge der inputs == 1) 'rows' (--> "0" == 1st row), then 2) 'columns' (--> "4" == 5th column)
print(test) # check if it worked? --> should print the value '65'
```

    65.0


### <a id="example-loc-sel" style="color:black; text-decoration: none;"><u>Selection of the `entry` situated in the 1st row and 5th column only with `loc`</u></a>

The **key** to use `loc` is that you know *the names* of the columns & rows. 

- <u>Wichtige Bemerkung</u>: Der **Default-<u>Name</u>** (= Standard-Name) der `rows` innerhalb eines Dataframes ist einfach die Zahlenabfolge von `0,1,2...,10,11,12,...`. Lustigerweise ist der **row-label // <u>index</u> von rows** auch standardmässig die Zahlenabfolge `0,1,2,...,10,11,12,...`. Deshalb kommt es sehr oft vor, dass `loc` und `iloc` ***denselben*** ersten Input haben! xD


```python
test = dd.loc[0,'order_total'] # Reihenfolge der inputs == 1) 'rows' (--> "0" == NAME der 1st row), then 2) 'columns' (--> "order_total" == NAME der gewünschten 5th column)
print(test) # check if it worked? --> should print the value '65'
```

    65.0


### <a id="example-sel-with-threshold-loc" style="color:black; text-decoration: none;"><u>Apply a Threshold-Filter using `loc` on each row</u></a>:

This can be useful, when you want to replace some values in certain columns &#8594; see 'Outliers'-chapter


```python
# Only display all rows, where the 'pressure'-column is > than the threshold of 1051 bar

df_weather.loc[df_weather.pressure > 1051, 'pressure']
```

### <a id="example-sel-entire-row-with-iloc" style="color:black; text-decoration: none;"><u>Selection of an ENTIRE row // record // observation from a ddset with `iloc`</u></a>:


```python
#####
## Möglichkeit 1: selection of only 1 row

test_row1 = dd.iloc[0,] # Important: 1) rows, then 2) columns --> we want the entire 1st row, which includes ALL columns
                        # Note: das Komma NACH dem "0" zeigt an, dass wir ALLE columns selektieren wollen!

#####
## Möglichkeit 2: selection of > 1 row --> notice the additional wrap with [...] WITHIN iloc[]!

test_multiRow = dd.iloc[[0,1,2,3,5,8],] # '[0,1,2,3,5,8]' will select the '1st, 2nd, 3rd, 4th, 6th and 9th' row
                                            # while also selecting ALL columns simultaneously

#####   
## check if it worked? --> yes!
print(test_row1) # should print only 1 row BUT with ALL the column --> weird output, because the columns sind abgebildet als rows xD
print(test_multiRow)
```

    order_id            1000
    order_date       12/1/17
    customer_id            1
    items_ordered          3
    order_total           65
    Name: 0, dtype: object
       order_id order_date  customer_id  items_ordered  order_total
    0      1000    12/1/17            1              3        65.00
    1      1001    12/1/17            3              2        25.75
    2      1002    12/1/17            6              1        20.75
    3      1003    12/1/17            8              6        86.00
    5      1005    12/2/17            1              1        20.00
    8      1008    12/2/17            9              1        20.75


### <a id="example-sel-entire-row-with-loc" style="color:black; text-decoration: none;"><u>Selection of an ENTIRE row // record // observation from a ddset with `loc`</u></a>:

<u>**Tipp**</u>: Von der Eleganz &amp; Effizienz her, empfehle ich dir undbedingt **Möglichkeit 3** im unteren Code-Beispiel!!


```python
#####
## Möglichkeit 1: selection of only 1 row

test_row1 = dd.loc[0,] # das Komma NACH dem "0" zeigt an, dass wir ALLE columns selektieren wollen!

#####
## Möglichkeit 2: selection of > 1 row --> notice the additional wrap [...] WITHIN loc[]!

test_multiRow = dd.loc[[0,1,2,3,5,8],] # Weil - per default - die 'row-labels' (= name des Indexes 
                                            # der Zeilen) dieselben sind, wie die Position, ist der Code 
                                            # für 'loc' derselbe, wie für 'iloc' hier...
        
#####
## Möglichkeit 3: Beste & schönste Solution (meiner Meinung nach!)
rows = list(range(0,16)) # will create a list that goes from 0 to 99 --> this will be for the row-labels
columns = ['order_id', 'order_date', 'order_total'] # this will be for the column-labels
                                                    # Pro-Tipp: columns = list(data.columns)
df = dd.loc[rows, columns]
        
        
#####   
## check if it worked? --> yes!
print(test_row1) # should print only 1 row BUT with ALL the column --> weird output, because the columns sind abgebildet als rows xD
print(test_multiRow)
print(df)
```

    order_id            1000
    order_date       12/1/17
    customer_id            1
    items_ordered          3
    order_total           65
    Name: 0, dtype: object
       order_id order_date  customer_id  items_ordered  order_total
    0      1000    12/1/17            1              3        65.00
    1      1001    12/1/17            3              2        25.75
    2      1002    12/1/17            6              1        20.75
    3      1003    12/1/17            8              6        86.00
    5      1005    12/2/17            1              1        20.00
    8      1008    12/2/17            9              1        20.75
        order_id order_date  order_total
    0       1000    12/1/17        65.00
    1       1001    12/1/17        25.75
    2       1002    12/1/17        20.75
    3       1003    12/1/17        86.00
    4       1004    12/1/17        30.00
    5       1005    12/2/17        20.00
    6       1006    12/2/17        21.00
    7       1007    12/2/17        26.25
    8       1008    12/2/17        20.75
    9       1009    12/3/17        15.75
    10      1010    12/3/17        45.00
    11      1011    12/3/17        30.00
    12      1012    12/3/17        15.00
    13      1013    12/3/17        85.00
    14      1014    12/3/17        25.00
    15      1015    12/3/17        15.00


### <a id="replace-values-within-rows" style="color:black; text-decoration: none;"><u>Replace values within a Column with some new Values</u></a>:


```python
df.loc[df.ungewichtet > 1, 'ungewichtet'] = 1 # hier werde ich alle Werte der Spalte "ungewichtet" > 1 mit dem Wert "1" ersetzen!
```

### <a id="different-slicing-loc-and-iloc" style="color:black; text-decoration: none;"><u>Different Slicing when using `iloc` VS. `loc`</u></a>: 

When using `iloc`, **the range 0:5** will select entries `0,...,4` that is: it indexes EXCLUSIVELY. On the other hand, `loc`, meanwhile, indexes *INCLUSIVELY*. So **the <u>same</u> range 0:5** will select entries `0,...,5`!!!

*Hence, if you want the SAME output with `loc` and `iloc`, you simply need to slightly change the `range()`-function.*

<u>Example</u>:


```python
## Möglichkeit 1: with 'iloc'
iloc_test = dd.iloc[0:5,0] # row-position == 0:5 --> first 5 rows; EXCLUDES '5' from the range "0,1,2,3,4,5" 
                                          # --> hence range(0:5) results in --> "0,1,2,3,4"
                                          # column-position == 0 --> 1st row --> remember: indexing in 
                                          # Python starts at '0'!

    # IMPORTANT: 'iloc' uses the 'Python stdlib' indexing scheme, where the first element of the range is 
    # included and the last one excluded. So 0:5 will select entries 0,...,4 (= these are the first *5* 
    # entries!!).

## Möglichkeit 2: to get the SAME output with 'loc', we need a slightly DIFFERENT range!
loc_test = dd.loc[0:4,'order_id'] # row-position == 0:4 --> first 5 rows; INCLUDES '4' 
                                  # --> hence range(0:4) results in --> "0,1,2,3,4"

## check if the output are the same, even though "range()" has slightly different inputs? --> yes!
print(iloc_test)
print(loc_test)
```

    0    1000
    1    1001
    2    1002
    3    1003
    4    1004
    Name: order_id, dtype: int64
    0    1000
    1    1001
    2    1002
    3    1003
    4    1004
    Name: order_id, dtype: int64


# <a id="sort-and-filt" style="color:black; text-decoration: none;">Sorting &amp; Filtering</a>
---

### <u><a id="sort-pearson-corr-from-most-imp-to-least-imp" style="color:black; text-decoration: none;">How to sort a Data-Frame Column (hier: Pearson-Correlation Matrix) from '_most important_' to '_least important_'?</a></u>:


```python
### Find the correlations' ranking for the day-ahead electricity price and the rest of the features:

# Step 1: Create a Pearson-Korrelation Matrix out of your dataframe:
correlations = df.corr(method='pearson') # the variable 'correlations' is a dataframe!

# Step 2: use 'sort_values' to sort the column from "most important" (highest value) to "least important":
print(correlations['pricesCH'].sort_values(ascending=False).to_string())
```

### <a id="example-of-sort-corr" style="color:black; text-decoration: none;"><u>Filter the WHOLE Dataframe after a condition (hier: Correlations > 0.75), select ONLY all observations that fullfill the condition (= 'stack them') and sort them  left in your Data Frame</u></a>:

- Dieser Code beruht auf den Abschnitt: [_How to sort a Data-Frame Column (hier: Pearson-Correlation Matrix) from 'most imporant' to 'least important'_](#pearson).


```python
highly_correlated = correlations[correlations > 0.75]
print(highly_correlated[highly_correlated < 1.0].stack().to_string())
```

# <a id="dealing-with-missings" style="color:black; text-decoration: none;">Data-Cleaning: Missings</a>
---

### <a id="count-total-missings-ddset" style="color:black; text-decoration: none;"><u>Number of Missing Values across the WHOLE Data-Frame</u></a>:


```python
print('There are {} missing values or NaNs in df_energy.'
      .format(df_energy.isnull().values.sum()))
```

### <a id="count-missings-per-columns" style="color:black; text-decoration: none;"><u>Count the TOTAL number of Missings in each column</u></a>:


```python
df_energy.isnull().sum(axis=0) # outputs the number of NaNs for each column
```

### <a id="count-non-missings-only-if-no-0" style="color:black; text-decoration: none;"><u>Count the non-missings (and non-zero values) in each column</u></a>:

**<u>Achtung</u>**: the "Code-Trick" below only works, if your columns don't contain values, that are '0'! This is because the number '0' - as a `boolean` - will be printed out to `False`, and hence, we will get the "wrong" number of missing values. 

*Verwende den unteren Code also nur, wenn du zuerst abgecheckt hast, dass du keine `0` in den einzlenen Spalten hast!*


```python
# Display the number of non-missing values in each column

print('Non-zero values in each column:\n', df_energy.astype(bool).sum(axis=0), sep='\n')
```

Since the above cell only gives out the "correct" number of non-missing values, if you have no `0` in your columns, here is code to count how many `0` you have in each column:


```python
(df_energy == 0).astype(int).sum(axis=0) # count the numbers of '0s' in each column [axis = 0, for columns...]
```

### <a id="show-only-rows-with-missings" style="color:black; text-decoration: none;"><u>Display each row with a Missing-Value</u></a>:


```python
# Display the rows with null // missing values:

df_energy[df_energy.isnull().any(axis=1)].tail()
```

### <a id="replace-missings-with-0" style="color:black; text-decoration: none;"><u>Replace all Missings in a Column with 0s</u></a>:



```python
df['ColumnWithMissings'] = df_tot['ColumnWithMissings'].fillna(0) # replaces all missing-values within the column with 0s. 
```

### <u>Replace all Values in a Column with `Missings`</u>:
    
*Assume that we have a time-series that has a row-index with time-stamps*!


```python
### Step 1: define the range in which you want to replace values
start = '2020-01-01' # 01. Januar 2020 --> ab hier wollen wir die values der Time-Series mit Missings ersetzen
stop = '2020-08-01' # 01. August 2020 --> bis zu diesem Datum sollen die Missings eingefügt werden

### Step 2: replace the values with missings via the ".loc[row-indexer, column-indexer]"
df.loc[start:stop, 'y_hat'] = None # This will replace all the values within the 'y_hat'-column - in the range from 
                                   # 01.01.2020-01.08.2020 with Missing-values (instead of "normal"-values)
```

## <a id="missing-imputations" style="color:black; text-decoration: none;">Missing Imputation</a>
---

This list will grow with time, the more I stumble onto various codes:

### <u>Missing Interpolation</u>:

#### <a id="interpolation-for-time-series" style="color:black; text-decoration: none;"><u>For a Time Series</u></a>:


```python
# Fill null values using interpolation:

df_energy.interpolate(method='linear', limit_direction='forward', inplace=True, axis=0) # since we have 
```

# <a id="data-duplicates" style="color:black; text-decoration: none;">Data-Cleaning: Duplicates</a>
---


### <a id="count-all-duplicates" style="color:black; text-decoration: none;"><u>Number of Duplicates across the WHOLE Data-Frame</u></a>:


```python
temp_energy = df_energy.duplicated(keep='first').sum()

print('There are {} duplicate rows in df_energy based on all columns.'
      .format(temp_energy))
```

### <a id="drop-duplicates" style="color:black; text-decoration: none;"><u>Drop Duplicate values</u></a>:


```python
# Variante 1: mit reset_index & neuer set_index
df_weather_2 = df_weather.reset_index().drop_duplicates(subset=['time', 'city_name'], # Drop the duplicate, if all the rows are the same // have the same values (Achtung: we only look at the duplicates in the 'time' & 'city_name'-column from this analysis!).
                                                        keep='last').set_index('time') # if you have duplicate, keep only the last of the duplicated-rows.
```


```python
# Variante 2: man dropt den "alten" Index (VOR merge) und neuem set_index 
df_unique_dates = df_ferien.drop_duplicates(subset='datum', # only consider the column "datum" [= column that has duplicates] when dropping the duplicates 
                                    keep='first').reset_index(drop=True) # reset the index, otherwise you get weird indizes (mit 10'000 für manche)
                                                                         # 'drop = True' means that we do not keep the "old" index as a separate 'column'
df_unique_dates
```

# <a id="outliers" style="color:black; text-decoration: none;">Data-Cleaning: Outliers</a>
---

### <a id="draw-boxplot" style="color:black; text-decoration: none;"><u>Draw a Boxplot for a specific column</u></a>:


```python
import seaborn as sns

sns.boxplot(x=df_weather['pressure'])
plt.show()
```

<ul>
    <li><strong><u>Key-Question</u></strong>: <h1><em><center>Are there Outliers? &#8594; Yes / No</center></em></h1></li><br>
    <ul>
        <li><strong><u>"Trick" to answer the question</u></strong>: If you deal with temperature for example, google for "highest Temperature on earth" and look it up in Wikipedia to dertermine whether your value is an outlier.</li>
    </ul>
</ul>

- <u>Example</u>: ![Outlier Examples](bilder/outlier-example.jpg) Even a pressure of approximately 100,000 HPa or 10 MPa, which is clearly visible in the above figure, corresponds to a quantity greater than the atmospheric pressure of Venus. In order to be sure, we will set as `NaN`s every value in the `pressure`-column which is **higher than 1051 hPa**, which is just above the highest air pressure ever recorded in the Iberian peninsula. While outliers on the low side are not visible in the boxplot above, it is a good idea to also replace the values which are **lower than 931 hPa**, i.e. the lowest air pressure ever recorded in the Iberian peninsula.

**<u>Step 2</u>**:<br>
<h3><em>If the answer to the above question is 'yes', then set the value above values to <code>NaN</code>s, which are above a certain "unprobable" threshold.</em></h3>


```python
# Replace outliers in the `Set_Name_of_Column_with_the_Outlier_hier`-column with `NaN`s

df_weather.loc[df_weather.pressure > 1051, 'pressure'] = np.nan
df_weather.loc[df_weather.pressure < 931, 'pressure'] = np.nan
```

# <a id="merging-and-splitting" style="color:black; text-decoration: none;">Merging &amp; Splitting</a>
---

- <u>**Important note before you start**</u>: If you want to merge 2 dataframes, where _one is smaller than the other_, then you <mark>CANNOT make the bigger dataset become _smaller_ with `merge()`</mark>. I did spend alot of time, but without success. However, there is **another solution**: you just need to drop the duplicates of the bigger dataframe ([see chapter `Duplicates`](#duplicates-chapter) in order to be able to make the dataset smaller! =)

### <a id="split-big-ddset-by-categories" style="color:black; text-decoration: none;"><u>Splitting a Dataset into smaller parts, by using categories to split it up</u></a>:


```python
# Split the df_weather into 5 dataframes (one for each of the 5 cities):

df_1, df_2, df_3, df_4, df_5 = [x for _, x in df_weather.groupby('city_name')]
dfs = [df_1, df_2, df_3, df_4, df_5]
```


```python
str(planets).replace("'", "")
```




    '[Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune]'



### <a id="merging-by-idx" style="color:black; text-decoration: none;"><u>Merge 2 separate Data Sets together via their row-index (= row-label)</u></a>:

Damit du dieses Merging erfolgreich durchführen kannst, müssen beide Datensätze denselben Zeilen-Index besitzen. 

- **<u>Ziel</u>**: *Dadurch kannst du zum Beispiel **neue Spalten von einem <u>anderen</u> Datensatz hinzufügen** (passiert sehr oft in der Praxis)*.


```python
# Let's merge all the y- & X-Variables from the training-set together:
test = pd.merge(
    y_train, # der Trainingsdatensatz für die y-Variable
    x_train, # der Trainingsdatensatz für alle X-Variablen
    how="outer",
    left_on=y_train.index, # merging via index // row-label des DataFrames der y-Variable
    right_on=x_train.index, # merging via index // row-label des DataFrames der x-Variablen
).set_index('key_0') # optional: da wir hier eine Zeitreihe haben, dessen Row-Index den Column-Name 'key_0' animmt beim Merging, wird hier als neuer Row-Index für den gemerged Dataframe gesetzt
test # check if it worked
```

### <a id="merge-multiple-ddset" style="color:black; text-decoration: none;"><u>Merge different datasets simultaneously together</u></a>:

**<u>Ausgangssituation</u>**: Assume that - initially - we have 2 datasets, 1 is for the weather and 1 is for the Energy-Prices. Furthermore, they those two datasets are **time-series**. Hence, they have the same time-index (= row-label), formatted in **UTC**.

**<u>Step 1</u>**: Split up the weather-dataset, sorted by cities


```python
# Split the df_weather into 5 dataframes (one for each of the 5 cities):

df_1, df_2, df_3, df_4, df_5 = [x for _, x in df_weather.groupby('city_name')]
dfs = [df_1, df_2, df_3, df_4, df_5]
```

**<u>Step 2</u>**: Merge the 5 sub-datasets with the Energy-Price Dataset


```python
# Step 1: save a copy, in case you do a wrong merging!
df_final = df_energy 

# Step 2: make a for-loop, to merge all the 6 datasets simultaneously
for df in dfs: # here, we loop through every city-group of our list of data frames (see step 1)
    city = df['city_name'].unique() # we store the names of the 5 cities - as a list - in a variable
    city_str = str(city).replace("'", "").replace('[', '').replace(']', '').replace(' ', '') # we perform some string-manipulation to eliminate all the characters that are not necessary
    df = df.add_suffix('_{}'.format(city_str)) # we re-name the columns, by adding the name of the city to each column
    df_final = df_final.merge(df, # this is the merging-part!
                              on=['time'], # we want to merge via the index // row-label of both datasets --> since they are both in UTC-time, this will work!
                              how='outer') # 'outer' means: we want the 'union' --> see this youtube-video for a good explanation: https://www.youtube.com/watch?v=h4hOPGo4UVU
    df_final = df_final.drop('city_name_{}'.format(city_str), axis=1) # let's drop some columns that we don't need anymore
    
# Step 3: "final results"-check
df_final.columns # show the merging-results, by displaying all the column-names --> DONE! =)
```

<u>**Step 3**</u>: Make some final-checks

<ul>
    <li><strong><u>Key-Question</u></strong>: <h1><em><center>Did the merging really worked? &#8594; Yes / No</center></em></h1></li><br>
    <ul>
        <li><strong><u>"Trick" to answer the question</u></strong>: Look at Missings &amp; Duplicates.</li>
    </ul>
</ul>


```python
# Display the number of NaNs and duplicates in the final dataframe

print('There are {} missing values or NaNs in df_final.'
      .format(df_final.isnull().values.sum()))

temp_final = df_final.duplicated(keep='first').sum()

print('\nThere are {} duplicate rows in df_energy based on all columns.'
      .format(temp_final))
```

<h3><em>If the answer is 'yes', then merging should have worked, if you have no missings and no duplicates. <br><br>
    If the answer to the above question is 'no', then you need to go back to step 2 and try to figure out what you did wrong when merging the datasets together.</em></h3>

# <a id="advanced-exploration" style="color:black; text-decoration: none;">Advanced Exploration of your data</a>
---

### <a id="create-correlation-matrix" style="color:black; text-decoration: none;"><u>How to create &amp; plot a `Pearson Correlation Matrix` out of a Dataframe?</u></a>:


```python
# Step 1: construct a 'Pearson Correlation Matrix' as a Dataframe:
correlations = df.corr(method='pearson')

# Step 2: Load Libraries & plot Pearson correlation matrix:
import matplotlib.pyplot as plt
import seaborn as sns

fig = plt.figure(figsize=(24, 24)) #
sns.heatmap(correlations, annot=True, fmt='.2f') # import seaborn as sns
plt.title('Pearson Correlation Matrix')
plt.show()
```

# <a id="time-series-exploration" style="color:black; text-decoration: none;">Time-Series: Exploration &amp; Visualizations</a>
---

### <a id="plot-time-series" style="color:black; text-decoration: none;"><u>Plotting a Time Series</u></a>:

**<u>Step 1</u>**: Transform the `date`-column into a `date-time`-column.


```python
from datetime import datetime as dt # this is the package we need to convert a column into a 'date-time'

# Step 1: Define a function that is reading the date-column correctly

def parse_date(date):
    data=str(date)
    if date=="": # this condition will never be true, since the column 'Date-Time' NEVER has an empty string; ### raise exception
        return None
    else:
        return pd.to_datetime(date, format='%Y-%m-%d %H:%M:%S', yearfirst = True, utc = True)

# Step 2: apply the above function on the Column 'Date-Time' to transform the column into a 'date-time'-type
raw_dd["Date-Time"] = raw_dd["Date-Time"].apply(parse_date)
```

**<u>Step 2</u>**: Set the 'time'-column as the index (= row-label), otherwise the plotting won't work


```python
# set the date-column as our row-label:
raw_dd = raw_dd.set_index("Date-Time")
```

<u>**Step 3**</u>: Visualize the Time-Series


```python
from sktime.utils.plotting import plot_series # use sktime and the 'plot_series'-module for this task

# Define our y-variable (= DA-Prices in CH):
y = raw_dd["pricesCH"] 

plot_series(y) # visual check if it worked? --> yes!
```

### <a id="auto-and-partial-auto-corr" style="color:black; text-decoration: none;"><u>Plot Auto-Correlation &amp; Partial-Auto-Correlation Functions</u></a>:

The partial autocorrelation plot of the eletricity price time series shows that the direct relationship between an observation at a given hour (t) is strongest with the observations at t-1, t-2, t-24 and t-25 time-steps and diminishes afterwards. **Thus, we are going to use the 25 previous values of each time series which will constitute a feature for our models**.


```python
# Step 1: load library
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # to plot 

# Step 2: Plot autocorrelation and partial autocorrelation plots
fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 6))
plot_acf(df_energy['pricesCH'], lags=50, ax=ax1)
plot_pacf(df_energy['pricesCH'], lags=50, ax=ax2)
plt.tight_layout()
plt.show()
```

### <a id="cross-corr" style="color:black; text-decoration: none;"><u>Plot the Cross-Correlation(Y<sub>t</sub>, X<sub>t - k</sub>)</u></a>:

It would quite definitely be **more beneficial if we only chose to use specific past values (observations at certain time-lags) of a given feature, based on the cross-correlation between the electricity price and each one of the features in the dataset**. For example, below we can see the cross-correlation between the electricity price and the Renewable Generation from Switzerland. We see that there are many time-lags with a correlation which is close to zero and could be ommited.


```python
from statsmodels.tsa.stattools import ccf # to plot the cross-correlation function

# Step 2: plot the cross-correlation between the day-ahead price "today" and with each of the first 50-lags of the column 'RenGen' 
cross_corr = ccf(df_energy['RenGenCH'], df_energy['pricesCH'])
plt.plot(cross_corr[0:50])
plt.show()
```

# <a id="feature-engineering-time-series" style="color:black; text-decoration: none;">Feature Engineering for Time-Series</a>
---

Wenn du mit Variablen arbeitest, welche mit "Zeit" zu tun haben, brauchst du einige "Tricks", um Data Cleaning betreiben zu können mit sogenannten `Date-Time Objects` in Python.

### <a id="time-stamps-as-index" style="color:black; text-decoration: none;"><u>Creating an index for Time-Series</u></a>:


```python
start = '2020-01-01'
stop = '2021-01-01' # ACHTUNG: wird am 31.12.2020 enden
ts_index = pd.date_range(start, stop, freq = 'h', closed = 'left', tz = 'UTC')
ts_index
```




    DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-01 01:00:00+00:00',
                   '2020-01-01 02:00:00+00:00', '2020-01-01 03:00:00+00:00',
                   '2020-01-01 04:00:00+00:00', '2020-01-01 05:00:00+00:00',
                   '2020-01-01 06:00:00+00:00', '2020-01-01 07:00:00+00:00',
                   '2020-01-01 08:00:00+00:00', '2020-01-01 09:00:00+00:00',
                   ...
                   '2020-12-31 14:00:00+00:00', '2020-12-31 15:00:00+00:00',
                   '2020-12-31 16:00:00+00:00', '2020-12-31 17:00:00+00:00',
                   '2020-12-31 18:00:00+00:00', '2020-12-31 19:00:00+00:00',
                   '2020-12-31 20:00:00+00:00', '2020-12-31 21:00:00+00:00',
                   '2020-12-31 22:00:00+00:00', '2020-12-31 23:00:00+00:00'],
                  dtype='datetime64[ns, UTC]', length=8784, freq='H')



#### <u>(Optionale) Erweiterung</u>: Index-Slicing

Der **Nachteil** am obigen Index ist, dass er immer erst **ab 0:00 Uhr beginnt**. Das **Problem** dabei ist, dass *du nicht immer einen Index benötigen wirst, welcher punktgenau um 0:00 Uhr beginnt, sondern vielleicht einmal um 23:00 Uhr*. 

- <u>Lösung</u>: <strong>Verwende `Slicing`, um ab der ersten Beobachtung vom Jahr 2020, um 00:00 zu starten, beispielsweise so: `DF2 = DF1.loc[:'2019-05-26 13:00:00+00:00']` für einen DataFrame, beziehungsweise so: `ts_index[4:]` für eine Pandas-Series</strong>. 

### <a id="copy-index-as-columns" style="color:black; text-decoration: none;"><u>Make a copy of the index as a Column of a DataFrame</u></a>:


```python
p_train = p_train.reset_index()
p_train['date'] = p_train['Date-Time']
p_train.set_index('Date-Time')
```

### <a id="make-datetime-and-set-index" style="color:black; text-decoration: none;"><u>Converting a Time-Column into a Datetime-Column and set it as an Index (= row label)</u></a>:

**If you work with time-series**, it will oftentimes be the case that you will need to convert your date-column - which **oftentimes are strings** - into an actual `date-type` column!


```python
df_energy['Date-Time'] = pd.to_datetime(df_energy['Date-Time'], utc=True, infer_datetime_format=True)
df_energy = df_energy.set_index('Date-Time') # set the 'Date-Time'-Column as the index // row-label of our data-frame
```

##### <u>If you want to set the format by yourself</u>:


```python
#convert Delivery day to a date time column
epex_df['Delivery day'] = pd.to_datetime(epex_df['Delivery day'], format = '%Y%m%d')
```

### <a id="create-range-of-dates" style="color:black; text-decoration: none;"><u>Creating a range of dates in Python</u></a>:

Specify start and end, with the default daily frequency.


```python
import pandas as pd

pd.date_range(start='1/1/2018', end='1/08/2018')
```




    DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
                   '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],
                  dtype='datetime64[ns]', freq='D')



## <a id="rechnen-mit-date-time-objects" style="color:black; text-decoration: none;">Rechnen mit Date-Time Objects: Addition und Subtraktion von Zeiten</a>
---

Oftmals wirst du Zeitpunkte in der Zukunft oder Vergangenheit definieren müssen. Deshalb ist es essentiell, dass du lernst, wie man mit `Date-Time Objekten` rechnet:

### <a id="today" style="color:black; text-decoration: none;"><u>Finde den genauen Zeitpunkt "jetzt" // "heute" heraus</u></a>:


```python
import datetime # Da wir mit "Zeit" arbeiten, brauche ich das Package "datetime"

jetzt = datetime.datetime.now() # current time

print(jetzt) # check: should output the current time
```

    2021-07-01 13:54:53.425398


### <a id="zukunft-relativ-zu-jetzt" style="color:black; text-decoration: none;"><u>Zukunftszeitpunkt mit dem "jetzt" als Referenzpunkt // Startpunkt</u></a>:


```python
import datetime # Da wir mit "Zeit" arbeiten, brauche ich das Package "datetime"

jetzt = datetime.datetime.now() # current time
added_time = datetime.timedelta(hours= 1, minutes= 1) # anzahl an Stunden & Minuten, die du dazu addieren willst
ende = jetzt + added_time

print(ende) # check: should output "current time + 5h 2 min"
```

    2021-07-01 15:00:59.045610


### <a id="subtraktion-von-zeitpunkt" style="color:black; text-decoration: none;"><u>Rechnen mit Datum: Subtraktion mit `fixed time-point` als Referenzpunkt // Startpunkt</u></a>:


```python
import datetime # Da wir mit "Zeit" arbeiten, brauche ich das Package "datetime"

fixe_zeit = datetime.datetime(2021, 7, 1, 8, 12) # datetime(year, month, day, hour, minute, second, microsecond), 
# wobei die ersten 3 Argumente (Jahr, Monat, Tag) OBLIGATORISCH sind! --> hier interessiert uns die letzten 2 Inputs, 
# nämlich die "8" und die "12" --> diese wiederspiegeln meine fixe 8h 12 min Arbeitszeit bei der SBB
added_time = datetime.timedelta(hours= 1, minutes= 1) # anzahl an Stunden, die du noch arbeiten musst
wie_lange_noch = fixe_arbeitszeit - added_time

print(wie_lange_noch.strftime("%H%M")) # check: should output 7h 11min --> yes!
```

    0711


### <a id="wochentag-herausfinden" style="color:black; text-decoration: none;"><u>Day of the Week given a Date</u></a>:

If you want to know - for example - which "Wochentag" z.B. the `21.03.2020` was, simply use this code:


```python
import datetime

fixe_zeit = datetime.datetime(2020, 12, 12) # datetime(year, month, day, hour, minute, second, microsecond)
fixe_zeit.weekday() 

# --- Gemäss Documentation

# 0 = Monday
# 1 = Tuesday 
# 2 = Wednesday
# 3 = Thursday
# 4 = Friday
# 5 = Saturday
# 6 = Sunday
```




    5




```python
addierte_anzahl_tage = datetime.timedelta(days = 28)
next_day = fixe_zeit + addierte_anzahl_tage
next_next_day = next_day + addierte_anzahl_tage
next_next_next_day = next_next_day + addierte_anzahl_tage
next_next_next_next_day = next_next_next_day + addierte_anzahl_tage
next_next_next_next_day
```




    datetime.datetime(2021, 3, 22, 0, 0)



We see that the `21.03.2020` was a **Saturday**. I checked it with the calendar: it's correct =)

# Dummy-Variables
---

### <a id="create-normal-dummies" style="color:black; text-decoration: none;"><u>Create a "normal" Dummy</u></a>:

In Python, you need the `where`-method to create a dummy on a single condition. 

This is equivalent to the `ifelse()`-Function in `R`:


```python
import numpy as np

np.where(maupayment['log_month'] == maupayment['install_month'], 'install', 'existing')
```

### <a id="flagg-dummy-by-merging" style="color:black; text-decoration: none;"><u>Create a Dummy by `merging` &amp; `flagging`</u></a>:

This can be useful, when you need to track which of the variable was part of the "left" and which one was part of the "right" dataset. You literally create a dummy-variable when you do this and it is called the `_merge`-column.

- <u>Andwendungsfall 1</u>: als ich bei der SBB versucht habe, einen "Ferien-Dummy" zu kreieren.


```python
df2 = df.merge(df_year_2019, 
                 on='date', # in this case, we merge over the "date"-column; note that 'df' has 365 rows, while 'df_year_2019' only 270 rows
                 how='outer', # we need to set
                 indicator=True) # the flag is "indicator"
df2 # we get a ddset with 360 rows. On the outside, the 'testo'-ddset seems NOT to be different form the 'df', however, the purpose of the merge was to create a new dummy-variable to see from which dataset it came from
```


```python
# step 2: we want some more meaningful values within the 'merge'-column --> apply an if-statement onto the (string-) column

df2.loc[df2['_merge'] == 'both', 'ferien'] = 1 # Here, we create a column 'Ferien', which will be equal to "1", if the value in the '_merge'-column is equal to the string "both"
df2.loc[df2['_merge'] != 'both', 'ferien'] = 'False' # Ferien-Dummy equals zero otherwise
```

## <a id="dummy-variables-for-time-variables" style="color:black; text-decoration: none;">Dummy-Variables: Create Columns for specific hours, seasons of the year etc...</a>
---

In the following, I will use a column called `Date-Time` to create *new* columns to extract some 'time-information' (hours, quarters etc...) needed.

### <a id="dummy-for-hours" style="color:black; text-decoration: none;"><u>Extract `hours` and put it into a separate column</u></a>:


```python
raw_dd['Hour'] = raw_dd['Date-Time'].apply(lambda x: x.hour) # Werte von 0 [= Stunde 0:00-1:00] bis 23 [= Stunde 23:00-24:00]
```

### <a id="dummy-for-quarters" style="color:black; text-decoration: none;"><u>Extract `Seasons` (= "qarters") and put it into a separate column</u></a>:


```python
raw_dd['Quarter'] = raw_dd['Date-Time'].apply(lambda x: x.quarter) # Werte von 1 [Januar bis März] bis 4 [Oktober bis Dezember]
```

### <a id="dummy-for-days" style="color:black; text-decoration: none;"><u>Extract `Days` and put it into a separate column</u></a>:


```python
raw_dd['day_of_week'] = raw_dd['Date-Time'].apply(lambda x: x.weekday()) # Werte von 0 [= Montag] bis 6 [= Sonntag]
```

## <a id="scaling-variables" style="color:black; text-decoration: none;">Scaling Variables</a>
---

When applying machine learning, one condition to use those fancy models will be to scale our variables first. 

The main **methods for scaling can be seen in [this Stack-Exchange Post](https://stats.stackexchange.com/questions/70553/what-does-normalization-mean-and-how-to-verify-that-a-sample-or-a-distribution)**.

### <a id="normalization-scaling" style="color:black; text-decoration: none;"><u>Normalization</u></a>:

If you normalize variables, then it means that the RANGE of possible values (= Definitionsbereich), is set between 0 and 1. 

#### <u>Scale _all_ X-Variables and your Y-variable</u>:


```python
from sklearn.preprocessing import MinMaxScaler # Der Name der Klasse für Normalisierung = 'MinMaxScaler' (--> mega weird xDD)

# step 1: initialize the Class
scaler_X = MinMaxScaler(feature_range=(0, 1))
scaler_y = MinMaxScaler(feature_range=(0, 1))
```


```python
# step 2: wende den Scaler uf die X- & Y-Variablen (dabei nur auf das Training-Set) an (Achtung: funktioniert nur auf numy-arrays, 
# aber nicht auf Series // Data-Frame Columns!)
scaler_X.fit(X_compl_df[:train_end_idx]) # alternativ: scaler_y.fit(X_train.to_numpy().reshape(-1, 1))
scaler_y.fit(y_compl_df[:train_end_idx]) # alternativ: scaler_y.fit(y_train.to_numpy().reshape(-1, 1))
```


```python
# step 3: nach dem "fit" haben wir die Werte noch nicht als "DataFrame"-Typ gespeichert, weshalb wir nun noch 'transform' anwenden
X_norm = scaler_X.transform(X_compl_df)
y_norm = scaler_y.transform(y_compl_df)
```

# <a id="umwandlungen" style="color:black; text-decoration: none;">Umwandlungen</a>
---


### <a id="series-to-array" style="color:black; text-decoration: none;"><u>Convert a Series into an array</u></a>:


```python
Series.to_numpy() # this also works on a column of a dataframe =)
```


```python

```

# Apply If-Conditions on a DataFrame
---

- Here is a really good reference: https://datatofish.com/if-condition-in-pandas-dataframe/

# <a id="data-viz" style="color:black; text-decoration: none;">Visualization of the Data</a>
---

### <a id="plot-and-save-image" style="color:black; text-decoration: none;"><u>Plot &amp; Save an Image</u></a>:


```python
# Step 1: import the library matplotlib for visualization
import matplotlib.pyplot as plt # for the settings of the graph, such as title, windows-size etc...

# Step 2: make the plot
plt.figure(figsize=(14,6))
fig, ax = plot_series(y_train[10290:10320], # only take the last 30 observations from the trainings-set
            y_test, 
            y_pred, 
            labels=["y_train", "y_test", "y_pred"])
plt.xticks(rotation=90) # rotates Beschriftung der X-Achse um 90-Grad, damit man überhaupt die X-Achse lesen kann

# Step 3: save the plot
fig.savefig("monochrome-2-test.png") # save the graph
```

### <a id="multi-plots-for-loop" style="color:black; text-decoration: none;"><u>For-Loops to iterate over lots of variables and save them</u></a>:


```python
# step 1: create a list of all the columns, you want to iterate through

cov_list = X_train.columns # I select all columns in my dataframe 'X_train'
```


```python
# step 2:

for i in cov_list:
    print(i) # just as a check, to see how the loop progresses
    covariate_name = i # everytime I iterate over a new variable, I store it into this variable --> I will use this variable later to save each 
    covariate = X_train[i] # I need each variable to be of type "Series" when creating a plot
    x = list(range(0, 720, 24)) # this will put a tick-mark after each day
    x.append(719) # I also add '719', since the range()-function does not include this number
    fig, ax = plot_series(covariate[-720:]) # this plots the last month
    plt.xticks(x); # to only display some dates
    plt.xticks(rotation=90); # we need to rotate the x-axis, otherwise you cannot read it
    fig.savefig("visualisation_30_days-{}.png".format(covariate_name)) # save the graph for each column. The "trick" here is, that we can use 
    # f-strings to create a unique name for each file =)
```

# <a id="magic-commands" style="color:black; text-decoration: none;">Python Magic Commands</a>
---

## <u><a id="view-all-columns" style="color:black; text-decoration: none;">Show ALL Columns of a DataFrames</a></u>

Wenn grosse Datensätze geprinted werden, ist beim Output oftmals ein Teil der Spalten maskiert, da schlichtweg der Platz fehlt, um alle Spalten anzuzeigen. Allerdings gibt es einen Weg, die Einstellungen via `Pandas` zu ändern:


```python
import pandas as pd

pd.set_option('display.max_rows', 500)

pd.set_option("display.max.columns", None) # this will tell Python: "show me ALL the columns!"
pd.set_option('display.max_columns', 500)

pd.set_option('display.width', 1000)
```

## <a id="html-with-jupyter-notebooks" style="color:black; text-decoration: none;"><u>Working with HTML within Jupyter-Notebooks</u></a>


### <u>How to use anchor-tags to make references within your Jupyter-Notebook</u>?

[Klick here to move to the very top of this Jupyter-Notebook](#top). This is achieved by using the `<a>`-HTML-Tag, and some unique ID-name &amp; `CSS`-styling on the `<a>`-Tag.

```html
<a id="gebe-hier-passenden-id-namen" style="color:black; text-decoration: none;">Hier kommt der Text, auf welchem du zeigen willst...</a>
```

```markdown
[Und das ist der Text, welcher dich weiterleitet](#top)
```

## <a id="share-variables-between-notebooks" style="color:black; text-decoration: none;"><u>Share Variables between Notebooks</u></a>

This `Magic Commands` allows you to share any variable between different Jupyter Notebooks. You need to pass the original variable with the magic command. To retrieve the variable, you need to pass the same command with the `-r` parameter.


```python
myData = "The World Makes Sense"
%store myData
```

    Stored 'myData' (str)


Now that you 'saved' the variable `myData` with the `Magic Commands`, **go and open another of your Jupyter Notebooks** and type in the following:

```python
%store -r myData # step one: run this line in the first cell
myData # step 2: run this line in the secons cell
```

# <a id="useful-tricks" style="color:black; text-decoration: none;">Useful "Tricks" I stumbled upon</a>
---

### <a id="reshape-1-1" style="color:black; text-decoration: none;"><u>Reshape(-1, 1)</u></a>:

> What does `reshape(-1,1)` do?

If you have an array or list, like this: `[1,2,3,4,5]`...

Using `reshape(-1,1)` on it, will create a new **list of lists, with only 1 element in each sub-list**, e.g. the output will be `[[1], [2], [3], [4], [5]]`. 

*See also [this Stack-Overflow answer](https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape)*.

# <a id="math-tricks" style="color:black; text-decoration: none;">Math Tricks for Hacks</a>
---

### <a id="round-downwards" style="color:black; text-decoration: none;"><u>How to _always_ round downwards</u></a>?:

Mit der Funktion `round`, kann man auf- oder abwärts runden, wie wir es gewohnt sind:


```python
round(47/24, ndigits = 2) # runde auf 2 Nachkommastellen
```




    1.96



**Mit der `floor`-Funktion, kann allerdings stets abwärts gerundet werden, wie das Beispiel zeigt**:


```python
import math # um 'floo'

math.floor(47/24)
```




    1



### <a id="calc-median" style="color:black; text-decoration: none;"><u>Calculate the Median?</u></a>

Since there is no built-in function for the median, you can write a function which will be able to calculate the median:


```python
# Step 1: make a function for the case "median calculation for an ODD (= ungerade) list of numbers"

def _median_odd(xs) -> float:
    return sorted(xs)[len(xs) // 2]
```


```python
# Step 2: make a function for the case "median calculation for an EVEN (= gerade) list of numbers"

def _median_even(xs) -> float:
    sorted_xs = sorted(xs)
    hi_midpoint = len(xs) // 2
    return (sorted_xs[hi_midpoint - 1] + sorted_xs[hi_midpoint]) / 2
```


```python
# Step 3: Use the 2 above functions to finally be able to build the median-function

def median(v) -> float:
    return _median_even(v) if len(v) % 2 == 0 else _median_odd(v)
```


```python
assert median([1, 10, 2, 9, 5]) == 5 # check if the above function was written correctly (you can change the number '5' and see
# what happens, if this expression becomes incorrect)
```

# Fun
---

In this section, I will just put some important concepts &amp; some of my "inventions" that I found useful.


```python
import random 
import numpy as np

df = pd.DataFrame(np.random.randn(10, 2),
                  columns=['Col1', 'Col2'])
df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',
                     'B', 'B', 'B', 'B', 'B'])
```


```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Col1</th>
      <th>Col2</th>
      <th>X</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.616909</td>
      <td>-0.998962</td>
      <td>A</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.036902</td>
      <td>-0.991193</td>
      <td>A</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.418524</td>
      <td>-0.070984</td>
      <td>A</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.207328</td>
      <td>1.896855</td>
      <td>A</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.376757</td>
      <td>-0.333235</td>
      <td>A</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.295216</td>
      <td>1.488426</td>
      <td>B</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1.226809</td>
      <td>-1.351006</td>
      <td>B</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.476242</td>
      <td>-0.431204</td>
      <td>B</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-1.565872</td>
      <td>1.174200</td>
      <td>B</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.469251</td>
      <td>-0.353409</td>
      <td>B</td>
    </tr>
  </tbody>
</table>
</div>




```python
for i,el in enumerate(list(df.columns.values)[:-1]):
    a = df.boxplot(el, by ='type')
```


    
![png](output_252_0.png)
    



    
![png](output_252_1.png)
    



    
![png](output_252_2.png)
    


### <u>Arbeitszeit (left to work...) Berechnung</u>:

Um zu sehen, **wie viele Stunden &amp; Minuten ich heute noch arbeiten muss**, habe ich - als Übung - eine Funktion `hours_to_go` selbst erfasst, welche mir die Anzahl an noch zu verbleibenden Arbeitsstunden am Tag berechnet:


```python
4*30*24
```




    2880




```python
import datetime # Da wir mit "Zeit" arbeiten, brauche ich das Package "datetime"


def hours_to_go(Stunden, Minuten): # Input: wie viele Stunden & Minuten hast du heute bereits gearbeitet?
    """gibt an, wie lange ich noch heute "Schaffen" muss --> gebe dafür bloss die Anzahl stunden & minuten ein, die du "heute" 
    bereits gearbeitet hast
    > hours_to_go(3,5) --> 0507 
    interpretation: ich habe fix 8h 12min zu arbeiten. Davon ziehe ich nun 3h 5 min ab --> 0507 heisst, ich habe noch 5h 7min 
    zu arbeiten, was stimmt!"""
    
    fixe_arbeitszeit = datetime.datetime(2021, 7, 1, 8, 12) # datetime(year, month, day, hour, minute, second, microsecond), 
    # wobei die ersten 3 Argumente (Jahr, Monat, Tag) OBLIGATORISCH sind! --> hier interessiert uns die letzten 2 Inputs, 
    # nämlich die "8" und die "12" --> diese wiederspiegeln meine fixe 8h 12 min Arbeitszeit bei der SBB
    gearbeitet = datetime.timedelta(hours= Stunden, minutes= Minuten) # anzahl an Stunden, die du noch arbeiten musst
    wie_lange_noch = fixe_arbeitszeit - gearbeitet
    print(wie_lange_noch.strftime("%H%M"))
    
hours_to_go(0,5) # call the function to output, how many hours are left to work
```

    0807


Um zu sehen, **um welche Uhrzeit ich heute Feierabend machen darf**, habe ich - als Übung - eine Funktion `arbeit_ende` selbst erfasst:


```python
import datetime # Da wir mit "Zeit" arbeiten, brauche ich das Package "datetime"

def arbeit_ende(Stunden, Minuten): 
    """gibt an, wann ich heute mit "Schaffen" fertig bin --> gebe dafür bloss die Anzahl stunden & minuten ein, die du "heute" 
    noch arbeiten musst, zum Beispiel arbeite ich heute noch '4h 44 min':
    > arbeit_ende(4, 44) --> 2021-07-01 17:53:02.907698 """
    
    jetzt = datetime.datetime.now() # current time
    added_time = datetime.timedelta(hours= Stunden, minutes= Minuten) # anzahl an Stunden, die du noch arbeiten musst
    ende = jetzt + added_time
    print(ende)
    
    
arbeit_ende(8,7) # call the function to output, until when I need to work today 
```

    2021-07-29 15:53:13.919695
